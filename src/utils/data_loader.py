# src/utils/data_loader.py
"""
ã‚¿ã‚¹ã‚¯3ç”¨çµ±ä¸€ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼

æ©Ÿèƒ½:
- å¤šæ§˜ãªãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®çµ±ä¸€èª­ã¿è¾¼ã¿ (.npz, .npy, .csv, .json)
- ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ãƒ»å‰å‡¦ç†
- æ­£è¦åŒ–ãƒ»æ¨™æº–åŒ– 
- æ™‚ç³»åˆ—é †ã‚’ä¿ã£ãŸå­¦ç¿’/æ¤œè¨¼/ãƒ†ã‚¹ãƒˆåˆ†å‰²
- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†

å‰æãƒ‡ãƒ¼ã‚¿å‹: (T, d) å¤šå¤‰é‡æ™‚ç³»åˆ— torch.FloatTensor
"""

import os
import json
import warnings
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple
from dataclasses import dataclass, asdict

import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler, MinMaxScaler


@dataclass
class DataMetadata:
    """ãƒ‡ãƒ¼ã‚¿ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿"""
    original_shape: Tuple[int, int]
    feature_names: Optional[List[str]]
    time_index: Optional[List]
    sampling_rate: Optional[float]
    missing_ratio: float
    data_source: str
    normalization_method: str
    train_indices: Tuple[int, int]
    val_indices: Tuple[int, int]
    test_indices: Tuple[int, int]
    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿é–¢é€£ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰è¿½åŠ 
    has_target_data: bool = False
    target_shape: Optional[Tuple[int, int]] = None
    target_feature_names: Optional[List[str]] = None
    target_dtype: Optional[str] = None


class DataLoaderError(Exception):
    """ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼å°‚ç”¨ä¾‹å¤–"""
    pass


class UniversalTimeSeriesDataset(Dataset):
    """
    çµ±ä¸€æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    
    ç‰¹å¾´:
    - æ™‚ç³»åˆ—é †ä¿æŒåˆ†å‰²
    - è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼å¯¾å¿œ
    - è‡ªå‹•æ­£è¦åŒ–
    - æ¬ æå€¤å‡¦ç†
    """
    
    def __init__(
        self,
        data_path: str,
        split: str = "train",
        train_ratio: float = 0.7,
        val_ratio: float = 0.2,
        test_ratio: float = 0.1,
        normalization: str = "standard",
        handle_missing: str = "interpolate",
        feature_names: Optional[List[str]] = None,
        experiment_mode: str = "reconstruction"  # æ‰‹å‹•åˆ‡ã‚Šæ›¿ãˆç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    ):
        """
        Args:
            data_path: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            split: "train", "val", "test"
            train_ratio: è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ¯”ç‡
            val_ratio: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿æ¯”ç‡
            test_ratio: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ¯”ç‡
            normalization: "standard", "minmax", "none"
            handle_missing: "interpolate", "forward_fill", "remove"
            feature_names: ç‰¹å¾´é‡åãƒªã‚¹ãƒˆ
            experiment_mode: "reconstruction" or "target_prediction"
        """
        super().__init__()
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¤œè¨¼
        if abs(train_ratio + val_ratio + test_ratio - 1.0) > 1e-6:
            raise DataLoaderError(f"åˆ†å‰²æ¯”ç‡ã®åˆè¨ˆãŒ1.0ã§ã‚ã‚Šã¾ã›ã‚“: {train_ratio + val_ratio + test_ratio}")
        
        if split not in ["train", "val", "test"]:
            raise ValueError(f"split must be 'train', 'val', or 'test'; got '{split}'")
            
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self.data_path = Path(data_path)
        self.split = split
        self.normalization = normalization
        self.experiment_mode = experiment_mode  # å®Ÿé¨“ãƒ¢ãƒ¼ãƒ‰ä¿å­˜
        
        # ç”Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        raw_data = self._load_raw_data()
        
        # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãƒ»ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        cleaned_data = self._validate_and_clean(raw_data, handle_missing)
        
        # æ­£è¦åŒ–
        normalized_data, self.scaler = self._normalize_data(cleaned_data, normalization)
        
        # æ™‚ç³»åˆ—é †åˆ†å‰²
        split_data = self._split_time_series(normalized_data, train_ratio, val_ratio, test_ratio)
        
        # åˆ†å‰²ãƒ‡ãƒ¼ã‚¿å–å¾—
        self.data = split_data[split]
        self.length = self.data.shape[0]
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        self._create_metadata(raw_data, split_data, feature_names)

    def _detect_target_data(self, data: dict) -> Dict[str, Any]:
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã®è‡ªå‹•æ¤œå‡ºï¼ˆquad_linkãƒ‡ãƒ¼ã‚¿å¯¾å¿œãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰"""
        # ã‚¯ãƒ©ã‚¹ãƒ¬ãƒ™ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§é‡è¤‡å‡¦ç†ã‚’é˜²ã
        # ç†ç”±: split="all"æ™‚ã«3ã¤ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹(train/val/test)ãŒä½œæˆã•ã‚Œã€
        #      å„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒåŒã˜ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾ã—ã¦_detect_target_dataã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚
        cache_key = str(self.data_path)
        if not hasattr(UniversalTimeSeriesDataset, '_class_target_cache'):
            UniversalTimeSeriesDataset._class_target_cache = {}

        if cache_key in UniversalTimeSeriesDataset._class_target_cache:
            return UniversalTimeSeriesDataset._class_target_cache[cache_key]

        target_info = {
            'has_target': False,
            'input_data': None,
            'target_data': None,
            'target_test_data': None
        }

        # quad_linkãƒ‡ãƒ¼ã‚¿å½¢å¼ã®ã‚­ãƒ¼ã‚’å„ªå…ˆçš„ã«æ¤œå‡º
        target_keys_train = ['train_targets', 'y_train', 'target_train', 'labels_train']
        target_keys_test = ['test_targets', 'y_test', 'target_test', 'labels_test']
        input_keys_train = ['train_obs', 'X_train', 'input_train', 'obs_train']
        input_keys_test = ['test_obs', 'X_test', 'input_test', 'obs_test']

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿æ¤œå‡º
        target_train = None
        target_test = None
        input_train = None
        input_test = None

        for key in target_keys_train:
            if key in data:
                candidate = data[key]
                # (1, T, d) â†’ (T, d) ã¸ã®reshapeå¯¾å¿œ
                if candidate.ndim == 3 and candidate.shape[0] == 1:
                    target_train = candidate.reshape(candidate.shape[1], candidate.shape[2])
                elif candidate.ndim == 2:
                    target_train = candidate
                print(f"è¨“ç·´ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿æ¤œå‡º: '{key}' â†’ shape={target_train.shape}")
                break

        for key in target_keys_test:
            if key in data:
                candidate = data[key]
                # (1, T, d) â†’ (T, d) ã¸ã®reshapeå¯¾å¿œ
                if candidate.ndim == 3 and candidate.shape[0] == 1:
                    target_test = candidate.reshape(candidate.shape[1], candidate.shape[2])
                elif candidate.ndim == 2:
                    target_test = candidate
                print(f"ãƒ†ã‚¹ãƒˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿æ¤œå‡º: '{key}' â†’ shape={target_test.shape}")
                break

        for key in input_keys_train:
            if key in data:
                input_train = data[key]
                print(f"è¨“ç·´å…¥åŠ›ãƒ‡ãƒ¼ã‚¿æ¤œå‡º: '{key}' â†’ shape={input_train.shape}")
                break

        for key in input_keys_test:
            if key in data:
                input_test = data[key]
                print(f"ãƒ†ã‚¹ãƒˆå…¥åŠ›ãƒ‡ãƒ¼ã‚¿æ¤œå‡º: '{key}' â†’ shape={input_test.shape}")
                break

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆ
        if target_train is not None and input_train is not None:
            target_info['has_target'] = True
            target_info['input_data'] = input_train
            target_info['target_data'] = target_train
            target_info['target_test_data'] = target_test  # ãƒ†ã‚¹ãƒˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            target_info['input_test_data'] = input_test    # ãƒ†ã‚¹ãƒˆå…¥åŠ›ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

            print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿æ§‹é€ ç¢ºèªå®Œäº†:")
            print(f"   - å…¥åŠ›: {input_train.shape} ({input_train.dtype})")
            print(f"   - ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: {target_train.shape} ({target_train.dtype})")
            if target_test is not None:
                print(f"   - ãƒ†ã‚¹ãƒˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: {target_test.shape} ({target_test.dtype})")

        # çµæœã‚’ã‚¯ãƒ©ã‚¹ãƒ¬ãƒ™ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ï¼ˆåŒä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®é‡è¤‡æ¤œå‡ºã‚’é˜²æ­¢ï¼‰
        UniversalTimeSeriesDataset._class_target_cache[cache_key] = target_info
        return target_info

    def _load_raw_data(self) -> np.ndarray:
        """ç”Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿è‡ªå‹•æ¤œå‡ºå¯¾å¿œï¼‰"""
        if not self.data_path.exists():
            raise FileNotFoundError(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {self.data_path}")

        ext = self.data_path.suffix.lower()

        try:
            if ext == ".npz":
                data = np.load(self.data_path)

                # æ‰‹å‹•åˆ‡ã‚Šæ›¿ãˆã«åŸºã¥ããƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
                if self.experiment_mode == "target_prediction":
                    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆäºˆæ¸¬ãƒ¢ãƒ¼ãƒ‰ï¼šã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å¼·åˆ¶çš„ã«æ¤œå‡º
                    target_info = self._detect_target_data(data)

                    if target_info['has_target']:
                        self.has_target = True
                        self.target_data = target_info['target_data']
                        self.target_test_data = target_info.get('target_test_data', None)
                        self.input_test_data = target_info.get('input_test_data', None)
                        raw_data = target_info['input_data']
                        print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆäºˆæ¸¬ãƒ¢ãƒ¼ãƒ‰: å…¥åŠ›{raw_data.shape} â†’ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ{self.target_data.shape}")
                    else:
                        raise DataLoaderError(
                            f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆäºˆæ¸¬ãƒ¢ãƒ¼ãƒ‰ãŒæŒ‡å®šã•ã‚Œã¾ã—ãŸãŒã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n"
                            f"åˆ©ç”¨å¯èƒ½ãªã‚­ãƒ¼: {list(data.keys())}\n"
                            f"æœŸå¾…ã•ã‚Œã‚‹ã‚­ãƒ¼: train_targets, y_train, target_train ãªã©"
                        )
                else:
                    # å†æ§‹æˆãƒ¢ãƒ¼ãƒ‰ï¼šã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç„¡è¦–
                    self.has_target = False
                    self.target_data = None
                    self.target_test_data = None
                    self.input_test_data = None
                    print(f"å†æ§‹æˆãƒ¢ãƒ¼ãƒ‰: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã¯ä½¿ç”¨ã—ã¾ã›ã‚“")

                    # æŸ”è»Ÿãªã‚­ãƒ¼æ¢ç´¢: å„ªå…ˆåº¦é †ã§é©åˆ‡ãªãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•é¸æŠï¼ˆç”»åƒãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰
                    candidate_keys = ['Y', 'X', 'data', 'arr_0', 'train_obs', 'test_obs']
                    raw_data = None

                    # å„ªå…ˆåº¦é †ã§ã‚­ãƒ¼ã‚’æ¢ç´¢
                    for key in candidate_keys:
                        if key in data:
                            candidate = data[key]
                            # 2æ¬¡å…ƒæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯4æ¬¡å…ƒç”»åƒãƒ‡ãƒ¼ã‚¿ã‹ãƒã‚§ãƒƒã‚¯
                            if ((candidate.ndim == 2 and candidate.shape[0] > 1) or
                                (candidate.ndim == 4 and candidate.shape[0] > 1)):
                                raw_data = candidate
                                print(f"npzãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚­ãƒ¼ '{key}' ã‚’ä½¿ç”¨: shape={candidate.shape}")
                                break

                    # å„ªå…ˆã‚­ãƒ¼ãŒãªã„å ´åˆã€åˆ©ç”¨å¯èƒ½ãªå…¨ã‚­ãƒ¼ã‹ã‚‰æœ€é©ãªã‚‚ã®ã‚’é¸æŠ
                    if raw_data is None:
                        available_keys = list(data.keys())
                        for key in available_keys:
                            candidate = data[key]
                            # 2æ¬¡å…ƒæ™‚ç³»åˆ—ã¾ãŸã¯4æ¬¡å…ƒç”»åƒãƒ‡ãƒ¼ã‚¿ã§æ™‚ç³»åˆ—ã¨ã—ã¦å¦¥å½“ãªã‚µã‚¤ã‚º
                            if (hasattr(candidate, 'ndim') and
                                ((candidate.ndim == 2 and candidate.shape[0] > 1 and candidate.shape[1] > 0) or
                                 (candidate.ndim == 4 and candidate.shape[0] > 1))):
                                raw_data = candidate
                                print(f"npzãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ¨å®šã‚­ãƒ¼ '{key}' ã‚’ä½¿ç”¨: shape={candidate.shape}")
                                break

                    # ãã‚Œã§ã‚‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
                    if raw_data is None:
                        available_info = []
                        for key in data.keys():
                            try:
                                shape = data[key].shape if hasattr(data[key], 'shape') else 'scalar'
                                dtype = data[key].dtype if hasattr(data[key], 'dtype') else type(data[key])
                                available_info.append(f"'{key}': shape={shape}, dtype={dtype}")
                            except:
                                available_info.append(f"'{key}': (èª­ã¿è¾¼ã¿ä¸å¯)")

                        raise DataLoaderError(
                            f"npzãƒ•ã‚¡ã‚¤ãƒ«ã«é©åˆ‡ãªãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n"
                            f"åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿: {', '.join(available_info)}\n"
                            f"æœŸå¾…ã•ã‚Œã‚‹å½¢å¼: (T, d) ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ ã¾ãŸã¯ (T, H, W, C) ã®ç”»åƒãƒ‡ãƒ¼ã‚¿"
                        )
                    
            elif ext == ".npy":
                raw_data = np.load(self.data_path)

                # npyãƒ•ã‚¡ã‚¤ãƒ«ã®æŸ”è»Ÿãªå½¢çŠ¶å¯¾å¿œ
                if raw_data.ndim == 1:
                    # 1æ¬¡å…ƒã®å ´åˆã¯å˜å¤‰é‡æ™‚ç³»åˆ—ã¨ã—ã¦æ‰±ã†
                    raw_data = raw_data.reshape(-1, 1)
                    print(f"npyãƒ•ã‚¡ã‚¤ãƒ«: 1æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã‚’2æ¬¡å…ƒã«å¤‰æ› shape={raw_data.shape}")
                elif raw_data.ndim > 2:
                    # 3æ¬¡å…ƒä»¥ä¸Šã®å ´åˆã¯æœ€åˆã®2æ¬¡å…ƒã‚’ä½¿ç”¨
                    original_shape = raw_data.shape
                    raw_data = raw_data.reshape(raw_data.shape[0], -1)
                    print(f"npyãƒ•ã‚¡ã‚¤ãƒ«: {original_shape} â†’ {raw_data.shape} ã«å¤‰æ›")
                elif raw_data.ndim == 2:
                    print(f"npyãƒ•ã‚¡ã‚¤ãƒ«: 2æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ shape={raw_data.shape}")
                else:
                    raise DataLoaderError(f"npyãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ãŒ0æ¬¡å…ƒã§ã™: shape={raw_data.shape}")
                
            elif ext == ".csv":
                df = pd.read_csv(self.data_path, index_col=0 if 'time' in pd.read_csv(self.data_path, nrows=1).columns else None)
                raw_data = df.values
                self._csv_feature_names = df.columns.tolist()
                
            elif ext == ".json":
                with open(self.data_path, 'r') as f:
                    json_data = json.load(f)
                if 'data' in json_data:
                    raw_data = np.array(json_data['data'])
                    self._json_metadata = {k: v for k, v in json_data.items() if k != 'data'}
                else:
                    raw_data = np.array(json_data)
                    
            else:
                raise DataLoaderError(f"å¯¾å¿œã—ã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {ext}. å¯¾å¿œå½¢å¼: .npz, .npy, .csv, .json")
                
        except Exception as e:
            raise DataLoaderError(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({self.data_path}): {e}")
        
        return raw_data
    
    def _validate_and_clean(self, data: np.ndarray, handle_missing: str) -> np.ndarray:
        """ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãƒ»ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆç”»åƒãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰"""
        # å½¢çŠ¶ãƒã‚§ãƒƒã‚¯
        if data.ndim == 1:
            warnings.warn("1æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã‚’2æ¬¡å…ƒã«å¤‰æ›ã—ã¾ã™")
            data = data.reshape(-1, 1)
        elif data.ndim == 2:
            # æ¨™æº–çš„ãªæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ (T, d)
            pass
        elif data.ndim == 4:
            # ç”»åƒãƒ‡ãƒ¼ã‚¿ (T, H, W, C) - RKNç”»åƒãƒ‡ãƒ¼ã‚¿å¯¾å¿œ
            T, H, W, C = data.shape
            print(f"ç”»åƒãƒ‡ãƒ¼ã‚¿æ¤œå‡º: {data.shape} (T={T}, H={H}, W={W}, C={C})")
            # ç”»åƒãƒ‡ãƒ¼ã‚¿ã¯ãã®ã¾ã¾ä¿æŒï¼ˆãƒ•ãƒ©ãƒƒãƒˆåŒ–ã—ãªã„ï¼‰
        else:
            raise DataLoaderError(f"ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {data.shape}. ã‚µãƒãƒ¼ãƒˆå½¢çŠ¶: (T,), (T, d), (T, H, W, C)")

        # ãƒ‡ãƒ¼ã‚¿é•·ãƒã‚§ãƒƒã‚¯
        T = data.shape[0]  # æœ€åˆã®æ¬¡å…ƒã¯å¸¸ã«æ™‚ç³»åˆ—é•·
        if T < 10:
            warnings.warn(f"ãƒ‡ãƒ¼ã‚¿é•·ãŒçŸ­ã™ãã¾ã™: T={T}. æœ€ä½10ä»¥ä¸Šæ¨å¥¨")
        
        # æ¬ æå€¤ãƒã‚§ãƒƒã‚¯ï¼ˆç”»åƒãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰
        missing_mask = np.isnan(data) | np.isinf(data)
        missing_ratio = missing_mask.sum() / data.size

        if missing_ratio > 0:
            warnings.warn(f"æ¬ æå€¤ã‚’æ¤œå‡º: {missing_ratio:.1%}")

            if handle_missing == "interpolate":
                if data.ndim == 2:
                    # 2æ¬¡å…ƒæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®ç·šå½¢è£œé–“
                    d = data.shape[1]
                    for j in range(d):
                        col_data = data[:, j]
                        missing_idx = np.isnan(col_data) | np.isinf(col_data)
                        if missing_idx.any():
                            valid_idx = ~missing_idx
                            if valid_idx.sum() > 1:
                                col_data[missing_idx] = np.interp(
                                    np.where(missing_idx)[0],
                                    np.where(valid_idx)[0],
                                    col_data[valid_idx]
                                )
                            else:
                                col_data[missing_idx] = 0.0
                elif data.ndim == 4:
                    # 4æ¬¡å…ƒç”»åƒãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯0ã§ç½®æ›ï¼ˆuint8ç”»åƒãƒ‡ãƒ¼ã‚¿ã¯é€šå¸¸æ¬ æå€¤ãªã—ï¼‰
                    print("ç”»åƒãƒ‡ãƒ¼ã‚¿ã®æ¬ æå€¤ã‚’0ã§ç½®æ›")
                    data = np.nan_to_num(data, nan=0.0, posinf=255.0, neginf=0.0)

            elif handle_missing == "forward_fill":
                if data.ndim == 2:
                    data = pd.DataFrame(data).fillna(method='ffill').fillna(method='bfill').values
                else:
                    # ç”»åƒãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯0ã§ç½®æ›
                    data = np.nan_to_num(data, nan=0.0, posinf=255.0, neginf=0.0)

            elif handle_missing == "remove":
                if data.ndim == 2:
                    valid_rows = ~missing_mask.any(axis=1)
                    data = data[valid_rows]
                    warnings.warn(f"æ¬ æè¡Œã‚’å‰Šé™¤: {T} -> {data.shape[0]} è¡Œ")
                else:
                    # ç”»åƒãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯å‰Šé™¤ã§ã¯ãªãç½®æ›
                    data = np.nan_to_num(data, nan=0.0, posinf=255.0, neginf=0.0)

            # ç„¡é™å¤§ãƒ»NaNã®æœ€çµ‚ãƒã‚§ãƒƒã‚¯
            if np.isnan(data).any() or np.isinf(data).any():
                warnings.warn("æ¬ æå€¤å‡¦ç†å¾Œã«ã‚‚NaN/InfãŒæ®‹å­˜ã€‚å€¤ã§ç½®æ›ã—ã¾ã™ã€‚")
                if data.ndim == 4:  # ç”»åƒãƒ‡ãƒ¼ã‚¿
                    data = np.nan_to_num(data, nan=0.0, posinf=255.0, neginf=0.0)
                else:  # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿
                    data = np.nan_to_num(data, nan=0.0, posinf=1e6, neginf=-1e6)
        
        self.missing_ratio = missing_ratio
        return data
    
    def _normalize_data(self, data: np.ndarray, method: str) -> Tuple[np.ndarray, Optional[object]]:
        """ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–ï¼ˆç”»åƒãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰"""
        if method == "none":
            return data, None

        elif method == "standard":
            # æ¨™æº–åŒ– (å¹³å‡0, æ¨™æº–åå·®1)
            if data.ndim == 4:  # ç”»åƒãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯å…¨ç”»ç´ ã§æ¨™æº–åŒ–
                data_flat = data.reshape(-1, data.shape[-1])  # (T*H*W, C)
                scaler = StandardScaler()
                normalized_flat = scaler.fit_transform(data_flat)
                normalized = normalized_flat.reshape(data.shape)  # å…ƒã®å½¢çŠ¶ã«æˆ»ã™
            else:
                scaler = StandardScaler()
                normalized = scaler.fit_transform(data)

        elif method == "minmax":
            # Min-Maxæ­£è¦åŒ– (0-1ç¯„å›²)
            if data.ndim == 4:  # ç”»åƒãƒ‡ãƒ¼ã‚¿ã®å ´åˆ
                data_flat = data.reshape(-1, data.shape[-1])
                scaler = MinMaxScaler()
                normalized_flat = scaler.fit_transform(data_flat)
                normalized = normalized_flat.reshape(data.shape)
            else:
                scaler = MinMaxScaler()
                normalized = scaler.fit_transform(data)

        elif method == "unit_scale":
            # Unit Scaleæ­£è¦åŒ–: [0, 255] â†’ [0, 1] (ç”»åƒç”¨)
            print(f"Unit Scaleæ­£è¦åŒ–: {data.dtype} [{data.min()}, {data.max()}] â†’ [0, 1]")
            if data.dtype == np.uint8:
                # uint8ç”»åƒãƒ‡ãƒ¼ã‚¿: [0, 255] â†’ [0, 1]
                normalized = data.astype(np.float32) / 255.0
            else:
                # ãã®ä»–ã®ãƒ‡ãƒ¼ã‚¿: æ—¢ã«[0, 1]ç¯„å›²ã¨ä»®å®š
                normalized = data.astype(np.float32)
            scaler = None  # unit_scaleã¯é€†å¤‰æ›ç”¨scalerã‚’ä¿æŒã—ãªã„

        else:
            raise DataLoaderError(f"å¯¾å¿œã—ã¦ã„ãªã„æ­£è¦åŒ–æ–¹æ³•: {method}. ä½¿ç”¨å¯èƒ½: 'standard', 'minmax', 'unit_scale', 'none'")

        return normalized, scaler
    
    def _split_time_series(self, data: np.ndarray, train_ratio: float, val_ratio: float, test_ratio: float) -> Dict[str, np.ndarray]:
        """æ™‚ç³»åˆ—é †åˆ†å‰²"""
        T = data.shape[0]
        
        # åˆ†å‰²ç‚¹è¨ˆç®—
        train_end = int(train_ratio * T)
        val_end = int((train_ratio + val_ratio) * T)
        
        splits = {
            "train": data[:train_end],
            "val": data[train_end:val_end],
            "test": data[val_end:]
        }
        
        # åˆ†å‰²ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨˜éŒ²
        self.split_indices = {
            "train": (0, train_end),
            "val": (train_end, val_end),
            "test": (val_end, T)
        }
        
        return splits
    
    def _create_metadata(self, raw_data: np.ndarray, split_data: Dict[str, np.ndarray], feature_names: Optional[List[str]]):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰"""
        # ç‰¹å¾´é‡åã®æ±ºå®šï¼ˆç”»åƒãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰
        if feature_names:
            final_feature_names = feature_names
        elif hasattr(self, '_csv_feature_names'):
            final_feature_names = self._csv_feature_names
        else:
            if hasattr(raw_data, 'shape'):
                if raw_data.ndim == 2:
                    # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿: (T, d)
                    final_feature_names = [f"feature_{i}" for i in range(raw_data.shape[1])]
                elif raw_data.ndim == 4:
                    # ç”»åƒãƒ‡ãƒ¼ã‚¿: (T, H, W, C)
                    T, H, W, C = raw_data.shape
                    final_feature_names = [f"image_pixel_{H}x{W}x{C}"]
                else:
                    final_feature_names = ["feature_0"]
            else:
                final_feature_names = ["feature_0"]

        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿æƒ…å ±ã®æº–å‚™
        has_target = getattr(self, 'has_target', False)
        target_shape = None
        target_feature_names = None
        target_dtype = None

        if has_target and hasattr(self, 'target_data') and self.target_data is not None:
            target_shape = self.target_data.shape
            target_dtype = str(self.target_data.dtype)
            target_feature_names = [f"target_{i}" for i in range(self.target_data.shape[1])]

        self.metadata = DataMetadata(
            original_shape=raw_data.shape,
            feature_names=final_feature_names,
            time_index=None,  # TODO: å¿…è¦ã«å¿œã˜ã¦å®Ÿè£…
            sampling_rate=None,
            missing_ratio=self.missing_ratio,
            data_source=str(self.data_path),
            normalization_method=self.normalization,
            train_indices=self.split_indices["train"],
            val_indices=self.split_indices["val"],
            test_indices=self.split_indices["test"],
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿æƒ…å ±è¿½åŠ 
            has_target_data=has_target,
            target_shape=target_shape,
            target_feature_names=target_feature_names,
            target_dtype=target_dtype
        )
    
    def __len__(self) -> int:
        return self.length
    
    def __getitem__(self, idx: int) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        """ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰"""
        sample = self.data[idx]  # shape: (d,) or (H, W, C)

        if hasattr(self, 'has_target') and self.has_target:
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚ã‚Šã®å ´åˆ: (å…¥åŠ›, ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ) ã®ã‚¿ãƒ—ãƒ«ã‚’è¿”ã™
            if hasattr(self, 'target_data') and self.target_data is not None:
                # åˆ†å‰²ã•ã‚ŒãŸã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¯¾å¿œã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
                target_sample = self._get_target_for_split(idx)
                return (torch.from_numpy(sample).float(),
                       torch.from_numpy(target_sample).float())
            else:
                return torch.from_numpy(sample).float()
        else:
            # å¾“æ¥é€šã‚Šï¼šå…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®ã¿
            return torch.from_numpy(sample).float()

    def _get_target_for_split(self, idx: int) -> np.ndarray:
        """åˆ†å‰²ã«å¯¾å¿œã™ã‚‹ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        if not hasattr(self, 'target_data') or self.target_data is None:
            raise ValueError("ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

        # åˆ†å‰²ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«åŸºã¥ã„ã¦ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ãƒ©ã‚¤ã‚¹
        split_start, split_end = self.split_indices[self.split]
        target_split_data = self.target_data[split_start:split_end]

        return target_split_data[idx]
    
    def get_full_data(self) -> torch.Tensor:
        """åˆ†å‰²ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã‚’å–å¾—"""
        return torch.from_numpy(self.data).float()
    
    def inverse_transform(self, normalized_data: Union[torch.Tensor, np.ndarray]) -> np.ndarray:
        """æ­£è¦åŒ–é€†å¤‰æ›"""
        if self.scaler is None:
            return normalized_data
            
        if isinstance(normalized_data, torch.Tensor):
            normalized_data = normalized_data.cpu().numpy()
            
        return self.scaler.inverse_transform(normalized_data)


def load_experimental_data(
    data_path: str,
    config: Optional[Dict[str, Any]] = None,
    return_loaders: bool = False
) -> Union[Dict[str, torch.Tensor], Dict[str, DataLoader]]:
    """
    å®Ÿé¨“ç”¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    
    Args:
        data_path: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        config: è¨­å®šè¾æ›¸ï¼ˆtrain_ratio, val_ratio, normalizationç­‰ï¼‰
        return_loaders: DataLoaderã‚’è¿”ã™ã‹Tensorã‚’è¿”ã™ã‹
        
    Returns:
        ãƒ‡ãƒ¼ã‚¿è¾æ›¸ã¾ãŸã¯DataLoaderè¾æ›¸
    """
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
    default_config = {
        'train_ratio': 0.7,
        'val_ratio': 0.2,
        'test_ratio': 0.1,
        'normalization': 'standard',
        'handle_missing': 'interpolate',
        'batch_size': 32,
        'num_workers': 4,
        'pin_memory': True
    }
    
    if config:
        default_config.update(config)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    datasets = {}
    for split in ["train", "val", "test"]:
        datasets[split] = UniversalTimeSeriesDataset(
            data_path=data_path,
            split=split,
            train_ratio=default_config['train_ratio'],
            val_ratio=default_config['val_ratio'],
            test_ratio=default_config['test_ratio'],
            normalization=default_config['normalization'],
            handle_missing=default_config['handle_missing']
        )
    
    if return_loaders:
        # DataLoaderä½œæˆ
        loaders = {}
        for split, dataset in datasets.items():
            # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã¯ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ãªã„
            loaders[split] = DataLoader(
                dataset,
                batch_size=default_config['batch_size'],
                shuffle=False,  # æ™‚ç³»åˆ—é †ä¿æŒ
                num_workers=default_config['num_workers'],
                pin_memory=default_config['pin_memory']
            )
        return loaders
    else:
        # Tensorè¾æ›¸ä½œæˆ
        data_dict = {split: dataset.get_full_data() for split, dataset in datasets.items()}
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚å«ã‚ã‚‹
        data_dict['metadata'] = datasets['train'].metadata
        return data_dict


def create_data_loader_from_tensor(
    tensor: torch.Tensor,
    batch_size: int = 32,
    shuffle: bool = False,
    **dataloader_kwargs
) -> DataLoader:
    """
    Tensorã‹ã‚‰ç›´æ¥DataLoaderã‚’ä½œæˆ
    
    Args:
        tensor: ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ³ã‚½ãƒ« (T, d)
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
        shuffle: ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã™ã‚‹ã‹
        **dataloader_kwargs: DataLoaderè¿½åŠ å¼•æ•°
        
    Returns:
        DataLoader
    """
    
    class TensorDataset(Dataset):
        def __init__(self, data: torch.Tensor):
            self.data = data
            
        def __len__(self) -> int:
            return self.data.shape[0]
            
        def __getitem__(self, idx: int) -> torch.Tensor:
            return self.data[idx]
    
    dataset = TensorDataset(tensor)
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, **dataloader_kwargs)


# æ—¢å­˜äº’æ›æ€§ã®ãŸã‚ã®é–¢æ•°
def build_dataloaders(
    file_path: str,
    batch_size: int,
    split: str = "train",
    train_ratio: float = 0.7,
    val_ratio: float = 0.15,
    test_ratio: float = 0.15,
    num_workers: int = 4,
    pin_memory: bool = True
) -> DataLoader:
    """
    æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¨ã®äº’æ›æ€§ã®ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°
    """
    dataset = UniversalTimeSeriesDataset(
        data_path=file_path,
        split=split,
        train_ratio=train_ratio,
        val_ratio=val_ratio,
        test_ratio=test_ratio
    )
    
    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory
    )


class QuadImageDataset(Dataset):
    """
    ã‚¯ã‚¢ãƒƒãƒ‰ã‚³ãƒ—ã‚¿ãƒ¼ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆç”»åƒå†æ§‹æˆå°‚ç”¨ï¼‰
    ç”»åƒã®ã¿ã®npzãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œ: ç”»åƒè‡ªå·±å†æ§‹æˆå­¦ç¿’

    ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶:
    - ç”»åƒ: (T, H, W, C) = (1500, 48, 48, 1)

    ç”¨é€”:
    - image_reconstruction: ç”»åƒ â†’ æ½œåœ¨è¡¨ç¾ â†’ ç”»åƒå¾©å…ƒ
    """

    def __init__(
        self,
        data_path: str,
        split: str = "train",
        train_ratio: float = 0.7,
        val_ratio: float = 0.2,
        test_ratio: float = 0.1,
        image_normalization: str = "unit_scale",  # [0,255] â†’ [0,1]
        **kwargs
    ):
        """
        Args:
            data_path: ç”»åƒnpzãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (train_obs, test_obså«ã‚€)
            split: "train", "val", "test"
            train_ratio: è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ¯”ç‡
            val_ratio: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿æ¯”ç‡
            test_ratio: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ¯”ç‡
            image_normalization: ç”»åƒæ­£è¦åŒ–æ–¹æ³• ("unit_scale", "standard", "none")
        """
        super().__init__()

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¤œè¨¼
        if abs(train_ratio + val_ratio + test_ratio - 1.0) > 1e-6:
            raise DataLoaderError(f"åˆ†å‰²æ¯”ç‡ã®åˆè¨ˆãŒ1.0ã§ã‚ã‚Šã¾ã›ã‚“: {train_ratio + val_ratio + test_ratio}")

        if split not in ["train", "val", "test"]:
            raise ValueError(f"split must be 'train', 'val', or 'test'; got '{split}'")

        self.data_path = Path(data_path)
        self.split = split
        self.image_normalization = image_normalization

        # quad*.npzèª­ã¿è¾¼ã¿
        data = np.load(data_path)

        # ãƒ‡ãƒ¼ã‚¿é¸æŠï¼ˆtrain/testï¼‰- ç”»åƒã®ã¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¯¾å¿œ
        if split in ["train", "val"]:
            # trainãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã€å†…éƒ¨ã§train/valåˆ†å‰²
            self.images = data['train_obs']      # (T, H, W, C) = (1500, 48, 48, 1)
        else:  # test
            self.images = data['test_obs']       # (T, H, W, C) = (1500, 48, 48, 1)

        # print(f"[DEBUG] ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: images={self.images.shape}")  # Resolved in Step 7

        # æ™‚ç³»åˆ—åˆ†å‰²ï¼ˆtrainå†…ã§ã•ã‚‰ã«train/valåˆ†å‰²ï¼‰
        T = len(self.images)
        if split == "train":
            start_idx = 0
            end_idx = int(T * train_ratio)
        elif split == "val":
            start_idx = int(T * train_ratio)
            end_idx = int(T * (train_ratio + val_ratio))
        else:  # test
            start_idx = 0  # testãƒ‡ãƒ¼ã‚¿ã¯å…¨ä½“ã‚’ä½¿ç”¨
            end_idx = T

        self.images = self.images[start_idx:end_idx]

        # print(f"[DEBUG] {split}åˆ†å‰²å¾Œ: images={self.images.shape}")  # Resolved in Step 7

        # æ­£è¦åŒ–
        self._normalize_data()

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        self.metadata = DataMetadata(
            original_shape=(T, 48, 48, 1),
            feature_names=['image_features'],
            time_index=None,
            sampling_rate=None,
            missing_ratio=0.0,
            data_source=str(data_path),
            normalization_method=f"image:{image_normalization}",
            train_indices=(0, int(T * train_ratio)),
            val_indices=(int(T * train_ratio), int(T * (train_ratio + val_ratio))),
            test_indices=(int(T * (train_ratio + val_ratio)), T)
        )

    def _normalize_data(self):
        """ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–"""
        # ç”»åƒæ­£è¦åŒ–
        if self.image_normalization == "unit_scale":
            # [0, 255] â†’ [0, 1]
            self.images = self.images.astype(np.float32) / 255.0
        elif self.image_normalization == "standard":
            # æ¨™æº–åŒ–
            mean = self.images.mean()
            std = self.images.std()
            self.images = (self.images - mean) / (std + 1e-8)
        elif self.image_normalization == "none":
            self.images = self.images.astype(np.float32)

        # ç”»åƒã®ã¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ­£è¦åŒ–ä¸è¦

    def __getitem__(self, idx):
        """
        ãƒ‡ãƒ¼ã‚¿å–å¾—: ç”»åƒå†æ§‹æˆç”¨

        Returns:
            (image, image) - è‡ªå·±å†æ§‹æˆãƒšã‚¢
        """
        image = torch.FloatTensor(self.images[idx])      # (H, W, C) = (48, 48, 1)

        # ç”»åƒå†æ§‹æˆ: å…¥åŠ›ã¨å‡ºåŠ›ãŒåŒã˜
        return image, image

    def __len__(self):
        return len(self.images)

    def get_full_data(self):
        """åˆ†å‰²ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã‚’å–å¾—ï¼ˆæ™‚ç³»åˆ—é †ï¼‰"""
        images = torch.FloatTensor(self.images)    # (T, H, W, C)
        return images





def load_experimental_data_with_architecture(
    data_path: str,
    config: Dict[str, Any],
    split: str = "train",
    return_dataloaders: bool = False,
    experiment_mode: Optional[str] = None  # æ‰‹å‹•åˆ‡ã‚Šæ›¿ãˆå¯¾å¿œ
) -> Union[Dataset, Dict[str, Dataset], DataLoader, Dict[str, DataLoader]]:
    """
    ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«åŸºã¥ãçµ±ä¸€ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼é–¢æ•°ï¼ˆå¾Œæ–¹äº’æ›æ€§ä¿è¨¼ï¼‰

    Args:
        data_path: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        config: å®Ÿé¨“è¨­å®šï¼ˆmodel.encoder.type ã§åˆ¤å®šï¼‰
        split: ãƒ‡ãƒ¼ã‚¿åˆ†å‰² ("train" | "val" | "test" | "all")
        return_dataloaders: DataLoaderã‚’è¿”ã™ã‹Datasetã‚’è¿”ã™ã‹
        experiment_mode: å®Ÿé¨“ãƒ¢ãƒ¼ãƒ‰ ("reconstruction" | "target_prediction")
                        Noneã®å ´åˆã¯config.experiment.modeã‹ã‚‰è‡ªå‹•å–å¾—

    Returns:
        Dataset/DataLoader: é©åˆ‡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ/ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚¯ãƒ©ã‚¹
    """
    # ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚¿ã‚¤ãƒ—åˆ¤å®š
    encoder_type = config.get('model', {}).get('encoder', {}).get('type', 'time_invariant')
    data_config = config.get('data', {})

    # å®Ÿé¨“ãƒ¢ãƒ¼ãƒ‰åˆ¤å®šï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è‡ªå‹•å–å¾— or æ‰‹å‹•æŒ‡å®šï¼‰
    if experiment_mode is None:
        experiment_mode = config.get('experiment', {}).get('mode', 'reconstruction')

    print(f"ğŸ“‹ å®Ÿé¨“ãƒ¢ãƒ¼ãƒ‰: {experiment_mode} (encoder: {encoder_type})")

    if encoder_type == "rkn":
        # â˜…ç”»åƒãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ï¼ˆUniversalTimeSeriesDatasetä½¿ç”¨ã§çµ±ä¸€ï¼‰

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ»DataLoaderãƒ»ãƒ¢ãƒ‡ãƒ« ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ†é›¢
        # å•é¡Œ: config.data ã«ã¯ Datasetç”¨ã€DataLoaderç”¨ã€ãƒ¢ãƒ‡ãƒ«ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæ··åœ¨
        # åŸå› : UniversalTimeSeriesDataset.__init__() ãŒå—ã‘å–ã‚‰ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ TypeError ç™ºç”Ÿ
        # è§£æ±º: Datasetä½œæˆæ™‚ã¯ Datasetç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ã‚’æ¸¡ã—ã€ä»–ã¯å„ç”¨é€”ã§å¾Œä½¿ç”¨
        # é™¤å¤–å¯¾è±¡:
        #   - DataLoaderç”¨: batch_size, num_workers, pin_memory
        #   - ãƒ¢ãƒ‡ãƒ«ç”¨: image_shape, target_shape (ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰æ™‚ã«å‚ç…§)
        dataset_params = {k: v for k, v in data_config.items()
                         if k not in ['batch_size', 'num_workers', 'pin_memory',
                                     'image_shape', 'target_shape']}

        if split == "all":
            # å…¨åˆ†å‰²ã‚’è¿”ã™
            datasets = {}
            for s in ["train", "val", "test"]:
                datasets[s] = UniversalTimeSeriesDataset(
                    data_path=data_path,
                    split=s,
                    experiment_mode=experiment_mode,  # å®Ÿé¨“ãƒ¢ãƒ¼ãƒ‰ã‚’æ¸¡ã™
                    **dataset_params  # Datasetç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿æ¸¡ã™ï¼ˆbatch_sizeç­‰ã‚’é™¤å¤–ï¼‰
                )

            if return_dataloaders:
                # DataLoaderè¾æ›¸ã‚’è¿”ã™
                loaders = {}
                batch_size = data_config.get('batch_size', 16)  # ç”»åƒç”¨ã«å°ã•ã‚
                for s, dataset in datasets.items():
                    loaders[s] = DataLoader(
                        dataset,
                        batch_size=batch_size,
                        shuffle=False,  # æ™‚ç³»åˆ—é †åºä¿æŒã®ãŸã‚å…¨åˆ†å‰²ã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«ç„¡åŠ¹
                        num_workers=data_config.get('num_workers', 4),
                        pin_memory=data_config.get('pin_memory', True)
                    )
                return loaders
            else:
                return datasets
        else:
            # å˜ä¸€åˆ†å‰²ã‚’è¿”ã™
            dataset = UniversalTimeSeriesDataset(
                data_path=data_path,
                split=split,
                experiment_mode=experiment_mode,  # å®Ÿé¨“ãƒ¢ãƒ¼ãƒ‰ã‚’æ¸¡ã™
                **dataset_params  # Datasetç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿æ¸¡ã™ï¼ˆbatch_sizeç­‰ã‚’é™¤å¤–ï¼‰
            )

            if return_dataloaders:
                batch_size = data_config.get('batch_size', 16)
                return DataLoader(
                    dataset,
                    batch_size=batch_size,
                    shuffle=False,  # æ™‚ç³»åˆ—é †åºä¿æŒã®ãŸã‚å…¨åˆ†å‰²ã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«ç„¡åŠ¹
                    num_workers=data_config.get('num_workers', 4),
                    pin_memory=data_config.get('pin_memory', True)
                )
            else:
                return dataset

    elif encoder_type in ["time_invariant", "tcn"]:
        # æ—¢å­˜ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ï¼ˆå¾Œæ–¹äº’æ›æ€§ä¿è¨¼ï¼‰

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ»DataLoaderãƒ»ãƒ¢ãƒ‡ãƒ« ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ†é›¢ï¼ˆæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ç”¨ã‚‚åŒæ§˜ã®å‡¦ç†ï¼‰
        dataset_params = {k: v for k, v in data_config.items()
                         if k not in ['batch_size', 'num_workers', 'pin_memory',
                                     'image_shape', 'target_shape']}

        if split == "all":
            datasets = {}
            for s in ["train", "val", "test"]:
                datasets[s] = UniversalTimeSeriesDataset(
                    data_path=data_path,
                    split=s,
                    experiment_mode=experiment_mode,  # å®Ÿé¨“ãƒ¢ãƒ¼ãƒ‰ã‚’æ¸¡ã™
                    **dataset_params  # Datasetç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿æ¸¡ã™
                )

            if return_dataloaders:
                loaders = {}
                batch_size = data_config.get('batch_size', 32)
                for s, dataset in datasets.items():
                    loaders[s] = DataLoader(
                        dataset,
                        batch_size=batch_size,
                        shuffle=False,  # æ™‚ç³»åˆ—é †åºä¿æŒã®ãŸã‚å…¨åˆ†å‰²ã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«ç„¡åŠ¹
                        num_workers=data_config.get('num_workers', 4),
                        pin_memory=data_config.get('pin_memory', True)
                    )
                return loaders
            else:
                return datasets
        else:
            dataset = UniversalTimeSeriesDataset(
                data_path=data_path,
                split=split,
                experiment_mode=experiment_mode,  # å®Ÿé¨“ãƒ¢ãƒ¼ãƒ‰ã‚’æ¸¡ã™
                **dataset_params  # Datasetç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿æ¸¡ã™
            )

            if return_dataloaders:
                batch_size = data_config.get('batch_size', 32)
                return DataLoader(
                    dataset,
                    batch_size=batch_size,
                    shuffle=False,  # æ™‚ç³»åˆ—é †åºä¿æŒã®ãŸã‚å…¨åˆ†å‰²ã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«ç„¡åŠ¹
                    num_workers=data_config.get('num_workers', 4),
                    pin_memory=data_config.get('pin_memory', True)
                )
            else:
                return dataset
    else:
        raise ValueError(f"Unknown encoder type: {encoder_type}")


# æ—¢å­˜é–¢æ•°ã®å¾Œæ–¹äº’æ›æ€§ãƒ©ãƒƒãƒ‘ãƒ¼
def load_experimental_data(
    data_path: str,
    config: Optional[Dict[str, Any]] = None,
    split: str = "all",
    return_dataloaders: bool = False
) -> Union[Dict[str, torch.Tensor], Dict[str, Dataset], Dict[str, DataLoader]]:
    """
    å¾Œæ–¹äº’æ›æ€§ä¿è¨¼ã®ãŸã‚ã®æ—¢å­˜é–¢æ•°ãƒ©ãƒƒãƒ‘ãƒ¼

    Args:
        data_path: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        config: å®Ÿé¨“è¨­å®šï¼ˆNoneã®å ´åˆã¯ time_invariant ã¨ä»®å®šï¼‰
        split: ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
        return_dataloaders: DataLoaderã‚’è¿”ã™ã‹

    Returns:
        ãƒ‡ãƒ¼ã‚¿è¾æ›¸: æ—¢å­˜å½¢å¼ã¨ã®äº’æ›æ€§ã‚’ä¿è¨¼
    """
    if config is None:
        # æ—¢å­˜ã®å‹•ä½œ: time_invariantå‰æ
        config = {
            'model': {'encoder': {'type': 'time_invariant'}},
            'data': {'batch_size': 32}
        }

    # æ–°ã—ã„çµ±ä¸€é–¢æ•°ã‚’ä½¿ç”¨
    result = load_experimental_data_with_architecture(
        data_path=data_path,
        config=config,
        split=split,
        return_dataloaders=return_dataloaders
    )

    # æ—¢å­˜å½¢å¼ã¸ã®å¤‰æ›ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
    if not return_dataloaders and split == "all" and isinstance(result, dict):
        # Datasetè¾æ›¸ â†’ Tensorè¾æ›¸å¤‰æ›ï¼ˆæ—¢å­˜äº’æ›æ€§ï¼‰
        if all(hasattr(dataset, 'get_full_data') for dataset in result.values()):
            tensor_dict = {}
            for s, dataset in result.items():
                tensor_dict[s] = dataset.get_full_data()
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚å«ã‚ã‚‹
            tensor_dict['metadata'] = result['train'].metadata
            return tensor_dict

    return result


if __name__ == "__main__":
    # ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
    print("çµ±ä¸€ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ãƒ†ã‚¹ãƒˆ")
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
    test_data = np.random.randn(100, 5)
    test_path = "test_data.npy"
    np.save(test_path, test_data)
    
    try:
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
        data_dict = load_experimental_data(test_path)
        
        print("ãƒ‡ãƒ¼ã‚¿åˆ†å‰²çµæœ:")
        for split, data in data_dict.items():
            if isinstance(data, torch.Tensor):
                print(f"  {split}: {data.shape}")
            else:
                print(f"  {split}: {type(data)}")
        
        print("\nçµ±ä¸€ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ãƒ†ã‚¹ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸ")
        
    finally:
        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        if os.path.exists(test_path):
            os.remove(test_path)