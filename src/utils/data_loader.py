# src/utils/data_loader.py
"""
ã‚¿ã‚¹ã‚¯3ç”¨çµ±ä¸€ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼

æ©Ÿèƒ½:
- å¤šæ§˜ãªãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã®çµ±ä¸€èª­ã¿è¾¼ã¿ (.npz, .npy, .csv, .json)
- ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ãƒ»å‰å‡¦ç†
- æ­£è¦åŒ–ãƒ»æ¨™æº–åŒ– 
- æ™‚ç³»åˆ—é †ã‚’ä¿ã£ãŸå­¦ç¿’/æ¤œè¨¼/ãƒ†ã‚¹ãƒˆåˆ†å‰²
- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†

å‰æãƒ‡ãƒ¼ã‚¿å‹: (T, d) å¤šå¤‰é‡æ™‚ç³»åˆ— torch.FloatTensor
"""

import os
import json
import warnings
from pathlib import Path
from typing import Dict, Any, Optional, Union, List, Tuple
from dataclasses import dataclass, asdict

import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler, MinMaxScaler


@dataclass
class DataMetadata:
    """ãƒ‡ãƒ¼ã‚¿ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿"""
    original_shape: Tuple[int, int]
    feature_names: Optional[List[str]]
    time_index: Optional[List]
    sampling_rate: Optional[float]
    missing_ratio: float
    data_source: str
    normalization_method: str
    train_indices: Tuple[int, int]
    val_indices: Tuple[int, int]
    test_indices: Tuple[int, int]


class DataLoaderError(Exception):
    """ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼å°‚ç”¨ä¾‹å¤–"""
    pass


class UniversalTimeSeriesDataset(Dataset):
    """
    çµ±ä¸€æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
    
    ç‰¹å¾´:
    - æ™‚ç³»åˆ—é †ä¿æŒåˆ†å‰²
    - è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼å¯¾å¿œ
    - è‡ªå‹•æ­£è¦åŒ–
    - æ¬ æå€¤å‡¦ç†
    """
    
    def __init__(
        self,
        data_path: str,
        split: str = "train",
        train_ratio: float = 0.7,
        val_ratio: float = 0.2,
        test_ratio: float = 0.1,
        normalization: str = "standard",
        handle_missing: str = "interpolate",
        feature_names: Optional[List[str]] = None
    ):
        """
        Args:
            data_path: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            split: "train", "val", "test"
            train_ratio: è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ¯”ç‡
            val_ratio: æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿æ¯”ç‡ 
            test_ratio: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ¯”ç‡
            normalization: "standard", "minmax", "none"
            handle_missing: "interpolate", "forward_fill", "remove"
            feature_names: ç‰¹å¾´é‡åãƒªã‚¹ãƒˆ
        """
        super().__init__()
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¤œè¨¼
        if abs(train_ratio + val_ratio + test_ratio - 1.0) > 1e-6:
            raise DataLoaderError(f"åˆ†å‰²æ¯”ç‡ã®åˆè¨ˆãŒ1.0ã§ã‚ã‚Šã¾ã›ã‚“: {train_ratio + val_ratio + test_ratio}")
        
        if split not in ["train", "val", "test"]:
            raise ValueError(f"split must be 'train', 'val', or 'test'; got '{split}'")
            
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        self.data_path = Path(data_path)
        self.split = split
        self.normalization = normalization
        
        # ç”Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        raw_data = self._load_raw_data()
        
        # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãƒ»ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        cleaned_data = self._validate_and_clean(raw_data, handle_missing)
        
        # æ­£è¦åŒ–
        normalized_data, self.scaler = self._normalize_data(cleaned_data, normalization)
        
        # æ™‚ç³»åˆ—é †åˆ†å‰²
        split_data = self._split_time_series(normalized_data, train_ratio, val_ratio, test_ratio)
        
        # åˆ†å‰²ãƒ‡ãƒ¼ã‚¿å–å¾—
        self.data = split_data[split]
        self.length = self.data.shape[0]
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        self._create_metadata(raw_data, split_data, feature_names)
    
    def _load_raw_data(self) -> np.ndarray:
        """ç”Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
        if not self.data_path.exists():
            raise FileNotFoundError(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {self.data_path}")
        
        ext = self.data_path.suffix.lower()
        
        try:
            if ext == ".npz":
                data = np.load(self.data_path)
                # æŸ”è»Ÿãªã‚­ãƒ¼æ¢ç´¢: å„ªå…ˆåº¦é †ã§é©åˆ‡ãªãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•é¸æŠ
                candidate_keys = ['Y', 'X', 'data', 'arr_0']
                raw_data = None

                # å„ªå…ˆåº¦é †ã§ã‚­ãƒ¼ã‚’æ¢ç´¢
                for key in candidate_keys:
                    if key in data:
                        candidate = data[key]
                        # 2æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã‹ã¤æ™‚ç³»åˆ—ã¨ã—ã¦é©åˆ‡ãªã‚µã‚¤ã‚ºã‹ãƒã‚§ãƒƒã‚¯
                        if candidate.ndim == 2 and candidate.shape[0] > 1:
                            raw_data = candidate
                            print(f"ğŸ’¡ npzãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚­ãƒ¼ '{key}' ã‚’ä½¿ç”¨: shape={candidate.shape}")
                            break

                # å„ªå…ˆã‚­ãƒ¼ãŒãªã„å ´åˆã€åˆ©ç”¨å¯èƒ½ãªå…¨ã‚­ãƒ¼ã‹ã‚‰æœ€é©ãªã‚‚ã®ã‚’é¸æŠ
                if raw_data is None:
                    available_keys = list(data.keys())
                    for key in available_keys:
                        candidate = data[key]
                        # 2æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã§æ™‚ç³»åˆ—ã¨ã—ã¦å¦¥å½“ãªã‚µã‚¤ã‚º
                        if (hasattr(candidate, 'ndim') and candidate.ndim == 2 and
                            candidate.shape[0] > 1 and candidate.shape[1] > 0):
                            raw_data = candidate
                            print(f"ğŸ’¡ npzãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ¨å®šã‚­ãƒ¼ '{key}' ã‚’ä½¿ç”¨: shape={candidate.shape}")
                            break

                # ãã‚Œã§ã‚‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼
                if raw_data is None:
                    available_info = []
                    for key in data.keys():
                        try:
                            shape = data[key].shape if hasattr(data[key], 'shape') else 'scalar'
                            dtype = data[key].dtype if hasattr(data[key], 'dtype') else type(data[key])
                            available_info.append(f"'{key}': shape={shape}, dtype={dtype}")
                        except:
                            available_info.append(f"'{key}': (èª­ã¿è¾¼ã¿ä¸å¯)")

                    raise DataLoaderError(
                        f"npzãƒ•ã‚¡ã‚¤ãƒ«ã«é©åˆ‡ãª2æ¬¡å…ƒæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n"
                        f"åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿: {', '.join(available_info)}\n"
                        f"æœŸå¾…ã•ã‚Œã‚‹å½¢å¼: (æ™‚ç³»åˆ—é•·, ç‰¹å¾´æ¬¡å…ƒ) ã®2æ¬¡å…ƒé…åˆ—"
                    )
                    
            elif ext == ".npy":
                raw_data = np.load(self.data_path)

                # npyãƒ•ã‚¡ã‚¤ãƒ«ã®æŸ”è»Ÿãªå½¢çŠ¶å¯¾å¿œ
                if raw_data.ndim == 1:
                    # 1æ¬¡å…ƒã®å ´åˆã¯å˜å¤‰é‡æ™‚ç³»åˆ—ã¨ã—ã¦æ‰±ã†
                    raw_data = raw_data.reshape(-1, 1)
                    print(f"ğŸ’¡ npyãƒ•ã‚¡ã‚¤ãƒ«: 1æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã‚’2æ¬¡å…ƒã«å¤‰æ› shape={raw_data.shape}")
                elif raw_data.ndim > 2:
                    # 3æ¬¡å…ƒä»¥ä¸Šã®å ´åˆã¯æœ€åˆã®2æ¬¡å…ƒã‚’ä½¿ç”¨
                    original_shape = raw_data.shape
                    raw_data = raw_data.reshape(raw_data.shape[0], -1)
                    print(f"ğŸ’¡ npyãƒ•ã‚¡ã‚¤ãƒ«: {original_shape} â†’ {raw_data.shape} ã«å¤‰æ›")
                elif raw_data.ndim == 2:
                    print(f"ğŸ’¡ npyãƒ•ã‚¡ã‚¤ãƒ«: 2æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ shape={raw_data.shape}")
                else:
                    raise DataLoaderError(f"npyãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ãŒ0æ¬¡å…ƒã§ã™: shape={raw_data.shape}")
                
            elif ext == ".csv":
                df = pd.read_csv(self.data_path, index_col=0 if 'time' in pd.read_csv(self.data_path, nrows=1).columns else None)
                raw_data = df.values
                self._csv_feature_names = df.columns.tolist()
                
            elif ext == ".json":
                with open(self.data_path, 'r') as f:
                    json_data = json.load(f)
                if 'data' in json_data:
                    raw_data = np.array(json_data['data'])
                    self._json_metadata = {k: v for k, v in json_data.items() if k != 'data'}
                else:
                    raw_data = np.array(json_data)
                    
            else:
                raise DataLoaderError(f"å¯¾å¿œã—ã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {ext}. å¯¾å¿œå½¢å¼: .npz, .npy, .csv, .json")
                
        except Exception as e:
            raise DataLoaderError(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({self.data_path}): {e}")
        
        return raw_data
    
    def _validate_and_clean(self, data: np.ndarray, handle_missing: str) -> np.ndarray:
        """ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ãƒ»ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        # å½¢çŠ¶ãƒã‚§ãƒƒã‚¯
        if data.ndim != 2:
            if data.ndim == 1:
                warnings.warn("1æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã‚’2æ¬¡å…ƒã«å¤‰æ›ã—ã¾ã™")
                data = data.reshape(-1, 1)
            else:
                raise DataLoaderError(f"ãƒ‡ãƒ¼ã‚¿ã¯(T, d)ã®2æ¬¡å…ƒé…åˆ—ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚å®Ÿéš›ã®å½¢çŠ¶: {data.shape}")
        
        T, d = data.shape
        if T < 10:
            warnings.warn(f"ãƒ‡ãƒ¼ã‚¿é•·ãŒçŸ­ã™ãã¾ã™: T={T}. æœ€ä½10ä»¥ä¸Šæ¨å¥¨")
        
        # æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
        missing_mask = np.isnan(data) | np.isinf(data)
        missing_ratio = missing_mask.sum() / data.size
        
        if missing_ratio > 0:
            warnings.warn(f"æ¬ æå€¤ã‚’æ¤œå‡º: {missing_ratio:.1%}")
            
            if handle_missing == "interpolate":
                # ç·šå½¢è£œé–“
                for j in range(d):
                    col_data = data[:, j]
                    missing_idx = np.isnan(col_data) | np.isinf(col_data)
                    if missing_idx.any():
                        valid_idx = ~missing_idx
                        if valid_idx.sum() > 1:
                            col_data[missing_idx] = np.interp(
                                np.where(missing_idx)[0],
                                np.where(valid_idx)[0],
                                col_data[valid_idx]
                            )
                        else:
                            col_data[missing_idx] = 0.0
                            
            elif handle_missing == "forward_fill":
                data = pd.DataFrame(data).fillna(method='ffill').fillna(method='bfill').values
                
            elif handle_missing == "remove":
                valid_rows = ~missing_mask.any(axis=1)
                data = data[valid_rows]
                warnings.warn(f"æ¬ æè¡Œã‚’å‰Šé™¤: {T} -> {data.shape[0]} è¡Œ")
            
            # ç„¡é™å¤§ãƒ»NaNã®æœ€çµ‚ãƒã‚§ãƒƒã‚¯
            if np.isnan(data).any() or np.isinf(data).any():
                warnings.warn("æ¬ æå€¤å‡¦ç†å¾Œã«ã‚‚NaN/InfãŒæ®‹å­˜ã€‚0ã§ç½®æ›ã—ã¾ã™ã€‚")
                data = np.nan_to_num(data, nan=0.0, posinf=1e6, neginf=-1e6)
        
        self.missing_ratio = missing_ratio
        return data
    
    def _normalize_data(self, data: np.ndarray, method: str) -> Tuple[np.ndarray, Optional[object]]:
        """ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–"""
        if method == "none":
            return data, None
            
        elif method == "standard":
            scaler = StandardScaler()
            normalized = scaler.fit_transform(data)
            
        elif method == "minmax":
            scaler = MinMaxScaler()
            normalized = scaler.fit_transform(data)
            
        else:
            raise DataLoaderError(f"å¯¾å¿œã—ã¦ã„ãªã„æ­£è¦åŒ–æ–¹æ³•: {method}. ä½¿ç”¨å¯èƒ½: 'standard', 'minmax', 'none'")
        
        return normalized, scaler
    
    def _split_time_series(self, data: np.ndarray, train_ratio: float, val_ratio: float, test_ratio: float) -> Dict[str, np.ndarray]:
        """æ™‚ç³»åˆ—é †åˆ†å‰²"""
        T = data.shape[0]
        
        # åˆ†å‰²ç‚¹è¨ˆç®—
        train_end = int(train_ratio * T)
        val_end = int((train_ratio + val_ratio) * T)
        
        splits = {
            "train": data[:train_end],
            "val": data[train_end:val_end],
            "test": data[val_end:]
        }
        
        # åˆ†å‰²ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨˜éŒ²
        self.split_indices = {
            "train": (0, train_end),
            "val": (train_end, val_end),
            "test": (val_end, T)
        }
        
        return splits
    
    def _create_metadata(self, raw_data: np.ndarray, split_data: Dict[str, np.ndarray], feature_names: Optional[List[str]]):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä½œæˆ"""
        # ç‰¹å¾´é‡åã®æ±ºå®š
        if feature_names:
            final_feature_names = feature_names
        elif hasattr(self, '_csv_feature_names'):
            final_feature_names = self._csv_feature_names
        else:
            final_feature_names = [f"feature_{i}" for i in range(raw_data.shape[1])]
        
        self.metadata = DataMetadata(
            original_shape=raw_data.shape,
            feature_names=final_feature_names,
            time_index=None,  # TODO: å¿…è¦ã«å¿œã˜ã¦å®Ÿè£…
            sampling_rate=None,
            missing_ratio=self.missing_ratio,
            data_source=str(self.data_path),
            normalization_method=self.normalization,
            train_indices=self.split_indices["train"],
            val_indices=self.split_indices["val"],
            test_indices=self.split_indices["test"]
        )
    
    def __len__(self) -> int:
        return self.length
    
    def __getitem__(self, idx: int) -> torch.Tensor:
        """ãƒ‡ãƒ¼ã‚¿å–å¾—"""
        sample = self.data[idx]  # shape: (d,)
        return torch.from_numpy(sample).float()
    
    def get_full_data(self) -> torch.Tensor:
        """åˆ†å‰²ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã‚’å–å¾—"""
        return torch.from_numpy(self.data).float()
    
    def inverse_transform(self, normalized_data: Union[torch.Tensor, np.ndarray]) -> np.ndarray:
        """æ­£è¦åŒ–é€†å¤‰æ›"""
        if self.scaler is None:
            return normalized_data
            
        if isinstance(normalized_data, torch.Tensor):
            normalized_data = normalized_data.cpu().numpy()
            
        return self.scaler.inverse_transform(normalized_data)


def load_experimental_data(
    data_path: str,
    config: Optional[Dict[str, Any]] = None,
    return_loaders: bool = False
) -> Union[Dict[str, torch.Tensor], Dict[str, DataLoader]]:
    """
    å®Ÿé¨“ç”¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    
    Args:
        data_path: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        config: è¨­å®šè¾æ›¸ï¼ˆtrain_ratio, val_ratio, normalizationç­‰ï¼‰
        return_loaders: DataLoaderã‚’è¿”ã™ã‹Tensorã‚’è¿”ã™ã‹
        
    Returns:
        ãƒ‡ãƒ¼ã‚¿è¾æ›¸ã¾ãŸã¯DataLoaderè¾æ›¸
    """
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
    default_config = {
        'train_ratio': 0.7,
        'val_ratio': 0.2,
        'test_ratio': 0.1,
        'normalization': 'standard',
        'handle_missing': 'interpolate',
        'batch_size': 32,
        'num_workers': 4,
        'pin_memory': True
    }
    
    if config:
        default_config.update(config)
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    datasets = {}
    for split in ["train", "val", "test"]:
        datasets[split] = UniversalTimeSeriesDataset(
            data_path=data_path,
            split=split,
            train_ratio=default_config['train_ratio'],
            val_ratio=default_config['val_ratio'],
            test_ratio=default_config['test_ratio'],
            normalization=default_config['normalization'],
            handle_missing=default_config['handle_missing']
        )
    
    if return_loaders:
        # DataLoaderä½œæˆ
        loaders = {}
        for split, dataset in datasets.items():
            # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã¯ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ãªã„
            loaders[split] = DataLoader(
                dataset,
                batch_size=default_config['batch_size'],
                shuffle=False,  # æ™‚ç³»åˆ—é †ä¿æŒ
                num_workers=default_config['num_workers'],
                pin_memory=default_config['pin_memory']
            )
        return loaders
    else:
        # Tensorè¾æ›¸ä½œæˆ
        data_dict = {split: dataset.get_full_data() for split, dataset in datasets.items()}
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚å«ã‚ã‚‹
        data_dict['metadata'] = datasets['train'].metadata
        return data_dict


def create_data_loader_from_tensor(
    tensor: torch.Tensor,
    batch_size: int = 32,
    shuffle: bool = False,
    **dataloader_kwargs
) -> DataLoader:
    """
    Tensorã‹ã‚‰ç›´æ¥DataLoaderã‚’ä½œæˆ
    
    Args:
        tensor: ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ³ã‚½ãƒ« (T, d)
        batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
        shuffle: ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã™ã‚‹ã‹
        **dataloader_kwargs: DataLoaderè¿½åŠ å¼•æ•°
        
    Returns:
        DataLoader
    """
    
    class TensorDataset(Dataset):
        def __init__(self, data: torch.Tensor):
            self.data = data
            
        def __len__(self) -> int:
            return self.data.shape[0]
            
        def __getitem__(self, idx: int) -> torch.Tensor:
            return self.data[idx]
    
    dataset = TensorDataset(tensor)
    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, **dataloader_kwargs)


# æ—¢å­˜äº’æ›æ€§ã®ãŸã‚ã®é–¢æ•°
def build_dataloaders(
    file_path: str,
    batch_size: int,
    split: str = "train",
    train_ratio: float = 0.7,
    val_ratio: float = 0.15,
    test_ratio: float = 0.15,
    num_workers: int = 4,
    pin_memory: bool = True
) -> DataLoader:
    """
    æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¨ã®äº’æ›æ€§ã®ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°
    """
    dataset = UniversalTimeSeriesDataset(
        data_path=file_path,
        split=split,
        train_ratio=train_ratio,
        val_ratio=val_ratio,
        test_ratio=test_ratio
    )
    
    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=pin_memory
    )


if __name__ == "__main__":
    # ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
    print("çµ±ä¸€ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ãƒ†ã‚¹ãƒˆ")
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
    test_data = np.random.randn(100, 5)
    test_path = "test_data.npy"
    np.save(test_path, test_data)
    
    try:
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
        data_dict = load_experimental_data(test_path)
        
        print("ãƒ‡ãƒ¼ã‚¿åˆ†å‰²çµæœ:")
        for split, data in data_dict.items():
            if isinstance(data, torch.Tensor):
                print(f"  {split}: {data.shape}")
            else:
                print(f"  {split}: {type(data)}")
        
        print("\nâœ… çµ±ä¸€ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ãƒ†ã‚¹ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸ")
        
    finally:
        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        if os.path.exists(test_path):
            os.remove(test_path)