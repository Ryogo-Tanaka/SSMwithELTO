"""
ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

DFIV Kalman Filterã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½ã‚’è©³ç´°ã«åˆ†æã—ã€
ã‚¿ãƒ¼ãƒŸãƒŠãƒ«å‡ºåŠ›ã¨æ•°å€¤ä¿å­˜ã‚’è¡Œã†ã€‚
"""

import torch
import numpy as np
import json
import csv
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union, Any
from datetime import datetime
import warnings

from .metrics import StateEstimationMetrics, ComputationalMetrics, CalibrationMetrics


class FilteringAnalyzer:
    """ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½ã®åŒ…æ‹¬çš„åˆ†æã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, output_dir: str, device: str = 'cpu'):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.device = torch.device(device)
        
        # åˆ†æçµæœä¿å­˜ç”¨
        self.analysis_results = {}
        self.experiment_metadata = {}
        
        # è©•ä¾¡å™¨ã®åˆæœŸåŒ–
        self.metrics_evaluator = StateEstimationMetrics(device=str(device))
        self.computational_metrics = ComputationalMetrics()
        
    def analyze_filtering_performance(
        self,
        inference_model,
        test_data: torch.Tensor,
        true_states: Optional[torch.Tensor] = None,
        experiment_name: str = "filtering_experiment",
        save_results: bool = True,
        verbose: bool = True
    ) -> Dict[str, Any]:
        """
        ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½ã®åŒ…æ‹¬çš„åˆ†æ
        
        Args:
            inference_model: æ¨è«–ãƒ¢ãƒ‡ãƒ«
            test_data: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ (T, n)
            true_states: çœŸã®çŠ¶æ…‹ (T, r) [optional]
            experiment_name: å®Ÿé¨“å
            save_results: çµæœã‚’ä¿å­˜ã™ã‚‹ã‹
            verbose: è©³ç´°å‡ºåŠ›ã™ã‚‹ã‹
            
        Returns:
            Dict: åˆ†æçµæœ
        """
        if verbose:
            print(f"\nğŸš€ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½åˆ†æé–‹å§‹: {experiment_name}")
            print("="*60)
            
        analysis_start_time = datetime.now()
        
        # 1. ãƒãƒƒãƒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œãƒ»åˆ†æ
        batch_results = self._analyze_batch_filtering(
            inference_model, test_data, true_states, verbose
        )
        
        # 2. ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œãƒ»åˆ†æ  
        online_results = self._analyze_online_filtering(
            inference_model, test_data, true_states, verbose
        )
        
        # 3. è¨ˆç®—åŠ¹ç‡åˆ†æ
        efficiency_results = self._analyze_computational_efficiency(
            inference_model, test_data, verbose
        )
        
        # 4. æ¯”è¼ƒåˆ†æï¼ˆãƒãƒƒãƒ vs ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ï¼‰
        comparison_results = self._compare_filtering_methods(
            batch_results, online_results, verbose
        )
        
        # 5. çµæœçµ±åˆ
        complete_analysis = {
            'experiment_info': {
                'name': experiment_name,
                'timestamp': analysis_start_time.isoformat(),
                'data_shape': list(test_data.shape),
                'has_true_states': true_states is not None,
                'device': str(self.device)
            },
            'batch_filtering': batch_results,
            'online_filtering': online_results,
            'computational_efficiency': efficiency_results,
            'method_comparison': comparison_results
        }
        
        # 6. çµæœä¿å­˜
        if save_results:
            self._save_analysis_results(complete_analysis, experiment_name)
            
        if verbose:
            print(f"\nâœ… åˆ†æå®Œäº†: {experiment_name}")
            print(f"ğŸ“ çµæœä¿å­˜å…ˆ: {self.output_dir}")
            
        return complete_analysis
    
    def _analyze_batch_filtering(
        self,
        inference_model,
        test_data: torch.Tensor,
        true_states: Optional[torch.Tensor],
        verbose: bool
    ) -> Dict[str, Any]:
        """ãƒãƒƒãƒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®åˆ†æ"""
        if verbose:
            print("\nğŸ“Š ãƒãƒƒãƒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åˆ†æ...")
            
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
        start_time = datetime.now()
        
        try:
            # æ¨è«–å®Ÿè¡Œï¼ˆé˜²å¾¡çš„ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
            if hasattr(inference_model, 'filter_sequence'):
                filtering_result = inference_model.filter_sequence(
                    test_data, return_likelihood=True
                )
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: inference_batchã‚’ä½¿ç”¨
                batch_result = inference_model.inference_batch(test_data, return_format='dict')
                X_means = torch.tensor(batch_result['summary']['mean_trajectory'])
                X_covariances = torch.tensor(batch_result['summary']['covariance_trajectory'])
                if 'likelihood' in batch_result['statistics']:
                    likelihoods = torch.tensor(batch_result['statistics']['likelihood']['likelihood_trajectory'])
                    filtering_result = (X_means, X_covariances, likelihoods)
                else:
                    filtering_result = (X_means, X_covariances)
            
            if len(filtering_result) == 3:
                X_means, X_covariances, likelihoods = filtering_result
            else:
                X_means, X_covariances = filtering_result
                likelihoods = None
                
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()
            
            if verbose:
                print(f"  âœ… ãƒãƒƒãƒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Œäº† ({processing_time:.2f}ç§’)")
                print(f"  ğŸ“ æ¨å®šçŠ¶æ…‹å½¢çŠ¶: {X_means.shape}")
                print(f"  ğŸ“ å…±åˆ†æ•£å½¢çŠ¶: {X_covariances.shape}")
                
        except Exception as e:
            import traceback
            error_details = {
                'error_message': str(e),
                'error_type': type(e).__name__,
                'full_traceback': traceback.format_exc(),
                'inference_model_type': type(inference_model).__name__,
                'available_methods': [m for m in dir(inference_model) if not m.startswith('_')],
                'filter_methods': [m for m in dir(inference_model) if not m.startswith('_') and 'filter' in m.lower()],
                'test_data_shape': list(test_data.shape),
                'test_data_type': str(test_data.dtype),
                'model_setup_status': getattr(inference_model, 'is_setup', 'unknown')
            }

            if verbose:
                print(f"  âŒ ãƒãƒƒãƒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
                print(f"  ğŸ” ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}")
                print(f"  ğŸ“Š ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {test_data.shape} (dtype: {test_data.dtype})")
                print(f"  ğŸ¯ ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {type(inference_model).__name__}")
                print(f"  ğŸ”§ åˆ©ç”¨å¯èƒ½ãªfilterãƒ¡ã‚½ãƒƒãƒ‰: {error_details['filter_methods']}")
                print(f"  âš™ï¸  ãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—çŠ¶æ³: {error_details['model_setup_status']}")
                print(f"  ğŸ“ è©³ç´°ãƒˆãƒ¬ãƒ¼ã‚¹:\n{traceback.format_exc()}")

            return {
                'error': str(e),
                'success': False,
                'error_details': error_details
            }
        
        # æ€§èƒ½è©•ä¾¡
        metrics = self.metrics_evaluator.compute_all_metrics(
            X_means, true_states, X_covariances, test_data, likelihoods, verbose=False
        )
        
        if verbose:
            self._print_batch_summary(metrics, processing_time)
            
        return {
            'success': True,
            'processing_time': processing_time,
            'estimated_states': X_means,
            'covariances': X_covariances,
            'likelihoods': likelihoods,
            'metrics': metrics
        }
    
    def _analyze_online_filtering(
        self,
        inference_model,
        test_data: torch.Tensor,
        true_states: Optional[torch.Tensor],
        verbose: bool
    ) -> Dict[str, Any]:
        """ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã®åˆ†æ"""
        if verbose:
            print("\nğŸ“± ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åˆ†æ...")
            
        # ãƒ•ã‚£ãƒ«ã‚¿çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆï¼ˆé˜²å¾¡çš„ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
        if hasattr(inference_model, 'reset_state'):
            inference_model.reset_state()
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ¨è«–ç’°å¢ƒã®å†ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
            if hasattr(inference_model, 'setup_inference') and inference_model.calibration_data is not None:
                inference_model.setup_inference(
                    calibration_data=inference_model.calibration_data,
                    method='data_driven'
                )
        
        # é€æ¬¡å‡¦ç†
        start_time = datetime.now()
        online_states = []
        online_covariances = []
        online_likelihoods = []
        step_times = []
        
        try:
            for t, observation in enumerate(test_data):
                step_start = datetime.now()
                
                # 1ã‚¹ãƒ†ãƒƒãƒ—ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                x_hat, Sigma_x, likelihood = inference_model.filter_online(observation)
                
                step_end = datetime.now()
                step_time = (step_end - step_start).total_seconds()
                
                # çµæœä¿å­˜
                online_states.append(x_hat)
                online_covariances.append(Sigma_x)
                online_likelihoods.append(likelihood)
                step_times.append(step_time)
                
                # é€²æ—è¡¨ç¤º
                if verbose and (t + 1) % 100 == 0:
                    print(f"  ğŸ“ˆ å‡¦ç†æ¸ˆã¿: {t+1}/{len(test_data)} (å¹³å‡: {np.mean(step_times):.4f}ç§’/ã‚¹ãƒ†ãƒƒãƒ—)")
                    
            end_time = datetime.now()
            total_time = (end_time - start_time).total_seconds()
            
            # çµæœçµåˆ
            X_means_online = torch.stack(online_states)
            X_covariances_online = torch.stack(online_covariances)
            likelihoods_online = torch.tensor(online_likelihoods)
            
            if verbose:
                print(f"  âœ… ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Œäº† ({total_time:.2f}ç§’)")
                print(f"  ğŸ“ æ¨å®šçŠ¶æ…‹å½¢çŠ¶: {X_means_online.shape}")
                print(f"  âš¡ å¹³å‡ã‚¹ãƒ†ãƒƒãƒ—æ™‚é–“: {np.mean(step_times):.4f}ç§’")
                
        except Exception as e:
            import traceback
            error_details = {
                'error_message': str(e),
                'error_type': type(e).__name__,
                'full_traceback': traceback.format_exc(),
                'inference_model_type': type(inference_model).__name__,
                'available_methods': [m for m in dir(inference_model) if not m.startswith('_')],
                'reset_methods': [m for m in dir(inference_model) if not m.startswith('_') and 'reset' in m.lower()],
                'streaming_methods': [m for m in dir(inference_model) if not m.startswith('_') and 'streaming' in m.lower()],
                'test_data_shape': list(test_data.shape),
                'test_data_type': str(test_data.dtype),
                'model_setup_status': getattr(inference_model, 'is_setup', 'unknown'),
                'streaming_estimator_exists': hasattr(inference_model, 'streaming_estimator') and inference_model.streaming_estimator is not None
            }

            if verbose:
                print(f"  âŒ ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
                print(f"  ğŸ” ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}")
                print(f"  ğŸ“Š ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {test_data.shape} (dtype: {test_data.dtype})")
                print(f"  ğŸ¯ ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {type(inference_model).__name__}")
                print(f"  ğŸ”§ åˆ©ç”¨å¯èƒ½ãªresetãƒ¡ã‚½ãƒƒãƒ‰: {error_details['reset_methods']}")
                print(f"  ğŸŒŠ åˆ©ç”¨å¯èƒ½ãªstreamingãƒ¡ã‚½ãƒƒãƒ‰: {error_details['streaming_methods']}")
                print(f"  âš™ï¸  ãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—çŠ¶æ³: {error_details['model_setup_status']}")
                print(f"  ğŸ”— StreamingEstimatorå­˜åœ¨: {error_details['streaming_estimator_exists']}")
                print(f"  ğŸ“ è©³ç´°ãƒˆãƒ¬ãƒ¼ã‚¹:\n{traceback.format_exc()}")

            return {
                'error': str(e),
                'success': False,
                'error_details': error_details
            }
        
        # æ€§èƒ½è©•ä¾¡
        metrics = self.metrics_evaluator.compute_all_metrics(
            X_means_online, true_states, X_covariances_online, 
            test_data, likelihoods_online, verbose=False
        )
        
        if verbose:
            self._print_online_summary(metrics, total_time, step_times)
            
        return {
            'success': True,
            'total_processing_time': total_time,
            'average_step_time': np.mean(step_times),
            'step_times': step_times,
            'estimated_states': X_means_online,
            'covariances': X_covariances_online,
            'likelihoods': likelihoods_online,
            'metrics': metrics
        }
    
    def _analyze_computational_efficiency(
        self,
        inference_model,
        test_data: torch.Tensor,
        verbose: bool
    ) -> Dict[str, Any]:
        """è¨ˆç®—åŠ¹ç‡ã®åˆ†æ"""
        if verbose:
            print("\nâš¡ è¨ˆç®—åŠ¹ç‡åˆ†æ...")
            
        efficiency_results = {}
        
        # ãƒãƒƒãƒæ¨è«–æ™‚é–“æ¸¬å®š
        batch_timing = self.computational_metrics.measure_inference_time(
            lambda data: inference_model.filter_sequence(data),
            test_data,
            n_trials=3,
            warmup=1
        )
        efficiency_results['batch_timing'] = batch_timing
        
        # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¨è«–æ™‚é–“æ¸¬å®š
        def online_inference(data):
            inference_model.reset_state()
            for obs in data:
                inference_model.filter_online(obs)
                
        online_timing = self.computational_metrics.measure_inference_time(
            online_inference,
            test_data,
            n_trials=3,
            warmup=1
        )
        efficiency_results['online_timing'] = online_timing
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®š
        memory_usage = self.computational_metrics.measure_memory_usage(
            lambda data: inference_model.filter_sequence(data),
            test_data
        )
        efficiency_results['memory_usage'] = memory_usage
        
        if verbose:
            self._print_efficiency_summary(efficiency_results)
            
        return efficiency_results
    
    def _compare_filtering_methods(
        self,
        batch_results: Dict,
        online_results: Dict,
        verbose: bool
    ) -> Dict[str, Any]:
        """ãƒãƒƒãƒ vs ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¯”è¼ƒ"""
        if not (batch_results.get('success') and online_results.get('success')):
            return {'comparison_available': False}
            
        if verbose:
            print("\nğŸ” ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ‰‹æ³•æ¯”è¼ƒ...")
            
        comparison = {
            'comparison_available': True,
            'time_comparison': {
                'batch_time': batch_results['processing_time'],
                'online_total_time': online_results['total_processing_time'],
                'online_avg_step_time': online_results['average_step_time'],
                'speed_ratio': online_results['total_processing_time'] / batch_results['processing_time']
            }
        }
        
        # ç²¾åº¦æ¯”è¼ƒ
        batch_metrics = batch_results.get('metrics', {})
        online_metrics = online_results.get('metrics', {})
        
        if 'accuracy' in batch_metrics and 'accuracy' in online_metrics:
            batch_acc = batch_metrics['accuracy']
            online_acc = online_metrics['accuracy']
            
            comparison['accuracy_comparison'] = {
                'mse_difference': online_acc['mse'] - batch_acc['mse'],
                'mae_difference': online_acc['mae'] - batch_acc['mae'],
                'correlation_difference': online_acc['correlation'] - batch_acc['correlation']
            }
            
        if verbose:
            self._print_comparison_summary(comparison)
            
        return comparison
    
    def _print_batch_summary(self, metrics: Dict, processing_time: float):
        """ãƒãƒƒãƒçµæœã‚µãƒãƒªå‡ºåŠ›"""
        print(f"\n  ğŸ“Š ãƒãƒƒãƒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ:")
        print(f"    å‡¦ç†æ™‚é–“: {processing_time:.4f}ç§’")
        
        if 'accuracy' in metrics:
            acc = metrics['accuracy']
            print(f"    MSE: {acc['mse']:.6f}")
            print(f"    MAE: {acc['mae']:.6f}")
            print(f"    ç›¸é–¢ä¿‚æ•°: {acc['correlation']:.4f}")
            
        if 'uncertainty' in metrics:
            unc = metrics['uncertainty']
            print(f"    å¹³å‡ä¸ç¢ºå®Ÿæ€§: {unc['mean_uncertainty']:.6f}")
            if 'coverage_95' in unc:
                print(f"    95%ã‚«ãƒãƒ¬ãƒƒã‚¸: {unc['coverage_95']:.4f}")
                
    def _print_online_summary(self, metrics: Dict, total_time: float, step_times: List[float]):
        """ã‚ªãƒ³ãƒ©ã‚¤ãƒ³çµæœã‚µãƒãƒªå‡ºåŠ›"""
        print(f"\n  ğŸ“± ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ:")
        print(f"    ç·å‡¦ç†æ™‚é–“: {total_time:.4f}ç§’")
        print(f"    å¹³å‡ã‚¹ãƒ†ãƒƒãƒ—æ™‚é–“: {np.mean(step_times):.6f}ç§’")
        print(f"    ã‚¹ãƒ†ãƒƒãƒ—æ™‚é–“æ¨™æº–åå·®: {np.std(step_times):.6f}ç§’")
        
        if 'accuracy' in metrics:
            acc = metrics['accuracy']
            print(f"    MSE: {acc['mse']:.6f}")
            print(f"    MAE: {acc['mae']:.6f}")
            print(f"    ç›¸é–¢ä¿‚æ•°: {acc['correlation']:.4f}")
            
    def _print_efficiency_summary(self, efficiency: Dict):
        """åŠ¹ç‡æ€§ã‚µãƒãƒªå‡ºåŠ›"""
        print(f"\n  âš¡ è¨ˆç®—åŠ¹ç‡:")
        
        if 'batch_timing' in efficiency:
            batch = efficiency['batch_timing']
            print(f"    ãƒãƒƒãƒæ¨è«–æ™‚é–“: {batch['mean_time']:.4f}Â±{batch['std_time']:.4f}ç§’")
            
        if 'online_timing' in efficiency:
            online = efficiency['online_timing']
            print(f"    ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¨è«–æ™‚é–“: {online['mean_time']:.4f}Â±{online['std_time']:.4f}ç§’")
            
        if 'memory_usage' in efficiency and 'peak_memory_mb' in efficiency['memory_usage']:
            mem = efficiency['memory_usage']
            print(f"    ãƒ”ãƒ¼ã‚¯ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {mem['peak_memory_mb']:.1f}MB")
            
    def _print_comparison_summary(self, comparison: Dict):
        """æ¯”è¼ƒçµæœã‚µãƒãƒªå‡ºåŠ›"""
        if 'time_comparison' in comparison:
            time_comp = comparison['time_comparison']
            print(f"\n  ğŸ” æ‰‹æ³•æ¯”è¼ƒ:")
            print(f"    ãƒãƒƒãƒå‡¦ç†æ™‚é–“: {time_comp['batch_time']:.4f}ç§’")
            print(f"    ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å‡¦ç†æ™‚é–“: {time_comp['online_total_time']:.4f}ç§’")
            print(f"    é€Ÿåº¦æ¯” (ã‚ªãƒ³ãƒ©ã‚¤ãƒ³/ãƒãƒƒãƒ): {time_comp['speed_ratio']:.2f}")
            
        if 'accuracy_comparison' in comparison:
            acc_comp = comparison['accuracy_comparison']
            print(f"    MSEå·®åˆ† (ã‚ªãƒ³ãƒ©ã‚¤ãƒ³-ãƒãƒƒãƒ): {acc_comp['mse_difference']:+.6f}")
            print(f"    MAEå·®åˆ† (ã‚ªãƒ³ãƒ©ã‚¤ãƒ³-ãƒãƒƒãƒ): {acc_comp['mae_difference']:+.6f}")
    
    def _save_analysis_results(self, analysis: Dict, experiment_name: str):
        """åˆ†æçµæœã®ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSONå½¢å¼ã§ä¿å­˜
        json_path = self.output_dir / f"{experiment_name}_{timestamp}_analysis.json"
        with open(json_path, 'w') as f:
            json.dump(self._make_json_serializable(analysis), f, indent=2)
            
        print(f"ğŸ“ åˆ†æçµæœä¿å­˜: {json_path}")
        
        # CSVå½¢å¼ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜
        self._save_metrics_csv(analysis, experiment_name, timestamp)
        
        # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆNPZå½¢å¼ï¼‰
        self._save_numerical_data(analysis, experiment_name, timestamp)
    
    def _save_metrics_csv(self, analysis: Dict, experiment_name: str, timestamp: str):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’CSVå½¢å¼ã§ä¿å­˜"""
        csv_path = self.output_dir / f"{experiment_name}_{timestamp}_metrics.csv"
        
        with open(csv_path, 'w', newline='') as f:
            writer = csv.writer(f)
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼
            writer.writerow([
                'method', 'mse', 'mae', 'rmse', 'correlation',
                'mean_uncertainty', 'coverage_95', 'processing_time'
            ])
            
            # ãƒãƒƒãƒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ
            if analysis['batch_filtering'].get('success'):
                batch_metrics = analysis['batch_filtering']['metrics']
                if 'accuracy' in batch_metrics:
                    acc = batch_metrics['accuracy']
                    unc = batch_metrics.get('uncertainty', {})
                    writer.writerow([
                        'batch',
                        acc['mse'],
                        acc['mae'], 
                        acc['rmse'],
                        acc['correlation'],
                        unc.get('mean_uncertainty', ''),
                        unc.get('coverage_95', ''),
                        analysis['batch_filtering']['processing_time']
                    ])
            
            # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ
            if analysis['online_filtering'].get('success'):
                online_metrics = analysis['online_filtering']['metrics']
                if 'accuracy' in online_metrics:
                    acc = online_metrics['accuracy']
                    unc = online_metrics.get('uncertainty', {})
                    writer.writerow([
                        'online',
                        acc['mse'],
                        acc['mae'],
                        acc['rmse'], 
                        acc['correlation'],
                        unc.get('mean_uncertainty', ''),
                        unc.get('coverage_95', ''),
                        analysis['online_filtering']['total_processing_time']
                    ])
                    
        print(f"ğŸ“Š ãƒ¡ãƒˆãƒªã‚¯ã‚¹CSVä¿å­˜: {csv_path}")
    
    def _save_numerical_data(self, analysis: Dict, experiment_name: str, timestamp: str):
        """æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã‚’NPZå½¢å¼ã§ä¿å­˜"""
        npz_path = self.output_dir / f"{experiment_name}_{timestamp}_data.npz"
        
        save_data = {}
        
        # ãƒãƒƒãƒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ
        if analysis['batch_filtering'].get('success'):
            batch_result = analysis['batch_filtering']
            if isinstance(batch_result['estimated_states'], torch.Tensor):
                save_data['batch_states'] = batch_result['estimated_states'].cpu().numpy()
                save_data['batch_covariances'] = batch_result['covariances'].cpu().numpy()
                if batch_result['likelihoods'] is not None:
                    save_data['batch_likelihoods'] = batch_result['likelihoods'].cpu().numpy()
        
        # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ  
        if analysis['online_filtering'].get('success'):
            online_result = analysis['online_filtering']
            if isinstance(online_result['estimated_states'], torch.Tensor):
                save_data['online_states'] = online_result['estimated_states'].cpu().numpy()
                save_data['online_covariances'] = online_result['covariances'].cpu().numpy()
                save_data['online_likelihoods'] = online_result['likelihoods'].cpu().numpy()
                save_data['step_times'] = np.array(online_result['step_times'])
        
        if save_data:
            np.savez(npz_path, **save_data)
            print(f"ğŸ’¾ æ•°å€¤ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {npz_path}")
    
    def _make_json_serializable(self, obj):
        """JSONå¯¾å¿œå½¢å¼ã«å¤‰æ›"""
        if isinstance(obj, dict):
            return {k: self._make_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_json_serializable(v) for v in obj]
        elif isinstance(obj, torch.Tensor):
            return obj.cpu().numpy().tolist()
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.floating, np.integer)):
            return float(obj)
        else:
            return obj


def create_filtering_analyzer(output_dir: str, device: str = 'cpu') -> FilteringAnalyzer:
    """ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åˆ†æå™¨ã®ä½œæˆ"""
    return FilteringAnalyzer(output_dir, device)


def run_quick_filtering_analysis(
    inference_model,
    test_data: torch.Tensor,
    output_dir: str,
    experiment_name: str = "quick_analysis"
) -> Dict[str, Any]:
    """
    ã‚¯ã‚¤ãƒƒã‚¯ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åˆ†æ
    
    ç°¡å˜ãªAPIã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åˆ†æã‚’å®Ÿè¡Œ
    """
    analyzer = FilteringAnalyzer(output_dir)
    return analyzer.analyze_filtering_performance(
        inference_model, test_data, experiment_name=experiment_name
    )