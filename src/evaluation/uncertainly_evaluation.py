"""
ä¸ç¢ºå®Ÿæ€§è©•ä¾¡

DFIV Kalman Filterä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–å“è³ªè©•ä¾¡:
ä¿¡é ¼åŒºé–“å“è³ª, ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³, å¦¥å½“æ€§æ¤œè¨¼
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Optional, Union
from scipy import stats
import seaborn as sns
from pathlib import Path


class UncertaintyEvaluator:
    """ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–è©•ä¾¡ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, output_dir: str = None):
        self.output_dir = Path(output_dir) if output_dir else None
        if self.output_dir:
            self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def evaluate_uncertainty_quality(
        self,
        predictions: torch.Tensor,
        uncertainties: torch.Tensor,
        true_values: Optional[torch.Tensor] = None,
        save_plots: bool = True,
        verbose: bool = True
    ) -> Dict[str, Union[float, Dict, List]]:
        """
        ä¸ç¢ºå®Ÿæ€§å“è³ªåŒ…æ‹¬è©•ä¾¡

        Args:
            predictions: äºˆæ¸¬ (T,d)
            uncertainties: æ¨™æº–åå·® (T,d)
            true_values: çœŸå€¤ (T,d)
            save_plots: ãƒ—ãƒ­ãƒƒãƒˆä¿å­˜
            verbose: è©³ç´°å‡ºåŠ›
        Returns: è©•ä¾¡çµæœ
        """
        if verbose:
            print("\nğŸ² ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–è©•ä¾¡é–‹å§‹")
            print("="*50)
            
        evaluation_results = {}
        
        # 1. åŸºæœ¬çµ±è¨ˆ
        evaluation_results['basic_stats'] = self._compute_uncertainty_stats(
            predictions, uncertainties, verbose
        )
        
        # 2. ä¿¡é ¼åŒºé–“è©•ä¾¡ï¼ˆçœŸå€¤ãŒã‚ã‚‹å ´åˆï¼‰
        if true_values is not None:
            evaluation_results['confidence_intervals'] = self._evaluate_confidence_intervals(
                predictions, uncertainties, true_values, verbose
            )
            
            # 3. ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡
            evaluation_results['calibration'] = self._evaluate_calibration(
                predictions, uncertainties, true_values, verbose
            )
            
            # 4. ä¸ç¢ºå®Ÿæ€§ã®æœ‰ç”¨æ€§è©•ä¾¡
            evaluation_results['uncertainty_utility'] = self._evaluate_uncertainty_utility(
                predictions, uncertainties, true_values, verbose
            )
        
        # 5. ä¸ç¢ºå®Ÿæ€§ã®æ™‚ç³»åˆ—ç‰¹æ€§
        evaluation_results['temporal_analysis'] = self._analyze_temporal_uncertainty(
            uncertainties, verbose
        )
        
        # 6. å¯è¦–åŒ–
        if save_plots and self.output_dir:
            self._create_uncertainty_plots(
                predictions, uncertainties, true_values, evaluation_results
            )
        
        if verbose:
            self._print_uncertainty_summary(evaluation_results)
            
        return evaluation_results
    
    def _compute_uncertainty_stats(
        self, 
        predictions: torch.Tensor, 
        uncertainties: torch.Tensor,
        verbose: bool
    ) -> Dict[str, float]:
        """ä¸ç¢ºå®Ÿæ€§ã®åŸºæœ¬çµ±è¨ˆ"""
        with torch.no_grad():
            stats = {
                'mean_uncertainty': uncertainties.mean().item(),
                'std_uncertainty': uncertainties.std().item(),
                'min_uncertainty': uncertainties.min().item(),
                'max_uncertainty': uncertainties.max().item(),
                'median_uncertainty': uncertainties.median().item(),
                'uncertainty_range': (uncertainties.max() - uncertainties.min()).item()
            }
            
            # æ¬¡å…ƒåˆ¥çµ±è¨ˆ
            if uncertainties.dim() > 1:
                stats['uncertainty_per_dimension'] = uncertainties.mean(dim=0).tolist()
                stats['uncertainty_std_per_dimension'] = uncertainties.std(dim=0).tolist()
            
            if verbose:
                print(f"\nä¸ç¢ºå®Ÿæ€§åŸºæœ¬çµ±è¨ˆ:")
                print(f"  å¹³å‡ä¸ç¢ºå®Ÿæ€§: {stats['mean_uncertainty']:.6f}")
                print(f"  ä¸ç¢ºå®Ÿæ€§æ¨™æº–åå·®: {stats['std_uncertainty']:.6f}")
                print(f"  ä¸ç¢ºå®Ÿæ€§ç¯„å›²: [{stats['min_uncertainty']:.6f}, {stats['max_uncertainty']:.6f}]")
                
        return stats
    
    def _evaluate_confidence_intervals(
        self,
        predictions: torch.Tensor,
        uncertainties: torch.Tensor,
        true_values: torch.Tensor,
        verbose: bool
    ) -> Dict[str, Union[float, List]]:
        """ä¿¡é ¼åŒºé–“ã®è©•ä¾¡"""
        confidence_levels = [0.68, 0.90, 0.95, 0.99]
        interval_results = {}
        
        for conf_level in confidence_levels:
            # Zå€¤è¨ˆç®—
            z_score = stats.norm.ppf((1 + conf_level) / 2)
            
            # ä¿¡é ¼åŒºé–“
            lower_bound = predictions - z_score * uncertainties
            upper_bound = predictions + z_score * uncertainties
            
            # ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ¤å®š
            covered = (true_values >= lower_bound) & (true_values <= upper_bound)
            
            if predictions.dim() > 1:
                # å…¨æ¬¡å…ƒãŒã‚«ãƒãƒ¼ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                full_coverage = covered.all(dim=1).float()
                partial_coverage = covered.float().mean(dim=1)
            else:
                full_coverage = covered.float()
                partial_coverage = covered.float()
            
            # ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡
            coverage_rate = full_coverage.mean().item()
            partial_coverage_rate = partial_coverage.mean().item()
            
            # åŒºé–“å¹…
            interval_width = (upper_bound - lower_bound).mean().item()
            
            # ã‚«ãƒãƒ¬ãƒƒã‚¸èª¤å·®
            coverage_error = abs(coverage_rate - conf_level)
            
            interval_results[f'confidence_{int(conf_level*100)}'] = {
                'expected_coverage': conf_level,
                'actual_coverage': coverage_rate,
                'partial_coverage': partial_coverage_rate,
                'coverage_error': coverage_error,
                'mean_interval_width': interval_width,
                'z_score_used': z_score
            }
        
        if verbose:
            print(f"\nä¿¡é ¼åŒºé–“è©•ä¾¡:")
            for level, result in interval_results.items():
                expected = result['expected_coverage']
                actual = result['actual_coverage']
                error = result['coverage_error']
                width = result['mean_interval_width']
                print(f"  {level}: ã‚«ãƒãƒ¬ãƒƒã‚¸ {actual:.3f} (æœŸå¾…: {expected:.3f}, èª¤å·®: {error:.3f}, å¹…: {width:.4f})")
        
        return interval_results
    
    def _evaluate_calibration(
        self,
        predictions: torch.Tensor,
        uncertainties: torch.Tensor,
        true_values: torch.Tensor,
        verbose: bool,
        n_bins: int = 10
    ) -> Dict[str, Union[float, List]]:
        """ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡"""
        with torch.no_grad():
            # æ­£è¦åŒ–ã•ã‚ŒãŸèª¤å·®
            normalized_errors = (predictions - true_values) / uncertainties
            
            # Expected Calibration Error (ECE)
            ece = self._compute_ece(predictions, uncertainties, true_values, n_bins)
            
            # æ­£è¦æ€§æ¤œå®š
            if normalized_errors.numel() > 0:
                normalized_flat = normalized_errors.flatten().cpu().numpy()
                
                # Shapiro-Wilkæ¤œå®šï¼ˆã‚µãƒ³ãƒ—ãƒ«æ•°åˆ¶é™ï¼‰
                if len(normalized_flat) <= 5000:
                    shapiro_stat, shapiro_p = stats.shapiro(normalized_flat[:5000])
                else:
                    # å¤§ããªã‚µãƒ³ãƒ—ãƒ«ã®å ´åˆã¯ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                    sample_indices = np.random.choice(len(normalized_flat), 5000, replace=False)
                    shapiro_stat, shapiro_p = stats.shapiro(normalized_flat[sample_indices])
                    
                # Kolmogorov-Smirnovæ¤œå®š
                ks_stat, ks_p = stats.kstest(normalized_flat, 'norm')
                
                normality_tests = {
                    'shapiro_stat': float(shapiro_stat),
                    'shapiro_p_value': float(shapiro_p),
                    'ks_stat': float(ks_stat),
                    'ks_p_value': float(ks_p),
                    'is_normal_shapiro': shapiro_p > 0.05,
                    'is_normal_ks': ks_p > 0.05
                }
            else:
                normality_tests = {'error': 'No data for normality test'}
            
            # æ®‹å·®åˆ†æ
            residual_stats = {
                'mean_normalized_error': normalized_errors.mean().item(),
                'std_normalized_error': normalized_errors.std().item(),
                'skewness': float(stats.skew(normalized_errors.flatten().cpu().numpy())),
                'kurtosis': float(stats.kurtosis(normalized_errors.flatten().cpu().numpy()))
            }
            
            calibration_result = {
                'ece': ece,
                'normality_tests': normality_tests,
                'residual_statistics': residual_stats
            }
            
            if verbose:
                print(f"\nã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡:")
                print(f"  Expected Calibration Error: {ece:.4f}")
                print(f"  æ­£è¦åŒ–æ®‹å·® - å¹³å‡: {residual_stats['mean_normalized_error']:.4f}")
                print(f"  æ­£è¦åŒ–æ®‹å·® - æ¨™æº–åå·®: {residual_stats['std_normalized_error']:.4f}")
                print(f"  æ­£è¦æ€§æ¤œå®š (Shapiro): p={normality_tests.get('shapiro_p_value', 'N/A'):.4f}")
                print(f"  æ­£è¦æ€§æ¤œå®š (KS): p={normality_tests.get('ks_p_value', 'N/A'):.4f}")
            
        return calibration_result
    
    def _evaluate_uncertainty_utility(
        self,
        predictions: torch.Tensor,
        uncertainties: torch.Tensor,
        true_values: torch.Tensor,
        verbose: bool
    ) -> Dict[str, float]:
        """ä¸ç¢ºå®Ÿæ€§ã®æœ‰ç”¨æ€§è©•ä¾¡"""
        with torch.no_grad():
            # çµ¶å¯¾èª¤å·®
            abs_errors = torch.abs(predictions - true_values)
            
            # ä¸ç¢ºå®Ÿæ€§ã¨èª¤å·®ã®ç›¸é–¢
            if abs_errors.dim() > 1:
                # å„ã‚µãƒ³ãƒ—ãƒ«ã®å¹³å‡èª¤å·®ã¨å¹³å‡ä¸ç¢ºå®Ÿæ€§
                mean_errors = abs_errors.mean(dim=1)
                mean_uncertainties = uncertainties.mean(dim=1)
            else:
                mean_errors = abs_errors
                mean_uncertainties = uncertainties
            
            # ç›¸é–¢è¨ˆç®—
            if len(mean_errors) > 1:
                correlation = np.corrcoef(
                    mean_errors.cpu().numpy(), 
                    mean_uncertainties.cpu().numpy()
                )[0, 1]
            else:
                correlation = 0.0
            
            # ä¸ç¢ºå®Ÿæ€§ã®äºˆæ¸¬çš„ä¾¡å€¤
            # é«˜ä¸ç¢ºå®Ÿæ€§ã‚µãƒ³ãƒ—ãƒ«ãŒå®Ÿéš›ã«é«˜èª¤å·®ã‹ã©ã†ã‹
            uncertainty_threshold = uncertainties.quantile(0.8)  # ä¸Šä½20%
            high_uncertainty_mask = uncertainties >= uncertainty_threshold
            
            if high_uncertainty_mask.any():
                high_unc_errors = abs_errors[high_uncertainty_mask].mean().item()
                low_unc_errors = abs_errors[~high_uncertainty_mask].mean().item()
                uncertainty_effectiveness = (high_unc_errors - low_unc_errors) / low_unc_errors
            else:
                uncertainty_effectiveness = 0.0
            
            utility_result = {
                'error_uncertainty_correlation': float(correlation),
                'uncertainty_effectiveness': uncertainty_effectiveness,
                'high_uncertainty_error_ratio': high_unc_errors / low_unc_errors if 'low_unc_errors' in locals() and low_unc_errors > 0 else 1.0
            }
            
            if verbose:
                print(f"\nä¸ç¢ºå®Ÿæ€§æœ‰ç”¨æ€§:")
                print(f"  èª¤å·®-ä¸ç¢ºå®Ÿæ€§ç›¸é–¢: {correlation:.4f}")
                print(f"  ä¸ç¢ºå®Ÿæ€§åŠ¹æœ: {uncertainty_effectiveness:.4f}")
            
        return utility_result
    
    def _analyze_temporal_uncertainty(
        self, 
        uncertainties: torch.Tensor,
        verbose: bool
    ) -> Dict[str, float]:
        """æ™‚ç³»åˆ—ä¸ç¢ºå®Ÿæ€§ã®åˆ†æ"""
        with torch.no_grad():
            T = uncertainties.size(0)
            
            if T < 2:
                return {'error': 'Insufficient time steps for temporal analysis'}
            
            # æ™‚ç³»åˆ—ç‰¹æ€§
            if uncertainties.dim() > 1:
                # å„æ™‚ç‚¹ã§ã®å¹³å‡ä¸ç¢ºå®Ÿæ€§
                temporal_uncertainty = uncertainties.mean(dim=1)
            else:
                temporal_uncertainty = uncertainties
            
            # å‚¾å‘åˆ†æ
            time_indices = torch.arange(T, dtype=torch.float32)
            
            # ç·šå½¢å‚¾å‘
            time_mean = time_indices.mean()
            unc_mean = temporal_uncertainty.mean()
            
            numerator = ((time_indices - time_mean) * (temporal_uncertainty - unc_mean)).sum()
            denominator = ((time_indices - time_mean) ** 2).sum()
            
            if denominator > 0:
                trend_slope = (numerator / denominator).item()
            else:
                trend_slope = 0.0
            
            # å¤‰å‹•æ€§
            uncertainty_volatility = temporal_uncertainty.std().item()
            
            # è‡ªå·±ç›¸é–¢ï¼ˆlag-1ï¼‰
            if T > 2:
                autocorr = np.corrcoef(
                    temporal_uncertainty[:-1].cpu().numpy(),
                    temporal_uncertainty[1:].cpu().numpy()
                )[0, 1]
            else:
                autocorr = 0.0
            
            temporal_result = {
                'trend_slope': trend_slope,
                'volatility': uncertainty_volatility,
                'autocorrelation_lag1': float(autocorr),
                'initial_uncertainty': temporal_uncertainty[0].item(),
                'final_uncertainty': temporal_uncertainty[-1].item(),
                'uncertainty_change': (temporal_uncertainty[-1] - temporal_uncertainty[0]).item()
            }
            
            if verbose:
                print(f"\nğŸ“ˆ æ™‚ç³»åˆ—ä¸ç¢ºå®Ÿæ€§:")
                print(f"  ä¸ç¢ºå®Ÿæ€§å‚¾å‘: {trend_slope:+.6f}/ã‚¹ãƒ†ãƒƒãƒ—")
                print(f"  ä¸ç¢ºå®Ÿæ€§å¤‰å‹•: {uncertainty_volatility:.6f}")
                print(f"  è‡ªå·±ç›¸é–¢(lag-1): {autocorr:.4f}")
                print(f"  ä¸ç¢ºå®Ÿæ€§å¤‰åŒ–: {temporal_result['uncertainty_change']:+.6f}")
            
        return temporal_result
    
    def _compute_ece(
        self,
        predictions: torch.Tensor,
        uncertainties: torch.Tensor,
        true_values: torch.Tensor,
        n_bins: int = 10
    ) -> float:
        """Expected Calibration Errorè¨ˆç®—"""
        with torch.no_grad():
            # æ­£è¦åŒ–ã•ã‚ŒãŸä¸ç¢ºå®Ÿæ€§ã‚’ç¢ºä¿¡åº¦ã«å¤‰æ›
            # ç°¡ç•¥ç‰ˆ: ä¸ç¢ºå®Ÿæ€§ã®é€†æ•°ã‚’ä½¿ç”¨
            max_uncertainty = uncertainties.max()
            confidences = 1.0 - (uncertainties / max_uncertainty)
            
            # æ­£ç¢ºæ€§ï¼ˆé–¾å€¤ãƒ™ãƒ¼ã‚¹ï¼‰
            threshold = uncertainties.mean()
            accuracies = (torch.abs(predictions - true_values) < threshold).float()
            
            if predictions.dim() > 1:
                confidences = confidences.mean(dim=1)
                accuracies = accuracies.mean(dim=1)
            
            # ãƒ“ãƒ³åˆ†å‰²
            bin_boundaries = torch.linspace(0, 1, n_bins + 1)
            ece = 0.0
            
            for i in range(n_bins):
                bin_lower = bin_boundaries[i]
                bin_upper = bin_boundaries[i + 1]
                
                in_bin = (confidences > bin_lower) & (confidences <= bin_upper)
                prop_in_bin = in_bin.float().mean()
                
                if prop_in_bin > 0:
                    accuracy_in_bin = accuracies[in_bin].mean()
                    avg_confidence_in_bin = confidences[in_bin].mean()
                    ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin
            
        return ece.item()
    
    def _create_uncertainty_plots(
        self,
        predictions: torch.Tensor,
        uncertainties: torch.Tensor,
        true_values: Optional[torch.Tensor],
        evaluation_results: Dict
    ):
        """ä¸ç¢ºå®Ÿæ€§å¯è¦–åŒ–"""
        if not self.output_dir:
            return
        
        plt.style.use('default')
        
        # 1. ä¸ç¢ºå®Ÿæ€§ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
        plt.figure(figsize=(12, 8))
        
        plt.subplot(2, 3, 1)
        plt.hist(uncertainties.flatten().cpu().numpy(), bins=50, alpha=0.7, color='skyblue')
        plt.title('Uncertainty Distribution')
        plt.xlabel('Uncertainty')
        plt.ylabel('Frequency')
        plt.grid(True, alpha=0.3)
        
        # 2. æ™‚ç³»åˆ—ä¸ç¢ºå®Ÿæ€§
        plt.subplot(2, 3, 2)
        if uncertainties.dim() > 1:
            temporal_unc = uncertainties.mean(dim=1).cpu().numpy()
        else:
            temporal_unc = uncertainties.cpu().numpy()
        plt.plot(temporal_unc, color='orange', linewidth=2)
        plt.title('Uncertainty Time Series')
        plt.xlabel('Time')
        plt.ylabel('Mean Uncertainty')
        plt.grid(True, alpha=0.3)
        
        if true_values is not None:
            # 3. èª¤å·® vs ä¸ç¢ºå®Ÿæ€§
            plt.subplot(2, 3, 3)
            abs_errors = torch.abs(predictions - true_values)
            if abs_errors.dim() > 1:
                errors_flat = abs_errors.mean(dim=1).cpu().numpy()
                uncertainties_flat = uncertainties.mean(dim=1).cpu().numpy()
            else:
                errors_flat = abs_errors.cpu().numpy()
                uncertainties_flat = uncertainties.cpu().numpy()
            
            plt.scatter(uncertainties_flat, errors_flat, alpha=0.6, color='green')
            plt.xlabel('Uncertainty')
            plt.ylabel('Absolute Error')
            plt.title('Uncertainty vs Error')
            plt.grid(True, alpha=0.3)
            
            # 4. ä¿¡é ¼åŒºé–“ãƒ—ãƒ­ãƒƒãƒˆï¼ˆã‚µãƒ³ãƒ—ãƒ«ï¼‰
            plt.subplot(2, 3, 4)
            n_samples = min(100, len(predictions))
            indices = torch.randperm(len(predictions))[:n_samples]
            
            sample_pred = predictions[indices]
            sample_true = true_values[indices]
            sample_unc = uncertainties[indices]
            
            if sample_pred.dim() > 1:
                sample_pred = sample_pred.mean(dim=1)
                sample_true = sample_true.mean(dim=1)
                sample_unc = sample_unc.mean(dim=1)
            
            x_pos = torch.arange(n_samples)
            plt.errorbar(
                x_pos.cpu().numpy(),
                sample_pred.cpu().numpy(),
                yerr=1.96 * sample_unc.cpu().numpy(),
                fmt='o',
                color='blue',
                alpha=0.6,
                capsize=3,
                label='Prediction Â±95% CI'
            )
            plt.scatter(x_pos.cpu().numpy(), sample_true.cpu().numpy(), 
                       color='red', s=20, label='True Value', zorder=5)
            plt.title('Confidence Interval Sample')
            plt.xlabel('Sample')
            plt.ylabel('Value')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
        # 5. ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ—ãƒ­ãƒƒãƒˆ
        if true_values is not None and 'calibration' in evaluation_results:
            plt.subplot(2, 3, 5)
            # ç°¡ç•¥ç‰ˆã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ—ãƒ­ãƒƒãƒˆ
            self._plot_calibration_curve(predictions, uncertainties, true_values)
            
        # 6. ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡ãƒ—ãƒ­ãƒƒãƒˆ
        if 'confidence_intervals' in evaluation_results:
            plt.subplot(2, 3, 6)
            intervals = evaluation_results['confidence_intervals']
            levels = []
            expected = []
            actual = []
            
            for key, value in intervals.items():
                if key.startswith('confidence_'):
                    level = int(key.split('_')[1])
                    levels.append(level)
                    expected.append(value['expected_coverage'])
                    actual.append(value['actual_coverage'])
            
            plt.plot(levels, expected, 'r--', label='Expected Coverage', linewidth=2)
            plt.plot(levels, actual, 'b-o', label='Actual Coverage', linewidth=2)
            plt.xlabel('Confidence Level (%)')
            plt.ylabel('Coverage Rate')
            plt.title('Coverage Rate Evaluation')
            plt.legend()
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plot_path = self.output_dir / 'uncertainty_analysis.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ä¸ç¢ºå®Ÿæ€§å¯è¦–åŒ–ä¿å­˜: {plot_path}")
    
    def _plot_calibration_curve(
        self, 
        predictions: torch.Tensor, 
        uncertainties: torch.Tensor, 
        true_values: torch.Tensor
    ):
        """ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ›²ç·šã®ãƒ—ãƒ­ãƒƒãƒˆ"""
        # ç°¡ç•¥ç‰ˆå®Ÿè£…
        confidence_levels = np.linspace(0.1, 0.9, 9)
        empirical_coverage = []
        
        for conf in confidence_levels:
            z_score = stats.norm.ppf((1 + conf) / 2)
            lower = predictions - z_score * uncertainties
            upper = predictions + z_score * uncertainties
            
            covered = ((true_values >= lower) & (true_values <= upper))
            if covered.dim() > 1:
                coverage = covered.all(dim=1).float().mean().item()
            else:
                coverage = covered.float().mean().item()
            
            empirical_coverage.append(coverage)
        
        plt.plot(confidence_levels, confidence_levels, 'r--', label='Perfect Calibration')
        plt.plot(confidence_levels, empirical_coverage, 'b-o', label='Actual')
        plt.xlabel('Expected Coverage')
        plt.ylabel('Actual Coverage')
        plt.title('Calibration Curve')
        plt.legend()
        plt.grid(True, alpha=0.3)
    
    def _print_uncertainty_summary(self, results: Dict):
        """ä¸ç¢ºå®Ÿæ€§è©•ä¾¡ã‚µãƒãƒªå‡ºåŠ›"""
        print(f"\nä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–è©•ä¾¡å®Œäº†")
        print("="*50)
        
        if 'confidence_intervals' in results:
            print(f"\nä¸»è¦ã‚«ãƒãƒ¬ãƒƒã‚¸çµæœ:")
            ci_results = results['confidence_intervals']
            for level in [68, 95]:
                key = f'confidence_{level}'
                if key in ci_results:
                    result = ci_results[key]
                    print(f"  {level}%ä¿¡é ¼åŒºé–“: {result['actual_coverage']:.3f} (èª¤å·®: {result['coverage_error']:.3f})")
        
        if 'calibration' in results:
            cal_results = results['calibration']
            print(f"\nã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³:")
            print(f"  ECE: {cal_results['ece']:.4f}")
            
        if 'uncertainty_utility' in results:
            utility = results['uncertainty_utility']
            print(f"\nä¸ç¢ºå®Ÿæ€§æœ‰ç”¨æ€§:")
            print(f"  èª¤å·®-ä¸ç¢ºå®Ÿæ€§ç›¸é–¢: {utility['error_uncertainty_correlation']:.4f}")


def create_uncertainty_evaluator(output_dir: str = None) -> UncertaintyEvaluator:
    """ä¸ç¢ºå®Ÿæ€§è©•ä¾¡å™¨ä½œæˆ"""
    return UncertaintyEvaluator(output_dir)


def quick_uncertainty_evaluation(
    predictions: torch.Tensor,
    uncertainties: torch.Tensor,
    true_values: torch.Tensor,
    output_dir: str = None
) -> Dict:
    """ã‚¯ã‚¤ãƒƒã‚¯ä¸ç¢ºå®Ÿæ€§è©•ä¾¡"""
    evaluator = UncertaintyEvaluator(output_dir)
    return evaluator.evaluate_uncertainty_quality(
        predictions, uncertainties, true_values
    )