"""
è©•ä¾¡æŒ‡æ¨™è¨ˆç®—ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

DFIV Kalman Filterã®çŠ¶æ…‹æ¨å®šæ€§èƒ½ã‚’åŒ…æ‹¬çš„ã«è©•ä¾¡ã™ã‚‹ãŸã‚ã®æŒ‡æ¨™ã‚’æä¾›ã€‚
- æ¨å®šç²¾åº¦ï¼ˆMSE, MAE, RMSEï¼‰
- ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–å“è³ªï¼ˆã‚«ãƒãƒ¬ãƒƒã‚¸ç‡ã€åŒºé–“å¹…ï¼‰
- äºˆæ¸¬æ€§èƒ½ï¼ˆå¯¾æ•°å°¤åº¦ã€ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
- è¨ˆç®—åŠ¹ç‡ï¼ˆæ™‚é–“ã€ãƒ¡ãƒ¢ãƒªï¼‰
"""

import torch
import numpy as np
import time
from typing import Dict, List, Tuple, Optional, Union
from scipy import stats
import warnings


class StateEstimationMetrics:
    """çŠ¶æ…‹æ¨å®šæ€§èƒ½è©•ä¾¡ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, device: str = 'cpu'):
        self.device = torch.device(device)
        
    def compute_all_metrics(
        self,
        X_estimated: torch.Tensor,
        X_true: Optional[torch.Tensor] = None,
        X_covariances: Optional[torch.Tensor] = None,
        observations: Optional[torch.Tensor] = None,
        likelihoods: Optional[torch.Tensor] = None,
        verbose: bool = True
    ) -> Dict[str, Union[float, Dict]]:
        """
        åŒ…æ‹¬çš„è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—
        
        Args:
            X_estimated: æ¨å®šçŠ¶æ…‹ (T, r)
            X_true: çœŸã®çŠ¶æ…‹ (T, r) [optional]
            X_covariances: çŠ¶æ…‹å…±åˆ†æ•£ (T, r, r) [optional]
            observations: è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ (T, n) [optional]
            likelihoods: è¦³æ¸¬å°¤åº¦ (T,) [optional]
            verbose: ã‚¿ãƒ¼ãƒŸãƒŠãƒ«å‡ºåŠ›ã™ã‚‹ã‹ã©ã†ã‹
            
        Returns:
            Dict: å…¨è©•ä¾¡æŒ‡æ¨™
        """
        metrics = {}
        
        # åŸºæœ¬çµ±è¨ˆ
        metrics['basic_stats'] = self._compute_basic_stats(X_estimated)
        
        # ç²¾åº¦è©•ä¾¡ï¼ˆçœŸå€¤ãŒã‚ã‚‹å ´åˆï¼‰
        if X_true is not None:
            metrics['accuracy'] = self._compute_accuracy_metrics(X_estimated, X_true)
            
        # ä¸ç¢ºå®Ÿæ€§è©•ä¾¡ï¼ˆå…±åˆ†æ•£ãŒã‚ã‚‹å ´åˆï¼‰
        if X_covariances is not None:
            metrics['uncertainty'] = self._compute_uncertainty_metrics(
                X_estimated, X_covariances, X_true
            )
            
        # å°¤åº¦è©•ä¾¡
        if likelihoods is not None:
            metrics['likelihood'] = self._compute_likelihood_metrics(likelihoods)
            
        # äºˆæ¸¬æ€§èƒ½ï¼ˆè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆï¼‰
        if observations is not None:
            metrics['prediction'] = self._compute_prediction_metrics(
                X_estimated, observations
            )
            
        # ã‚¿ãƒ¼ãƒŸãƒŠãƒ«å‡ºåŠ›
        if verbose:
            self._print_metrics_summary(metrics)
            
        return metrics
    
    def _compute_basic_stats(self, X_estimated: torch.Tensor) -> Dict[str, float]:
        """åŸºæœ¬çµ±è¨ˆæƒ…å ±"""
        with torch.no_grad():
            return {
                'sequence_length': X_estimated.size(0),
                'state_dimension': X_estimated.size(1),
                'mean_state_norm': torch.norm(X_estimated, dim=1).mean().item(),
                'std_state_norm': torch.norm(X_estimated, dim=1).std().item(),
                'max_state_value': X_estimated.max().item(),
                'min_state_value': X_estimated.min().item()
            }
    
    def _compute_accuracy_metrics(
        self, 
        X_estimated: torch.Tensor, 
        X_true: torch.Tensor
    ) -> Dict[str, float]:
        """æ¨å®šç²¾åº¦æŒ‡æ¨™"""
        with torch.no_grad():
            # ã‚¨ãƒ©ãƒ¼è¨ˆç®—
            errors = X_estimated - X_true
            squared_errors = errors ** 2
            abs_errors = torch.abs(errors)
            
            # æ¬¡å…ƒã”ã¨ã®æŒ‡æ¨™
            mse_per_dim = squared_errors.mean(dim=0)
            mae_per_dim = abs_errors.mean(dim=0)
            
            return {
                'mse': squared_errors.mean().item(),
                'mae': abs_errors.mean().item(),
                'rmse': torch.sqrt(squared_errors.mean()).item(),
                'mse_per_dimension': mse_per_dim.tolist(),
                'mae_per_dimension': mae_per_dim.tolist(),
                'relative_error': (torch.norm(errors, dim=1) / torch.norm(X_true, dim=1)).mean().item(),
                'correlation': self._compute_correlation(X_estimated, X_true).item()
            }
    
    def _compute_uncertainty_metrics(
        self,
        X_estimated: torch.Tensor,
        X_covariances: torch.Tensor,
        X_true: Optional[torch.Tensor] = None
    ) -> Dict[str, Union[float, List]]:
        """ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–å“è³ª"""
        with torch.no_grad():
            # æ¨™æº–åå·®æŠ½å‡º
            std_devs = torch.sqrt(torch.diagonal(X_covariances, dim1=1, dim2=2))
            
            metrics = {
                'mean_uncertainty': std_devs.mean().item(),
                'std_uncertainty': std_devs.std().item(),
                'uncertainty_per_dimension': std_devs.mean(dim=0).tolist(),
                'determinant_mean': torch.det(X_covariances).mean().item(),
                'trace_mean': torch.trace(X_covariances.view(-1, X_covariances.size(-1), X_covariances.size(-1))).mean().item()
            }
            
            # ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡è¨ˆç®—ï¼ˆçœŸå€¤ãŒã‚ã‚‹å ´åˆï¼‰
            if X_true is not None:
                coverage_results = self._compute_coverage_rates(
                    X_estimated, std_devs, X_true
                )
                metrics.update(coverage_results)
                
            return metrics
    
    def _compute_coverage_rates(
        self,
        X_estimated: torch.Tensor,
        std_devs: torch.Tensor,
        X_true: torch.Tensor,
        confidence_levels: List[float] = [0.68, 0.95, 0.99]
    ) -> Dict[str, float]:
        """ä¿¡é ¼åŒºé–“ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡"""
        coverage_results = {}
        
        for conf_level in confidence_levels:
            # Zå€¤è¨ˆç®—
            z_score = stats.norm.ppf((1 + conf_level) / 2)
            
            # ä¿¡é ¼åŒºé–“
            lower = X_estimated - z_score * std_devs
            upper = X_estimated + z_score * std_devs
            
            # ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ¤å®š
            covered = (X_true >= lower) & (X_true <= upper)
            coverage_rate = covered.all(dim=1).float().mean().item()
            
            coverage_results[f'coverage_{int(conf_level*100)}'] = coverage_rate
            coverage_results[f'coverage_error_{int(conf_level*100)}'] = abs(coverage_rate - conf_level)
            
        return coverage_results
    
    def _compute_likelihood_metrics(self, likelihoods: torch.Tensor) -> Dict[str, float]:
        """å°¤åº¦é–¢é€£æŒ‡æ¨™"""
        with torch.no_grad():
            return {
                'total_log_likelihood': likelihoods.sum().item(),
                'mean_log_likelihood': likelihoods.mean().item(),
                'std_log_likelihood': likelihoods.std().item(),
                'perplexity': torch.exp(-likelihoods.mean()).item(),
                'likelihood_trend': self._compute_likelihood_trend(likelihoods)
            }
    
    def _compute_prediction_metrics(
        self, 
        X_estimated: torch.Tensor, 
        observations: torch.Tensor
    ) -> Dict[str, float]:
        """äºˆæ¸¬æ€§èƒ½æŒ‡æ¨™"""
        # ä¸€æœŸå…ˆäºˆæ¸¬èª¤å·®ï¼ˆç°¡ç•¥ç‰ˆï¼‰
        if X_estimated.size(0) > 1:
            pred_errors = []
            for t in range(1, X_estimated.size(0)):
                # ç°¡å˜ãªç·šå½¢äºˆæ¸¬
                if t >= 2:
                    predicted = X_estimated[t-1] + (X_estimated[t-1] - X_estimated[t-2])
                else:
                    predicted = X_estimated[t-1]
                actual = X_estimated[t]
                pred_errors.append(torch.norm(actual - predicted).item())
                
            return {
                'one_step_prediction_error': np.mean(pred_errors),
                'prediction_error_std': np.std(pred_errors),
                'prediction_stability': 1.0 / (1.0 + np.std(pred_errors))
            }
        else:
            return {'prediction_error': 0.0}
    
    def _compute_correlation(self, X_estimated: torch.Tensor, X_true: torch.Tensor) -> torch.Tensor:
        """çŠ¶æ…‹æ¨å®šã®ç›¸é–¢ä¿‚æ•°"""
        # å„æ¬¡å…ƒã§ã®ç›¸é–¢è¨ˆç®—
        correlations = []
        for dim in range(X_estimated.size(1)):
            est_dim = X_estimated[:, dim]
            true_dim = X_true[:, dim]
            
            # æ¨™æº–åŒ–
            est_norm = (est_dim - est_dim.mean()) / est_dim.std()
            true_norm = (true_dim - true_dim.mean()) / true_dim.std()
            
            # ç›¸é–¢è¨ˆç®—
            corr = (est_norm * true_norm).mean()
            correlations.append(corr)
            
        return torch.stack(correlations).mean()
    
    def _compute_likelihood_trend(self, likelihoods: torch.Tensor) -> float:
        """å°¤åº¦ã®å‚¾å‘åˆ†æ"""
        if len(likelihoods) < 3:
            return 0.0
            
        # ç·šå½¢å›å¸°ã«ã‚ˆã‚‹å‚¾å‘
        x = torch.arange(len(likelihoods), dtype=torch.float32)
        y = likelihoods
        
        # å‚¾ãã‚’è¨ˆç®—
        x_mean = x.mean()
        y_mean = y.mean()
        slope = ((x - x_mean) * (y - y_mean)).sum() / ((x - x_mean) ** 2).sum()
        
        return slope.item()
    
    def _print_metrics_summary(self, metrics: Dict) -> None:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹çµæœã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«å‡ºåŠ›"""
        print("\n" + "="*50)
        print("ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½è©•ä¾¡çµæœ")
        print("="*50)
        
        # åŸºæœ¬çµ±è¨ˆ
        if 'basic_stats' in metrics:
            stats = metrics['basic_stats']
            print(f"\nğŸ“ˆ åŸºæœ¬çµ±è¨ˆ:")
            print(f"  ç³»åˆ—é•·: {stats['sequence_length']}")
            print(f"  çŠ¶æ…‹æ¬¡å…ƒ: {stats['state_dimension']}")
            print(f"  å¹³å‡çŠ¶æ…‹ãƒãƒ«ãƒ : {stats['mean_state_norm']:.4f}")
            print(f"  çŠ¶æ…‹ãƒãƒ«ãƒ æ¨™æº–åå·®: {stats['std_state_norm']:.4f}")
        
        # ç²¾åº¦æŒ‡æ¨™
        if 'accuracy' in metrics:
            acc = metrics['accuracy']
            print(f"\nğŸ¯ æ¨å®šç²¾åº¦:")
            print(f"  MSE: {acc['mse']:.6f}")
            print(f"  MAE: {acc['mae']:.6f}")
            print(f"  RMSE: {acc['rmse']:.6f}")
            print(f"  ç›¸é–¢ä¿‚æ•°: {acc['correlation']:.4f}")
            print(f"  ç›¸å¯¾èª¤å·®: {acc['relative_error']:.4f}")
        
        # ä¸ç¢ºå®Ÿæ€§æŒ‡æ¨™
        if 'uncertainty' in metrics:
            unc = metrics['uncertainty']
            print(f"\nğŸ² ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–:")
            print(f"  å¹³å‡ä¸ç¢ºå®Ÿæ€§: {unc['mean_uncertainty']:.6f}")
            
            # ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡
            for key, value in unc.items():
                if key.startswith('coverage_') and not key.endswith('_error'):
                    conf_level = key.split('_')[1]
                    error_key = f'coverage_error_{conf_level}'
                    error = unc.get(error_key, 0.0)
                    print(f"  {conf_level}%ä¿¡é ¼åŒºé–“ã‚«ãƒãƒ¬ãƒƒã‚¸: {value:.4f} (èª¤å·®: {error:.4f})")
        
        # å°¤åº¦æŒ‡æ¨™
        if 'likelihood' in metrics:
            like = metrics['likelihood']
            print(f"\nğŸ“ˆ å°¤åº¦è©•ä¾¡:")
            print(f"  ç·å¯¾æ•°å°¤åº¦: {like['total_log_likelihood']:.2f}")
            print(f"  å¹³å‡å¯¾æ•°å°¤åº¦: {like['mean_log_likelihood']:.4f}")
            print(f"  ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£: {like['perplexity']:.4f}")
            
        print("\n" + "="*50)


class ComputationalMetrics:
    """è¨ˆç®—åŠ¹ç‡è©•ä¾¡ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.timing_results = {}
        self.memory_results = {}
        
    def measure_inference_time(
        self, 
        inference_func, 
        *args, 
        n_trials: int = 5,
        warmup: int = 2
    ) -> Dict[str, float]:
        """æ¨è«–æ™‚é–“æ¸¬å®š"""
        times = []
        
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        for _ in range(warmup):
            _ = inference_func(*args)
            
        # æ¸¬å®š
        for trial in range(n_trials):
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            start_time = time.perf_counter()
            
            result = inference_func(*args)
            
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            end_time = time.perf_counter()
            
            times.append(end_time - start_time)
            
        return {
            'mean_time': np.mean(times),
            'std_time': np.std(times),
            'min_time': np.min(times),
            'max_time': np.max(times),
            'trials': n_trials
        }
    
    def measure_memory_usage(self, function_to_measure, *args) -> Dict[str, float]:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®š"""
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.empty_cache()
            
            initial_memory = torch.cuda.memory_allocated()
            
            result = function_to_measure(*args)
            
            peak_memory = torch.cuda.max_memory_allocated()
            final_memory = torch.cuda.memory_allocated()
            
            return {
                'initial_memory_mb': initial_memory / (1024**2),
                'peak_memory_mb': peak_memory / (1024**2),
                'final_memory_mb': final_memory / (1024**2),
                'memory_increase_mb': (final_memory - initial_memory) / (1024**2),
                'peak_increase_mb': (peak_memory - initial_memory) / (1024**2)
            }
        else:
            return {'message': 'CUDA not available, memory measurement skipped'}


class CalibrationMetrics:
    """ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def compute_calibration_error(
        predictions: torch.Tensor,
        uncertainties: torch.Tensor,
        true_values: torch.Tensor,
        n_bins: int = 10
    ) -> float:
        """ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³èª¤å·®ã®è¨ˆç®—"""
        # ä¸ç¢ºå®Ÿæ€§ã‚’ç¢ºç‡ã«å¤‰æ›ï¼ˆæ­£è¦åˆ†å¸ƒä»®å®šï¼‰
        probabilities = torch.sigmoid(uncertainties)
        
        # ãƒ“ãƒ³ã”ã¨ã®åˆ†æ
        bin_boundaries = torch.linspace(0, 1, n_bins + 1)
        calibration_error = 0.0
        
        for i in range(n_bins):
            bin_lower = bin_boundaries[i]
            bin_upper = bin_boundaries[i + 1]
            
            # ãƒ“ãƒ³å†…ã®ã‚µãƒ³ãƒ—ãƒ«
            in_bin = (probabilities >= bin_lower) & (probabilities < bin_upper)
            if in_bin.sum() == 0:
                continue
                
            # æœŸå¾…ä¿¡é ¼åº¦ã¨å®Ÿéš›ã®ç²¾åº¦
            expected_confidence = probabilities[in_bin].mean()
            actual_accuracy = ((predictions[in_bin] - true_values[in_bin]).abs() < uncertainties[in_bin]).float().mean()
            
            # ãƒ“ãƒ³ã®é‡ã¿
            bin_weight = in_bin.sum().float() / len(probabilities)
            
            # èª¤å·®ç´¯ç©
            calibration_error += bin_weight * abs(expected_confidence - actual_accuracy)
            
        return calibration_error.item()


def create_metrics_evaluator(device: str = 'cpu') -> StateEstimationMetrics:
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹è©•ä¾¡å™¨ã®ä½œæˆ"""
    return StateEstimationMetrics(device=device)


def print_comparison_summary(
    method1_metrics: Dict, 
    method2_metrics: Dict, 
    method1_name: str = "Method 1",
    method2_name: str = "Method 2"
) -> None:
    """2æ‰‹æ³•ã®æ¯”è¼ƒçµæœã‚’å‡ºåŠ›"""
    print(f"\nğŸ” æ‰‹æ³•æ¯”è¼ƒ: {method1_name} vs {method2_name}")
    print("="*60)
    
    # ç²¾åº¦æ¯”è¼ƒ
    if 'accuracy' in method1_metrics and 'accuracy' in method2_metrics:
        acc1 = method1_metrics['accuracy']
        acc2 = method2_metrics['accuracy']
        
        print(f"\nğŸ“Š ç²¾åº¦æ¯”è¼ƒ:")
        print(f"  MSE:  {method1_name}: {acc1['mse']:.6f}  |  {method2_name}: {acc2['mse']:.6f}")
        print(f"  MAE:  {method1_name}: {acc1['mae']:.6f}  |  {method2_name}: {acc2['mae']:.6f}")
        print(f"  RMSE: {method1_name}: {acc1['rmse']:.6f}  |  {method2_name}: {acc2['rmse']:.6f}")
        
        # æ”¹å–„ç‡è¨ˆç®—
        mse_improvement = (acc1['mse'] - acc2['mse']) / acc1['mse'] * 100
        mae_improvement = (acc1['mae'] - acc2['mae']) / acc1['mae'] * 100
        
        print(f"\nğŸ’¡ æ”¹å–„ç‡ ({method2_name} vs {method1_name}):")
        print(f"  MSEæ”¹å–„: {mse_improvement:+.2f}%")
        print(f"  MAEæ”¹å–„: {mae_improvement:+.2f}%")
    
    print("="*60)