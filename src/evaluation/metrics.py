"""
è©•ä¾¡æŒ‡æ¨™è¨ˆç®—ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

DFIV Kalman Filterã®çŠ¶æ…‹æ¨å®šæ€§èƒ½ã‚’åŒ…æ‹¬çš„ã«è©•ä¾¡ã™ã‚‹ãŸã‚ã®æŒ‡æ¨™ã‚’æä¾›ã€‚
- æ¨å®šç²¾åº¦ï¼ˆMSE, MAE, RMSEï¼‰
- ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–å“è³ªï¼ˆã‚«ãƒãƒ¬ãƒƒã‚¸ç‡ã€åŒºé–“å¹…ï¼‰
- äºˆæ¸¬æ€§èƒ½ï¼ˆå¯¾æ•°å°¤åº¦ã€ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
- è¨ˆç®—åŠ¹ç‡ï¼ˆæ™‚é–“ã€ãƒ¡ãƒ¢ãƒªï¼‰
"""

import torch
import torch.nn.functional as F
import numpy as np
import time
import matplotlib.pyplot as plt
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union, Any
from scipy import stats
from sklearn.metrics import r2_score
import warnings


class StateEstimationMetrics:
    """çŠ¶æ…‹æ¨å®šæ€§èƒ½è©•ä¾¡ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, device: str = 'cpu'):
        self.device = torch.device(device)
        
    def compute_all_metrics(
        self,
        X_estimated: torch.Tensor,
        X_true: Optional[torch.Tensor] = None,
        X_covariances: Optional[torch.Tensor] = None,
        observations: Optional[torch.Tensor] = None,
        likelihoods: Optional[torch.Tensor] = None,
        verbose: bool = True
    ) -> Dict[str, Union[float, Dict]]:
        """
        åŒ…æ‹¬çš„è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—
        
        Args:
            X_estimated: æ¨å®šçŠ¶æ…‹ (T, r)
            X_true: çœŸã®çŠ¶æ…‹ (T, r) [optional]
            X_covariances: çŠ¶æ…‹å…±åˆ†æ•£ (T, r, r) [optional]
            observations: è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ (T, n) [optional]
            likelihoods: è¦³æ¸¬å°¤åº¦ (T,) [optional]
            verbose: ã‚¿ãƒ¼ãƒŸãƒŠãƒ«å‡ºåŠ›ã™ã‚‹ã‹ã©ã†ã‹
            
        Returns:
            Dict: å…¨è©•ä¾¡æŒ‡æ¨™
        """
        metrics = {}
        
        # åŸºæœ¬çµ±è¨ˆ
        metrics['basic_stats'] = self._compute_basic_stats(X_estimated)
        
        # ç²¾åº¦è©•ä¾¡ï¼ˆçœŸå€¤ãŒã‚ã‚‹å ´åˆï¼‰
        if X_true is not None:
            metrics['accuracy'] = self._compute_accuracy_metrics(X_estimated, X_true)
            
        # ä¸ç¢ºå®Ÿæ€§è©•ä¾¡ï¼ˆå…±åˆ†æ•£ãŒã‚ã‚‹å ´åˆï¼‰
        if X_covariances is not None:
            metrics['uncertainty'] = self._compute_uncertainty_metrics(
                X_estimated, X_covariances, X_true
            )
            
        # å°¤åº¦è©•ä¾¡
        if likelihoods is not None:
            metrics['likelihood'] = self._compute_likelihood_metrics(likelihoods)
            
        # äºˆæ¸¬æ€§èƒ½ï¼ˆè¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆï¼‰
        if observations is not None:
            metrics['prediction'] = self._compute_prediction_metrics(
                X_estimated, observations
            )
            
        # ã‚¿ãƒ¼ãƒŸãƒŠãƒ«å‡ºåŠ›
        if verbose:
            self._print_metrics_summary(metrics)
            
        return metrics
    
    def _compute_basic_stats(self, X_estimated: torch.Tensor) -> Dict[str, float]:
        """åŸºæœ¬çµ±è¨ˆæƒ…å ±"""
        with torch.no_grad():
            return {
                'sequence_length': X_estimated.size(0),
                'state_dimension': X_estimated.size(1),
                'mean_state_norm': torch.norm(X_estimated, dim=1).mean().item(),
                'std_state_norm': torch.norm(X_estimated, dim=1).std().item(),
                'max_state_value': X_estimated.max().item(),
                'min_state_value': X_estimated.min().item()
            }
    
    def _compute_accuracy_metrics(
        self, 
        X_estimated: torch.Tensor, 
        X_true: torch.Tensor
    ) -> Dict[str, float]:
        """æ¨å®šç²¾åº¦æŒ‡æ¨™"""
        with torch.no_grad():
            # ã‚¨ãƒ©ãƒ¼è¨ˆç®—
            errors = X_estimated - X_true
            squared_errors = errors ** 2
            abs_errors = torch.abs(errors)
            
            # æ¬¡å…ƒã”ã¨ã®æŒ‡æ¨™
            mse_per_dim = squared_errors.mean(dim=0)
            mae_per_dim = abs_errors.mean(dim=0)
            
            return {
                'mse': squared_errors.mean().item(),
                'mae': abs_errors.mean().item(),
                'rmse': torch.sqrt(squared_errors.mean()).item(),
                'mse_per_dimension': mse_per_dim.tolist(),
                'mae_per_dimension': mae_per_dim.tolist(),
                'relative_error': (torch.norm(errors, dim=1) / torch.norm(X_true, dim=1)).mean().item(),
                'correlation': self._compute_correlation(X_estimated, X_true).item()
            }
    
    def _compute_uncertainty_metrics(
        self,
        X_estimated: torch.Tensor,
        X_covariances: torch.Tensor,
        X_true: Optional[torch.Tensor] = None
    ) -> Dict[str, Union[float, List]]:
        """ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–å“è³ª"""
        with torch.no_grad():
            # æ¨™æº–åå·®æŠ½å‡º
            std_devs = torch.sqrt(torch.diagonal(X_covariances, dim1=1, dim2=2))
            
            metrics = {
                'mean_uncertainty': std_devs.mean().item(),
                'std_uncertainty': std_devs.std().item(),
                'uncertainty_per_dimension': std_devs.mean(dim=0).tolist(),
                'determinant_mean': torch.det(X_covariances).mean().item(),
                'trace_mean': torch.trace(X_covariances.view(-1, X_covariances.size(-1), X_covariances.size(-1))).mean().item()
            }
            
            # ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡è¨ˆç®—ï¼ˆçœŸå€¤ãŒã‚ã‚‹å ´åˆï¼‰
            if X_true is not None:
                coverage_results = self._compute_coverage_rates(
                    X_estimated, std_devs, X_true
                )
                metrics.update(coverage_results)
                
            return metrics
    
    def _compute_coverage_rates(
        self,
        X_estimated: torch.Tensor,
        std_devs: torch.Tensor,
        X_true: torch.Tensor,
        confidence_levels: List[float] = [0.68, 0.95, 0.99]
    ) -> Dict[str, float]:
        """ä¿¡é ¼åŒºé–“ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡"""
        coverage_results = {}
        
        for conf_level in confidence_levels:
            # Zå€¤è¨ˆç®—
            z_score = stats.norm.ppf((1 + conf_level) / 2)
            
            # ä¿¡é ¼åŒºé–“
            lower = X_estimated - z_score * std_devs
            upper = X_estimated + z_score * std_devs
            
            # ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ¤å®š
            covered = (X_true >= lower) & (X_true <= upper)
            coverage_rate = covered.all(dim=1).float().mean().item()
            
            coverage_results[f'coverage_{int(conf_level*100)}'] = coverage_rate
            coverage_results[f'coverage_error_{int(conf_level*100)}'] = abs(coverage_rate - conf_level)
            
        return coverage_results
    
    def _compute_likelihood_metrics(self, likelihoods: torch.Tensor) -> Dict[str, float]:
        """å°¤åº¦é–¢é€£æŒ‡æ¨™"""
        with torch.no_grad():
            return {
                'total_log_likelihood': likelihoods.sum().item(),
                'mean_log_likelihood': likelihoods.mean().item(),
                'std_log_likelihood': likelihoods.std().item(),
                'perplexity': torch.exp(-likelihoods.mean()).item(),
                'likelihood_trend': self._compute_likelihood_trend(likelihoods)
            }
    
    def _compute_prediction_metrics(
        self, 
        X_estimated: torch.Tensor, 
        observations: torch.Tensor
    ) -> Dict[str, float]:
        """äºˆæ¸¬æ€§èƒ½æŒ‡æ¨™"""
        # ä¸€æœŸå…ˆäºˆæ¸¬èª¤å·®ï¼ˆç°¡ç•¥ç‰ˆï¼‰
        if X_estimated.size(0) > 1:
            pred_errors = []
            for t in range(1, X_estimated.size(0)):
                # ç°¡å˜ãªç·šå½¢äºˆæ¸¬
                if t >= 2:
                    predicted = X_estimated[t-1] + (X_estimated[t-1] - X_estimated[t-2])
                else:
                    predicted = X_estimated[t-1]
                actual = X_estimated[t]
                pred_errors.append(torch.norm(actual - predicted).item())
                
            return {
                'one_step_prediction_error': np.mean(pred_errors),
                'prediction_error_std': np.std(pred_errors),
                'prediction_stability': 1.0 / (1.0 + np.std(pred_errors))
            }
        else:
            return {'prediction_error': 0.0}
    
    def _compute_correlation(self, X_estimated: torch.Tensor, X_true: torch.Tensor) -> torch.Tensor:
        """çŠ¶æ…‹æ¨å®šã®ç›¸é–¢ä¿‚æ•°"""
        # å„æ¬¡å…ƒã§ã®ç›¸é–¢è¨ˆç®—
        correlations = []
        for dim in range(X_estimated.size(1)):
            est_dim = X_estimated[:, dim]
            true_dim = X_true[:, dim]
            
            # æ¨™æº–åŒ–
            est_norm = (est_dim - est_dim.mean()) / est_dim.std()
            true_norm = (true_dim - true_dim.mean()) / true_dim.std()
            
            # ç›¸é–¢è¨ˆç®—
            corr = (est_norm * true_norm).mean()
            correlations.append(corr)
            
        return torch.stack(correlations).mean()
    
    def _compute_likelihood_trend(self, likelihoods: torch.Tensor) -> float:
        """å°¤åº¦ã®å‚¾å‘åˆ†æ"""
        if len(likelihoods) < 3:
            return 0.0
            
        # ç·šå½¢å›å¸°ã«ã‚ˆã‚‹å‚¾å‘
        x = torch.arange(len(likelihoods), dtype=torch.float32)
        y = likelihoods
        
        # å‚¾ãã‚’è¨ˆç®—
        x_mean = x.mean()
        y_mean = y.mean()
        slope = ((x - x_mean) * (y - y_mean)).sum() / ((x - x_mean) ** 2).sum()
        
        return slope.item()
    
    def _print_metrics_summary(self, metrics: Dict) -> None:
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹çµæœã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«å‡ºåŠ›"""
        print("\n" + "="*50)
        print("ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½è©•ä¾¡çµæœ")
        print("="*50)
        
        # åŸºæœ¬çµ±è¨ˆ
        if 'basic_stats' in metrics:
            stats = metrics['basic_stats']
            print(f"\nğŸ“ˆ åŸºæœ¬çµ±è¨ˆ:")
            print(f"  ç³»åˆ—é•·: {stats['sequence_length']}")
            print(f"  çŠ¶æ…‹æ¬¡å…ƒ: {stats['state_dimension']}")
            print(f"  å¹³å‡çŠ¶æ…‹ãƒãƒ«ãƒ : {stats['mean_state_norm']:.4f}")
            print(f"  çŠ¶æ…‹ãƒãƒ«ãƒ æ¨™æº–åå·®: {stats['std_state_norm']:.4f}")
        
        # ç²¾åº¦æŒ‡æ¨™
        if 'accuracy' in metrics:
            acc = metrics['accuracy']
            print(f"\næ¨å®šç²¾åº¦:")
            print(f"  MSE: {acc['mse']:.6f}")
            print(f"  MAE: {acc['mae']:.6f}")
            print(f"  RMSE: {acc['rmse']:.6f}")
            print(f"  ç›¸é–¢ä¿‚æ•°: {acc['correlation']:.4f}")
            print(f"  ç›¸å¯¾èª¤å·®: {acc['relative_error']:.4f}")
        
        # ä¸ç¢ºå®Ÿæ€§æŒ‡æ¨™
        if 'uncertainty' in metrics:
            unc = metrics['uncertainty']
            print(f"\nğŸ² ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–:")
            print(f"  å¹³å‡ä¸ç¢ºå®Ÿæ€§: {unc['mean_uncertainty']:.6f}")
            
            # ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡
            for key, value in unc.items():
                if key.startswith('coverage_') and not key.endswith('_error'):
                    conf_level = key.split('_')[1]
                    error_key = f'coverage_error_{conf_level}'
                    error = unc.get(error_key, 0.0)
                    print(f"  {conf_level}%ä¿¡é ¼åŒºé–“ã‚«ãƒãƒ¬ãƒƒã‚¸: {value:.4f} (èª¤å·®: {error:.4f})")
        
        # å°¤åº¦æŒ‡æ¨™
        if 'likelihood' in metrics:
            like = metrics['likelihood']
            print(f"\nğŸ“ˆ å°¤åº¦è©•ä¾¡:")
            print(f"  ç·å¯¾æ•°å°¤åº¦: {like['total_log_likelihood']:.2f}")
            print(f"  å¹³å‡å¯¾æ•°å°¤åº¦: {like['mean_log_likelihood']:.4f}")
            print(f"  ãƒ‘ãƒ¼ãƒ—ãƒ¬ã‚­ã‚·ãƒ†ã‚£: {like['perplexity']:.4f}")
            
        print("\n" + "="*50)


class ComputationalMetrics:
    """è¨ˆç®—åŠ¹ç‡è©•ä¾¡ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.timing_results = {}
        self.memory_results = {}
        
    def measure_inference_time(
        self, 
        inference_func, 
        *args, 
        n_trials: int = 5,
        warmup: int = 2
    ) -> Dict[str, float]:
        """æ¨è«–æ™‚é–“æ¸¬å®š"""
        times = []
        
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        for _ in range(warmup):
            _ = inference_func(*args)
            
        # æ¸¬å®š
        for trial in range(n_trials):
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            start_time = time.perf_counter()
            
            result = inference_func(*args)
            
            torch.cuda.synchronize() if torch.cuda.is_available() else None
            end_time = time.perf_counter()
            
            times.append(end_time - start_time)
            
        return {
            'mean_time': np.mean(times),
            'std_time': np.std(times),
            'min_time': np.min(times),
            'max_time': np.max(times),
            'trials': n_trials
        }
    
    def measure_memory_usage(self, function_to_measure, *args) -> Dict[str, float]:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®š"""
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            torch.cuda.empty_cache()
            
            initial_memory = torch.cuda.memory_allocated()
            
            result = function_to_measure(*args)
            
            peak_memory = torch.cuda.max_memory_allocated()
            final_memory = torch.cuda.memory_allocated()
            
            return {
                'initial_memory_mb': initial_memory / (1024**2),
                'peak_memory_mb': peak_memory / (1024**2),
                'final_memory_mb': final_memory / (1024**2),
                'memory_increase_mb': (final_memory - initial_memory) / (1024**2),
                'peak_increase_mb': (peak_memory - initial_memory) / (1024**2)
            }
        else:
            return {'message': 'CUDA not available, memory measurement skipped'}


class CalibrationMetrics:
    """ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è©•ä¾¡ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def compute_calibration_error(
        predictions: torch.Tensor,
        uncertainties: torch.Tensor,
        true_values: torch.Tensor,
        n_bins: int = 10
    ) -> float:
        """ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³èª¤å·®ã®è¨ˆç®—"""
        # ä¸ç¢ºå®Ÿæ€§ã‚’ç¢ºç‡ã«å¤‰æ›ï¼ˆæ­£è¦åˆ†å¸ƒä»®å®šï¼‰
        probabilities = torch.sigmoid(uncertainties)
        
        # ãƒ“ãƒ³ã”ã¨ã®åˆ†æ
        bin_boundaries = torch.linspace(0, 1, n_bins + 1)
        calibration_error = 0.0
        
        for i in range(n_bins):
            bin_lower = bin_boundaries[i]
            bin_upper = bin_boundaries[i + 1]
            
            # ãƒ“ãƒ³å†…ã®ã‚µãƒ³ãƒ—ãƒ«
            in_bin = (probabilities >= bin_lower) & (probabilities < bin_upper)
            if in_bin.sum() == 0:
                continue
                
            # æœŸå¾…ä¿¡é ¼åº¦ã¨å®Ÿéš›ã®ç²¾åº¦
            expected_confidence = probabilities[in_bin].mean()
            actual_accuracy = ((predictions[in_bin] - true_values[in_bin]).abs() < uncertainties[in_bin]).float().mean()
            
            # ãƒ“ãƒ³ã®é‡ã¿
            bin_weight = in_bin.sum().float() / len(probabilities)
            
            # èª¤å·®ç´¯ç©
            calibration_error += bin_weight * abs(expected_confidence - actual_accuracy)
            
        return calibration_error.item()


def create_metrics_evaluator(device: str = 'cpu') -> StateEstimationMetrics:
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹è©•ä¾¡å™¨ã®ä½œæˆ"""
    return StateEstimationMetrics(device=device)


def print_comparison_summary(
    method1_metrics: Dict,
    method2_metrics: Dict,
    method1_name: str = "Method 1",
    method2_name: str = "Method 2"
) -> None:
    """2æ‰‹æ³•ã®æ¯”è¼ƒçµæœã‚’å‡ºåŠ›"""
    print(f"\nğŸ” æ‰‹æ³•æ¯”è¼ƒ: {method1_name} vs {method2_name}")
    print("="*60)

    # ç²¾åº¦æ¯”è¼ƒ
    if 'accuracy' in method1_metrics and 'accuracy' in method2_metrics:
        acc1 = method1_metrics['accuracy']
        acc2 = method2_metrics['accuracy']

        print(f"\nç²¾åº¦æ¯”è¼ƒ:")
        print(f"  MSE:  {method1_name}: {acc1['mse']:.6f}  |  {method2_name}: {acc2['mse']:.6f}")
        print(f"  MAE:  {method1_name}: {acc1['mae']:.6f}  |  {method2_name}: {acc2['mae']:.6f}")
        print(f"  RMSE: {method1_name}: {acc1['rmse']:.6f}  |  {method2_name}: {acc2['rmse']:.6f}")

        # æ”¹å–„ç‡è¨ˆç®—
        mse_improvement = (acc1['mse'] - acc2['mse']) / acc1['mse'] * 100
        mae_improvement = (acc1['mae'] - acc2['mae']) / acc1['mae'] * 100

        print(f"\næ”¹å–„ç‡ ({method2_name} vs {method1_name}):")
        print(f"  MSEæ”¹å–„: {mse_improvement:+.2f}%")
        print(f"  MAEæ”¹å–„: {mae_improvement:+.2f}%")

    print("="*60)


# =======================================
# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆäºˆæ¸¬è©•ä¾¡æ©Ÿèƒ½ï¼ˆStep 6è¿½åŠ ï¼‰
# =======================================

class TargetPredictionMetrics:
    """
    ã‚¿ãƒ¼ã‚²ãƒƒãƒˆäºˆæ¸¬è©•ä¾¡ã‚¯ãƒ©ã‚¹ï¼ˆæ—¢å­˜StateEstimationMetricsã¨çµ±ä¸€è¨­è¨ˆï¼‰

    RKN ã‚¿ãƒ¼ã‚²ãƒƒãƒˆäºˆæ¸¬å®Ÿé¨“ç”¨ã®è©•ä¾¡æŒ‡æ¨™è¨ˆç®—ãƒ»å¯è¦–åŒ–ã‚’æä¾›ã€‚
    æ—¢å­˜ã® StateEstimationMetrics ã‚¯ãƒ©ã‚¹ã¨çµ±ä¸€ã—ãŸã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æ¡ç”¨ã€‚

    æ©Ÿèƒ½:
    - é¸æŠå¯èƒ½ãªè©•ä¾¡æŒ‡æ¨™è¨ˆç®— (RMSE, MAE, RÂ², æ¬¡å…ƒåˆ¥RÂ²)
    - æŒ‡æ¨™åˆ¥ç‹¬ç«‹å¯è¦–åŒ– (è‹±èªãƒ©ãƒ™ãƒ«)
    - ç ”ç©¶ç™ºè¡¨ç”¨é«˜å“è³ªãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆ
    - æ—¢å­˜ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨çµ±ä¸€ã•ã‚ŒãŸã‚¿ãƒ¼ãƒŸãƒŠãƒ«å‡ºåŠ›
    """

    def __init__(self, device: str = 'cpu'):
        """
        åˆæœŸåŒ–

        Args:
            device: è¨ˆç®—ãƒ‡ãƒã‚¤ã‚¹ ('cpu' or 'cuda')
        """
        self.device = torch.device(device)

    def compute_target_metrics(
        self,
        y_true: torch.Tensor,
        y_pred: torch.Tensor,
        metrics: List[str] = ['rmse'],
        verbose: bool = True
    ) -> Dict[str, Union[float, List[float]]]:
        """
        ã‚¿ãƒ¼ã‚²ãƒƒãƒˆäºˆæ¸¬è©•ä¾¡æŒ‡æ¨™è¨ˆç®—ï¼ˆæ—¢å­˜ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼‰

        Args:
            y_true: çœŸå€¤ãƒ†ãƒ³ã‚½ãƒ« (T, d)
            y_pred: äºˆæ¸¬å€¤ãƒ†ãƒ³ã‚½ãƒ« (T, d)
            metrics: è¨ˆç®—ã™ã‚‹æŒ‡æ¨™ã®ãƒªã‚¹ãƒˆ ['rmse', 'mae', 'r2', 'r2_per_dim']
            verbose: ã‚¿ãƒ¼ãƒŸãƒŠãƒ«å‡ºåŠ›ã™ã‚‹ã‹ã©ã†ã‹

        Returns:
            Dict: è©•ä¾¡æŒ‡æ¨™çµæœ

        ä¾‹:
            target_evaluator = TargetPredictionMetrics()
            metrics = target_evaluator.compute_target_metrics(
                y_true, y_pred, metrics=['rmse', 'mae'], verbose=True
            )
            # {'rmse': 0.1234, 'mae': 0.0987}
        """
        # å…¥åŠ›æ¤œè¨¼
        if y_true.shape != y_pred.shape:
            raise ValueError(f"Shape mismatch: y_true {y_true.shape} vs y_pred {y_pred.shape}")

        # ãƒ‡ãƒã‚¤ã‚¹çµ±ä¸€
        y_true = y_true.to(self.device)
        y_pred = y_pred.to(self.device)

        # åˆ©ç”¨å¯èƒ½ãªè©•ä¾¡æŒ‡æ¨™å®šç¾©
        available_metrics = {
            'rmse': lambda: torch.sqrt(F.mse_loss(y_pred, y_true)).item(),
            'mae': lambda: F.l1_loss(y_pred, y_true).item(),
            'r2': lambda: self._compute_r2_score(y_true, y_pred),
            'r2_per_dim': lambda: self._compute_r2_per_dimension(y_true, y_pred)
        }

        # æŒ‡å®šã•ã‚ŒãŸæŒ‡æ¨™ã®ã¿è¨ˆç®—
        results = {}
        for metric in metrics:
            if metric in available_metrics:
                try:
                    results[metric] = available_metrics[metric]()
                except Exception as e:
                    print(f"æŒ‡æ¨™ '{metric}' ã®è¨ˆç®—ã§ã‚¨ãƒ©ãƒ¼: {e}")
                    results[metric] = None
            else:
                print(f"æœªçŸ¥ã®æŒ‡æ¨™: '{metric}'. åˆ©ç”¨å¯èƒ½: {list(available_metrics.keys())}")

        # ã‚¿ãƒ¼ãƒŸãƒŠãƒ«å‡ºåŠ›ï¼ˆæ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨çµ±ä¸€ï¼‰
        if verbose:
            self._print_target_metrics_summary(results)

        return results

    def save_target_metrics_results(
        self,
        results: Dict[str, Union[float, List[float]]],
        output_dir: str,
        experiment_info: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        ã‚¿ãƒ¼ã‚²ãƒƒãƒˆäºˆæ¸¬è©•ä¾¡çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜

        Args:
            results: compute_target_metrics()ã®çµæœ
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            experiment_info: è¿½åŠ ã®å®Ÿé¨“æƒ…å ±ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        import json
        from datetime import datetime
        from pathlib import Path

        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
        save_data = {
            'timestamp': datetime.now().isoformat(),
            'evaluation_type': 'target_prediction',
            'metrics': results,
            'experiment_info': experiment_info or {}
        }

        # JSONä¿å­˜ï¼ˆæ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨çµ±ä¸€ï¼‰
        save_file = output_path / 'target_prediction_metrics.json'
        with open(save_file, 'w') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)

        print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆäºˆæ¸¬è©•ä¾¡çµæœä¿å­˜: {save_file}")
        return str(save_file)

    def create_target_visualizations(
        self,
        y_true: torch.Tensor,
        y_pred: torch.Tensor,
        metrics: List[str] = ['rmse'],
        output_dir: Optional[str] = None
    ) -> List[str]:
        """
        ã‚¿ãƒ¼ã‚²ãƒƒãƒˆäºˆæ¸¬å¯è¦–åŒ–ï¼ˆæ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨çµ±ä¸€ï¼‰

        Args:
            y_true: çœŸå€¤ãƒ†ãƒ³ã‚½ãƒ«
            y_pred: äºˆæ¸¬å€¤ãƒ†ãƒ³ã‚½ãƒ«
            metrics: å¯è¦–åŒ–ã™ã‚‹æŒ‡æ¨™ãƒªã‚¹ãƒˆ
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆNoneã®å ´åˆã¯ä¿å­˜ã—ãªã„ï¼‰

        Returns:
            List[str]: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ

        ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ä¾‹:
            - target_prediction_rmse.png
            - target_prediction_mae.png
            - target_prediction_r2.png
            - target_prediction_r2_per_dim.png
        """
        # TODO: å®Ÿè£…æ¤œè¨ä¸­
        # å¯è¦–åŒ–æ©Ÿèƒ½ã¯ä¸€æ—¦ç„¡åŠ¹åŒ–ã—ã€æ•°å€¤å‡ºåŠ›ãƒ»ä¿å­˜ã®ã¿ã«å¤‰æ›´
        print(f"å¯è¦–åŒ–å‡¦ç†ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ•°å€¤å‡ºåŠ›ãƒ»ä¿å­˜æ©Ÿèƒ½ã§ä»£æ›¿ï¼‰")

        generated_files = []
        # å¯è¦–åŒ–ã‚³ãƒ¼ãƒ‰ã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
        # for metric in metrics:
        #     if output_dir:
        #         save_path = Path(output_dir) / f"target_prediction_{metric}.png"
        #     else:
        #         save_path = None
        #
        #     try:
        #         self._plot_individual_metric(y_true, y_pred, metric, save_path)
        #         if save_path:
        #             generated_files.append(str(save_path))
        #     except Exception as e:
        #         print(f"æŒ‡æ¨™ '{metric}' ã®å¯è¦–åŒ–ã§ã‚¨ãƒ©ãƒ¼: {e}")
        #
        # if output_dir and generated_files:
        #     print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆäºˆæ¸¬å¯è¦–åŒ–ç”Ÿæˆå®Œäº†: {len(generated_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«")
        #     print(f"ğŸ“ ä¿å­˜å…ˆ: {output_dir}")

        return generated_files

    def _compute_r2_score(self, y_true: torch.Tensor, y_pred: torch.Tensor) -> float:
        """RÂ²ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆå…¨ä½“ï¼‰"""
        try:
            y_true_np = y_true.cpu().numpy()
            y_pred_np = y_pred.cpu().numpy()
            return float(r2_score(y_true_np, y_pred_np, multioutput='uniform_average'))
        except Exception as e:
            print(f"RÂ²è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0

    def _compute_r2_per_dimension(self, y_true: torch.Tensor, y_pred: torch.Tensor) -> List[float]:
        """æ¬¡å…ƒåˆ¥RÂ²ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        try:
            y_true_np = y_true.cpu().numpy()
            y_pred_np = y_pred.cpu().numpy()
            r2_values = r2_score(y_true_np, y_pred_np, multioutput='raw_values')
            return r2_values.tolist()
        except Exception as e:
            print(f"æ¬¡å…ƒåˆ¥RÂ²è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return [0.0]

    def _plot_individual_metric(
        self,
        y_true: torch.Tensor,
        y_pred: torch.Tensor,
        metric: str,
        save_path: Optional[Path] = None,
        max_samples: int = 200
    ) -> None:
        """
        å€‹åˆ¥æŒ‡æ¨™ç”¨å¯è¦–åŒ–ï¼ˆè‹±èªãƒ©ãƒ™ãƒ«ï¼‰

        å„æŒ‡æ¨™ã«æœ€é©åŒ–ã•ã‚ŒãŸå¯è¦–åŒ–:
        - rmse: æ™‚ç³»åˆ—æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆ
        - mae: çµ¶å¯¾èª¤å·®åˆ†å¸ƒãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
        - r2: æ•£å¸ƒå›³ï¼ˆçœŸå€¤ vs äºˆæ¸¬å€¤ï¼‰
        - r2_per_dim: æ¬¡å…ƒåˆ¥RÂ²æ£’ã‚°ãƒ©ãƒ•
        """
        # TODO: å®Ÿè£…æ¤œè¨ä¸­
        # å¯è¦–åŒ–æ©Ÿèƒ½ã¯ä¸€æ—¦ç„¡åŠ¹åŒ–ã€æ•°å€¤å‡ºåŠ›ãƒ»ä¿å­˜æ©Ÿèƒ½ã§ä»£æ›¿
        print(f"å¯è¦–åŒ–æ©Ÿèƒ½ï¼ˆ{metric}ï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸ")

        # å…¨ã¦ã®å¯è¦–åŒ–ã‚³ãƒ¼ãƒ‰ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
        # plt.style.use('default')  # ã‚¹ã‚¿ã‚¤ãƒ«åˆæœŸåŒ–

        # if metric == 'rmse':
        #     # RMSEç”¨æ™‚ç³»åˆ—æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆ
        #     plt.figure(figsize=(12, 6))
        #     n_samples = min(len(y_true), max_samples)
        #
        #     # æœ€åˆã®æ¬¡å…ƒã®ã¿ãƒ—ãƒ­ãƒƒãƒˆï¼ˆè¤‡æ•°æ¬¡å…ƒã®å ´åˆï¼‰
        #     if y_true.dim() > 1 and y_true.shape[1] > 1:
        #         true_values = y_true[:n_samples, 0].cpu().numpy()
        #         pred_values = y_pred[:n_samples, 0].cpu().numpy()
        #         plt.title(f'Time Series Comparison (RMSE Analysis) - Dimension 0')
        #     else:
        #         true_values = y_true[:n_samples].cpu().numpy()
        #         pred_values = y_pred[:n_samples].cpu().numpy()
        #         plt.title('Time Series Comparison (RMSE Analysis)')
        #
        #     plt.plot(true_values, label='True Values', alpha=0.8, linewidth=1.5)
        #     plt.plot(pred_values, label='Predicted Values', alpha=0.8, linewidth=1.5)
        #     plt.xlabel('Time Step')
        #     plt.ylabel('Value')
        #     plt.legend()
        #     plt.grid(True, alpha=0.3)
        #
        # elif metric == 'mae':
        #     # MAEç”¨çµ¶å¯¾èª¤å·®åˆ†å¸ƒãƒ—ãƒ­ãƒƒãƒˆ
        #     plt.figure(figsize=(10, 6))
        #     absolute_errors = torch.abs(y_pred - y_true).cpu().numpy()
        #
        #     plt.hist(absolute_errors.flatten(), bins=50, alpha=0.7, edgecolor='black', linewidth=0.5)
        #     plt.title('Absolute Error Distribution (MAE Analysis)')
        #     plt.xlabel('Absolute Error')
        #     plt.ylabel('Frequency')
        #     plt.grid(True, alpha=0.3)
        #
        #     # çµ±è¨ˆæƒ…å ±ã‚’è¿½åŠ 
        #     mean_error = np.mean(absolute_errors)
        #     plt.axvline(mean_error, color='red', linestyle='--',
        #                label=f'Mean Absolute Error: {mean_error:.4f}')
        #     plt.legend()
        #
        # elif metric == 'r2':
        #     # RÂ²ç”¨æ•£å¸ƒå›³
        #     plt.figure(figsize=(8, 8))
        #
        #     y_true_flat = y_true.cpu().numpy().flatten()
        #     y_pred_flat = y_pred.cpu().numpy().flatten()
        #
        #     plt.scatter(y_true_flat, y_pred_flat, alpha=0.5, s=10)
        #
        #     # å®Œå…¨äºˆæ¸¬ç·š
        #     min_val = min(y_true_flat.min(), y_pred_flat.min())
        #     max_val = max(y_true_flat.max(), y_pred_flat.max())
        #     plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')
        #
        #     plt.title('Prediction Accuracy Scatter (RÂ² Analysis)')
        #     plt.xlabel('True Values')
        #     plt.ylabel('Predicted Values')
        #     plt.legend()
        #     plt.grid(True, alpha=0.3)
        #     plt.axis('equal')
        #
        # elif metric == 'r2_per_dim':
        #     # æ¬¡å…ƒåˆ¥RÂ²æ£’ã‚°ãƒ©ãƒ•
        #     plt.figure(figsize=(10, 6))
        #
        #     try:
        #         r2_values = self._compute_r2_per_dimension(y_true, y_pred)
        #         dimensions = range(len(r2_values))
        #
        #         bars = plt.bar(dimensions, r2_values, alpha=0.7, edgecolor='black', linewidth=0.5)
        #         plt.title('RÂ² Score per Dimension')
        #         plt.xlabel('Dimension')
        #         plt.ylabel('RÂ² Score')
        #         plt.grid(True, alpha=0.3)
        #
        #         # ã‚«ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆRÂ²å€¤ã«ã‚ˆã£ã¦è‰²åˆ†ã‘ï¼‰
        #         for bar, r2_val in zip(bars, r2_values):
        #             if r2_val < 0.3:
        #                 bar.set_color('red')
        #             elif r2_val < 0.7:
        #                 bar.set_color('orange')
        #             else:
        #                 bar.set_color('green')
        #
        #         # å¹³å‡RÂ²ãƒ©ã‚¤ãƒ³
        #         mean_r2 = np.mean(r2_values)
        #         plt.axhline(mean_r2, color='blue', linestyle='--',
        #                    label=f'Mean RÂ²: {mean_r2:.3f}')
        #         plt.legend()
        #
        #     except Exception as e:
        #         plt.text(0.5, 0.5, f'Error computing RÂ² per dimension: {e}',
        #                 ha='center', va='center', transform=plt.gca().transAxes)
        #         plt.title('RÂ² Score per Dimension (Error)')
        #
        # else:
        #     # æœªçŸ¥ã®æŒ‡æ¨™
        #     plt.figure(figsize=(8, 6))
        #     plt.text(0.5, 0.5, f'Unknown metric: {metric}\nAvailable: rmse, mae, r2, r2_per_dim',
        #             ha='center', va='center', transform=plt.gca().transAxes, fontsize=14)
        #     plt.title(f'Unsupported Metric: {metric}')
        #
        # # ä¿å­˜å‡¦ç†
        # if save_path:
        #     save_path.parent.mkdir(parents=True, exist_ok=True)
        #     plt.savefig(save_path, dpi=300, bbox_inches='tight')
        #     print(f"å¯è¦–åŒ–ä¿å­˜: {save_path}")
        #
        # plt.close()  # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯é˜²æ­¢

    def _print_target_metrics_summary(self, metrics: Dict[str, Union[float, List[float]]]) -> None:
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆäºˆæ¸¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹çµæœã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«å‡ºåŠ›ï¼ˆæ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨çµ±ä¸€ï¼‰"""
        print("\n" + "="*50)
        print("ã‚¿ãƒ¼ã‚²ãƒƒãƒˆäºˆæ¸¬è©•ä¾¡çµæœ")
        print("="*50)

        for metric_name, value in metrics.items():
            if value is None:
                print(f"  {metric_name.upper()}: è¨ˆç®—ã‚¨ãƒ©ãƒ¼")
            elif isinstance(value, list):
                if metric_name == 'r2_per_dim':
                    print(f"  RÂ² PER DIMENSION:")
                    for i, dim_r2 in enumerate(value):
                        print(f"    Dim {i}: {dim_r2:.4f}")
                    print(f"    Average: {np.mean(value):.4f}")
                else:
                    print(f"  {metric_name.upper()}: {value}")
            else:
                print(f"  {metric_name.upper()}: {value:.4f}")

        print("="*50)


# =======================================
# å†æ§‹æˆè©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆStep 8å®Ÿè£…ï¼‰
# =======================================

class ReconstructionMetrics:
    """
    ãƒ‡ãƒ¼ã‚¿å†æ§‹æˆè©•ä¾¡ã‚¯ãƒ©ã‚¹ï¼ˆTargetPredictionMetricsã¨çµ±ä¸€è¨­è¨ˆï¼‰

    ä»»æ„ã®ãƒ‡ãƒ¼ã‚¿å‹ï¼ˆç”»åƒãƒ»æ™‚ç³»åˆ—ãƒ»ãƒ™ã‚¯ãƒˆãƒ«ç­‰ï¼‰ã®å†æ§‹æˆå®Ÿé¨“ç”¨è©•ä¾¡æŒ‡æ¨™è¨ˆç®—ãƒ»å¯è¦–åŒ–ã‚’æä¾›ã€‚
    æ—¢å­˜ã® TargetPredictionMetrics ã‚¯ãƒ©ã‚¹ã¨çµ±ä¸€ã—ãŸã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æ¡ç”¨ã€‚

    æ©Ÿèƒ½:
    - é¸æŠå¯èƒ½ãªè©•ä¾¡æŒ‡æ¨™è¨ˆç®— (reconstruction_rmse, psnr, temporal_correlation)
    - ä»»æ„ã®ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶å¯¾å¿œ (ç”»åƒ: T,H,W,C / æ™‚ç³»åˆ—: T,d / ãã®ä»–)
    - æŒ‡æ¨™åˆ¥ç‹¬ç«‹å¯è¦–åŒ– (è‹±èªãƒ©ãƒ™ãƒ«)
    - ç ”ç©¶ç™ºè¡¨ç”¨é«˜å“è³ªãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆ
    - æ—¢å­˜ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨çµ±ä¸€ã•ã‚ŒãŸã‚¿ãƒ¼ãƒŸãƒŠãƒ«å‡ºåŠ›
    """

    def __init__(self, device: str = 'cpu'):
        """
        åˆæœŸåŒ–

        Args:
            device: è¨ˆç®—ãƒ‡ãƒã‚¤ã‚¹ ('cpu' or 'cuda')
        """
        self.device = torch.device(device)

    def compute_reconstruction_metrics(
        self,
        y_true: torch.Tensor,
        y_pred: torch.Tensor,
        metrics: List[str] = ['reconstruction_rmse'],
        verbose: bool = True
    ) -> Dict[str, float]:
        """
        ãƒ‡ãƒ¼ã‚¿å†æ§‹æˆè©•ä¾¡æŒ‡æ¨™è¨ˆç®—ï¼ˆæ—¢å­˜ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼‰

        Args:
            y_true: çœŸå€¤ãƒ†ãƒ³ã‚½ãƒ« (ä»»æ„å½¢çŠ¶: T,H,W,C / T,d / etc.)
            y_pred: äºˆæ¸¬ãƒ†ãƒ³ã‚½ãƒ« (ä»»æ„å½¢çŠ¶: y_trueã¨åŒã˜)
            metrics: è¨ˆç®—ã™ã‚‹æŒ‡æ¨™ã®ãƒªã‚¹ãƒˆ ['reconstruction_rmse', 'psnr', 'temporal_correlation']
            verbose: ã‚¿ãƒ¼ãƒŸãƒŠãƒ«å‡ºåŠ›ã™ã‚‹ã‹ã©ã†ã‹

        Returns:
            Dict: å†æ§‹æˆè©•ä¾¡æŒ‡æ¨™çµæœ

        ä¾‹:
            reconstruction_evaluator = ReconstructionMetrics()
            metrics = reconstruction_evaluator.compute_reconstruction_metrics(
                y_true, y_pred, metrics=['reconstruction_rmse', 'psnr'], verbose=True
            )
            # {'reconstruction_rmse': 0.1234, 'psnr': 25.67}
        """
        # ãƒ‡ãƒã‚¤ã‚¹ç§»è¡Œãƒ»å½¢çŠ¶èª¿æ•´
        y_true = y_true.to(self.device).detach()
        y_pred = y_pred.to(self.device).detach()

        # å½¢çŠ¶ç¢ºèªãƒ»è­¦å‘Š
        if y_true.shape != y_pred.shape:
            print(f"å½¢çŠ¶ä¸ä¸€è‡´: y_true{y_true.shape} vs y_pred{y_pred.shape}")
            return {'error': 1.0}

        results = {}

        for metric in metrics:
            try:
                if metric == 'reconstruction_rmse':
                    results[metric] = self._compute_reconstruction_rmse(y_true, y_pred)
                elif metric == 'psnr':
                    results[metric] = self._compute_psnr(y_true, y_pred)
                elif metric == 'temporal_correlation':
                    results[metric] = self._compute_temporal_correlation(y_true, y_pred)
                else:
                    print(f"æœªçŸ¥ã®æŒ‡æ¨™: '{metric}'")
                    results[metric] = 0.0

            except Exception as e:
                print(f"æŒ‡æ¨™ '{metric}' è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                results[metric] = 0.0

        # ã‚¿ãƒ¼ãƒŸãƒŠãƒ«å‡ºåŠ›
        if verbose:
            self._print_reconstruction_metrics_summary(results)

        return results

    def create_reconstruction_visualizations(
        self,
        y_true: torch.Tensor,
        y_pred: torch.Tensor,
        metrics: List[str] = ['reconstruction_rmse'],
        output_dir: str = None
    ) -> List[str]:
        """
        å†æ§‹æˆå¯è¦–åŒ–ç”Ÿæˆï¼ˆæ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨çµ±ä¸€ï¼‰

        Args:
            y_true: çœŸå€¤ãƒ†ãƒ³ã‚½ãƒ«
            y_pred: äºˆæ¸¬ãƒ†ãƒ³ã‚½ãƒ«
            metrics: å¯è¦–åŒ–ã™ã‚‹æŒ‡æ¨™ã®ãƒªã‚¹ãƒˆ
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

        Returns:
            List[str]: ç”Ÿæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        """
        # TODO: å¯è¦–åŒ–æ©Ÿèƒ½ã¯æ®µéšçš„å®Ÿè£…
        # Step 8ã§ã¯è©•ä¾¡æŒ‡æ¨™è¨ˆç®—ã«é›†ä¸­ã—ã€å¯è¦–åŒ–ã¯å¾Œç¶šã§å®Ÿè£…
        print(f"å¯è¦–åŒ–å‡¦ç†ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ•°å€¤å‡ºåŠ›ãƒ»ä¿å­˜æ©Ÿèƒ½ã§ä»£æ›¿ï¼‰")

        generated_files = []
        return generated_files

    def _compute_reconstruction_rmse(self, y_true: torch.Tensor, y_pred: torch.Tensor) -> float:
        """å†æ§‹æˆRMSEè¨ˆç®—ï¼ˆä»»æ„ãƒ‡ãƒ¼ã‚¿å‹å¯¾å¿œï¼‰"""
        mse = torch.mean((y_true - y_pred) ** 2).item()
        rmse = mse ** 0.5
        return float(rmse)

    def _compute_psnr(self, y_true: torch.Tensor, y_pred: torch.Tensor) -> float:
        """PSNRè¨ˆç®—ï¼ˆPeak Signal-to-Noise Ratioï¼‰ä»»æ„ãƒ‡ãƒ¼ã‚¿å‹å¯¾å¿œ"""
        try:
            # MSEè¨ˆç®—
            mse = torch.mean((y_true - y_pred) ** 2).item()
            if mse == 0:
                return float('inf')  # å®Œå…¨ä¸€è‡´

            # ãƒ‡ãƒ¼ã‚¿ç¯„å›²è‡ªå‹•æ¨å®š
            data_range = torch.max(y_true).item() - torch.min(y_true).item()
            if data_range <= 0:
                return float('inf')

            psnr = 20 * torch.log10(torch.tensor(data_range)) - 10 * torch.log10(torch.tensor(mse))
            return float(psnr.item())

        except Exception as e:
            print(f"PSNRè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0

    def _compute_temporal_correlation(self, y_true: torch.Tensor, y_pred: torch.Tensor) -> float:
        """æ™‚ç³»åˆ—å¾©å…ƒç›¸é–¢ä¿‚æ•°è¨ˆç®—ï¼ˆä»»æ„ãƒ‡ãƒ¼ã‚¿å‹å¯¾å¿œï¼‰"""
        try:
            # æ™‚é–“è»¸ï¼ˆç¬¬0æ¬¡å…ƒï¼‰ã§ã®ç›¸é–¢ä¿‚æ•°ã‚’è¨ˆç®—
            if len(y_true.shape) < 2:
                # 1æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã®å ´åˆã€å…¨ä½“ã§ã®ç›¸é–¢
                if torch.std(y_true) > 1e-8 and torch.std(y_pred) > 1e-8:
                    corr = torch.corrcoef(torch.stack([y_true, y_pred]))[0, 1].item()
                    return float(corr) if not torch.isnan(torch.tensor(corr)) else 0.0
                return 0.0

            # å¤šæ¬¡å…ƒãƒ‡ãƒ¼ã‚¿: å„æ™‚åˆ»ã§ã®ç›¸é–¢ä¿‚æ•°ã‚’è¨ˆç®—ã—ã€å¹³å‡ã‚’å–ã‚‹
            correlations = []
            for t in range(y_true.shape[0]):  # æ™‚é–“è»¸ã§ãƒ«ãƒ¼ãƒ—
                true_t = y_true[t].flatten()
                pred_t = y_pred[t].flatten()

                # ãƒ”ã‚¢ã‚½ãƒ³ç›¸é–¢ä¿‚æ•°
                if torch.std(true_t) > 1e-8 and torch.std(pred_t) > 1e-8:
                    corr = torch.corrcoef(torch.stack([true_t, pred_t]))[0, 1].item()
                    if not torch.isnan(torch.tensor(corr)):
                        correlations.append(corr)

            if correlations:
                return float(sum(correlations) / len(correlations))
            else:
                return 0.0

        except Exception as e:
            print(f"æ™‚ç³»åˆ—ç›¸é–¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0.0

    def _print_reconstruction_metrics_summary(self, results: Dict[str, float]):
        """å†æ§‹æˆè©•ä¾¡çµæœã‚¿ãƒ¼ãƒŸãƒŠãƒ«å‡ºåŠ›ï¼ˆçµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼‰"""
        print("\n" + "="*50)
        print("ãƒ‡ãƒ¼ã‚¿å†æ§‹æˆè©•ä¾¡çµæœ (Reconstruction Metrics)")
        print("="*50)

        for metric, value in results.items():
            if metric == 'reconstruction_rmse':
                print(f"ğŸ”¸ Reconstruction RMSE: {value:.6f}")
            elif metric == 'psnr':
                if value == float('inf'):
                    print(f"ğŸ”¸ PSNR: âˆ dB (Perfect Match)")
                else:
                    print(f"ğŸ”¸ PSNR: {value:.2f} dB")
            elif metric == 'temporal_correlation':
                print(f"ğŸ”¸ Temporal Correlation: {value:.6f}")
            else:
                print(f"ğŸ”¸ {metric}: {value:.6f}")

        print("="*50)

    def save_reconstruction_metrics_results(
        self,
        results: Dict[str, float],
        output_dir: str,
        experiment_info: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        å†æ§‹æˆè©•ä¾¡çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆTargetPredictionMetricsãƒ‘ã‚¿ãƒ¼ãƒ³ç¶™æ‰¿ï¼‰

        Args:
            results: compute_reconstruction_metrics()ã®çµæœ
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            experiment_info: è¿½åŠ ã®å®Ÿé¨“æƒ…å ±ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        import json
        from datetime import datetime
        from pathlib import Path

        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜ãƒ‡ãƒ¼ã‚¿æ§‹é€ ï¼ˆTargetPredictionãƒ‘ã‚¿ãƒ¼ãƒ³ç¶™æ‰¿ï¼‰
        save_data = {
            'timestamp': datetime.now().isoformat(),
            'evaluation_type': 'reconstruction',
            'metrics': results,
        }

        # å®Ÿé¨“æƒ…å ±è¿½åŠ 
        if experiment_info:
            save_data['experiment_info'] = experiment_info

        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        save_file = output_path / 'reconstruction_metrics.json'
        with open(save_file, 'w') as f:
            json.dump(save_data, f, indent=2)

        print(f"ğŸ“ å†æ§‹æˆè©•ä¾¡çµæœä¿å­˜: {save_file}")
        return str(save_file)


def create_target_prediction_evaluator(device: str = 'cpu') -> TargetPredictionMetrics:
    """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆäºˆæ¸¬è©•ä¾¡å™¨ã®ä½œæˆï¼ˆæ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨çµ±ä¸€ï¼‰"""
    return TargetPredictionMetrics(device=device)


def create_reconstruction_evaluator(device: str = 'cpu') -> ReconstructionMetrics:
    """å†æ§‹æˆè©•ä¾¡å™¨ã®ä½œæˆï¼ˆçµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼‰"""
    return ReconstructionMetrics(device=device)