#!/usr/bin/env python3
# scripts/run_full_experiment.py
"""
ã‚¿ã‚¹ã‚¯3: å®Ÿãƒ‡ãƒ¼ã‚¿å­¦ç¿’ãƒ»è©•ä¾¡ç’°å¢ƒæ§‹ç¯‰
å®Œå…¨å®Ÿé¨“ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æ©Ÿèƒ½:
- çµ±ä¸€ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã«ã‚ˆã‚‹å‰å‡¦ç†
- Phase-1 + Phase-2å­¦ç¿’ï¼ˆKalmanå«ã‚€ï¼‰
- å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ»è»¢é€ä½œç”¨ç´ ã®ä¿å­˜
- å­¦ç¿’éç¨‹ã®å¯è¦–åŒ–ãƒ»ãƒ­ã‚°è¨˜éŒ²
- å®Ÿé¨“å†ç¾æ€§æ‹…ä¿

å®Ÿè¡Œä¾‹:
python scripts/run_full_experiment.py \
    --config configs/full_experiment_config.yaml \
    --data data/sim_complex.npz \
    --output results/full_experiment_001 \
    --use-kalman
"""

import argparse
import sys
import yaml
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional

import numpy as np
import torch
import matplotlib.pyplot as plt

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹è¨­å®š
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(PROJECT_ROOT / 'src'))

# çµ±ä¸€ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
from src.utils.data_loader import load_experimental_data, DataMetadata

# æ—¢å­˜ã®å­¦ç¿’ã‚¯ãƒ©ã‚¹
from src.training.two_stage_trainer import TwoStageTrainer
from src.utils.gpu_utils import select_device


class FullExperimentPipeline:
    """
    å®Œå…¨å®Ÿé¨“ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã‚¯ãƒ©ã‚¹
    
    ã‚¿ã‚¹ã‚¯3ã®è¦ä»¶ã‚’æº€ãŸã™å®Œå…¨ãªå­¦ç¿’ãƒ»è©•ä¾¡ç’°å¢ƒ
    """
    
    def __init__(self, config: Dict[str, Any], output_dir: Path, device: torch.device):
        self.config = config
        self.output_dir = output_dir
        self.device = device
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™
        self.output_dir.mkdir(parents=True, exist_ok=True)
        (self.output_dir / 'models').mkdir(exist_ok=True)
        (self.output_dir / 'plots').mkdir(exist_ok=True)
        (self.output_dir / 'logs').mkdir(exist_ok=True)
        (self.output_dir / 'artifacts').mkdir(exist_ok=True)
        
        # ãƒ­ã‚°è¨­å®š
        self.experiment_log = []
        self.start_time = datetime.now()
        
        self._log_experiment_start()
    
    def _log_experiment_start(self):
        """å®Ÿé¨“é–‹å§‹ãƒ­ã‚°"""
        log_entry = {
            'timestamp': self.start_time.isoformat(),
            'event': 'experiment_start',
            'config': self.config,
            'device': str(self.device),
            'output_dir': str(self.output_dir)
        }
        self.experiment_log.append(log_entry)
        print(f"ğŸš€ å®Ÿé¨“é–‹å§‹: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
        print(f"ğŸ–¥ï¸  è¨ˆç®—ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
    
    def step_1_data_loading(self, data_path: str) -> Dict[str, torch.Tensor]:
        """
        Step 3.1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†
        
        Args:
            data_path: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            
        Returns:
            å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿è¾æ›¸
        """
        print("\n" + "="*50)
        print("Step 3.1: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†")
        print("="*50)
        
        start_time = datetime.now()
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿è¨­å®š
        data_config = self.config.get('data', {})
        
        # çµ±ä¸€ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã«ã‚ˆã‚‹èª­ã¿è¾¼ã¿
        print(f"ğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­: {data_path}")
        data_dict = load_experimental_data(data_path, data_config)
        
        # ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆè¡¨ç¤º
        metadata: DataMetadata = data_dict['metadata']
        print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
        print(f"  - å…ƒãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {metadata.original_shape}")
        print(f"  - ç‰¹å¾´é‡æ•°: {len(metadata.feature_names)}")
        print(f"  - æ¬ æå€¤ç‡: {metadata.missing_ratio:.2%}")
        print(f"  - æ­£è¦åŒ–æ–¹æ³•: {metadata.normalization_method}")
        print(f"  - è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {data_dict['train'].shape}")
        print(f"  - æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {data_dict['val'].shape}")
        print(f"  - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {data_dict['test'].shape}")
        
        # ãƒ‡ãƒ¼ã‚¿ã®æ¬¡å…ƒæ•°ã‚’å–å¾—ã—ã¦è¨­å®šã‚’å‹•çš„ã«æ›´æ–°
        data_dim = data_dict['train'].shape[1]  # (T, d) ã® d ã‚’å–å¾—
        if 'model' in self.config:
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®å…¥åŠ›æ¬¡å…ƒã‚’ãƒ‡ãƒ¼ã‚¿ã«åˆã‚ã›ã¦æ›´æ–°
            if 'encoder' in self.config['model']:
                original_input_dim = self.config['model']['encoder'].get('input_dim', data_dim)
                self.config['model']['encoder']['input_dim'] = data_dim
                if original_input_dim != data_dim:
                    print(f"ğŸ”§ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼å…¥åŠ›æ¬¡å…ƒã‚’è‡ªå‹•èª¿æ•´: {original_input_dim} â†’ {data_dim}")
            
            # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®å‡ºåŠ›æ¬¡å…ƒã‚’ãƒ‡ãƒ¼ã‚¿ã«åˆã‚ã›ã¦æ›´æ–°
            if 'decoder' in self.config['model']:
                original_output_dim = self.config['model']['decoder'].get('output_dim', data_dim)
                self.config['model']['decoder']['output_dim'] = data_dim
                if original_output_dim != data_dim:
                    print(f"ğŸ”§ ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼å‡ºåŠ›æ¬¡å…ƒã‚’è‡ªå‹•èª¿æ•´: {original_output_dim} â†’ {data_dim}")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
        for key in ['train', 'val', 'test']:
            data_dict[key] = data_dict[key].to(self.device)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        with open(self.output_dir / 'logs' / 'data_metadata.json', 'w') as f:
            # DataMetadataã¯__dict__ãŒãªã„ãŸã‚ã€asdict()ã‚’ä½¿ç”¨
            metadata_dict = {
                'original_shape': metadata.original_shape,
                'feature_names': metadata.feature_names,
                'time_index': metadata.time_index,
                'sampling_rate': metadata.sampling_rate,
                'missing_ratio': metadata.missing_ratio,
                'data_source': metadata.data_source,
                'normalization_method': metadata.normalization_method,
                'train_indices': metadata.train_indices,
                'val_indices': metadata.val_indices,
                'test_indices': metadata.test_indices
            }
            json.dump(metadata_dict, f, indent=2)
        
        # ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–
        self._plot_data_overview(data_dict)
        
        elapsed = (datetime.now() - start_time).total_seconds()
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†å®Œäº† ({elapsed:.1f}ç§’)")
        
        # ãƒ­ã‚°è¨˜éŒ²
        self.experiment_log.append({
            'timestamp': datetime.now().isoformat(),
            'event': 'data_loading_complete',
            'elapsed_seconds': elapsed,
            'data_shapes': {k: list(v.shape) for k, v in data_dict.items() if isinstance(v, torch.Tensor)},
            'metadata': metadata_dict
        })
        
        return data_dict
    
    def step_2_training_execution(self, data_dict: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """
        Step 3.2: å®Œå…¨å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        
        Args:
            data_dict: å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            å­¦ç¿’çµæœè¾æ›¸
        """
        print("\n" + "="*50)
        print("Step 3.2: Phase-1 + Phase-2å­¦ç¿’å®Ÿè¡Œ")
        print("="*50)
        
        start_time = datetime.now()
        
        # å­¦ç¿’å™¨åˆæœŸåŒ–
        use_kalman = self.config.get('training', {}).get('use_kalman_filtering', False)
        print(f"ğŸ”§ Kalmanãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: {'æœ‰åŠ¹' if use_kalman else 'ç„¡åŠ¹'}")
        
        # è¨­å®šè¾æ›¸ã‚’ä½¿ç”¨ã—ã¦TwoStageTrainerã‚’ç›´æ¥åˆæœŸåŒ–
        trainer = TwoStageTrainer(
            config=self.config,
            device=self.device,
            output_dir=str(self.output_dir),
            use_kalman_filtering=use_kalman
        )
        
        # Phase-1å­¦ç¿’
        print("ğŸƒâ€â™‚ï¸ Phase-1å­¦ç¿’é–‹å§‹...")
        phase1_start = datetime.now()
        
        phase1_results = trainer.train_phase1(data_dict['train'])
        
        phase1_elapsed = (datetime.now() - phase1_start).total_seconds()
        print(f"âœ… Phase-1å®Œäº† ({phase1_elapsed:.1f}ç§’)")
        
        # Phase-2å­¦ç¿’ï¼ˆEnd-to-endå¾®èª¿æ•´ï¼‰
        print("ğŸƒâ€â™‚ï¸ Phase-2å­¦ç¿’é–‹å§‹...")
        phase2_start = datetime.now()
        
        phase2_results = trainer.train_phase2(data_dict['train'], data_dict['val'])
        
        phase2_elapsed = (datetime.now() - phase2_start).total_seconds()
        print(f"âœ… Phase-2å®Œäº† ({phase2_elapsed:.1f}ç§’)")
        
        total_elapsed = (datetime.now() - start_time).total_seconds()
        print(f"âœ… å…¨å­¦ç¿’å®Œäº† ({total_elapsed:.1f}ç§’)")
        
        # å­¦ç¿’çµæœçµ±åˆ
        training_results = {
            'phase1': phase1_results,
            'phase2': phase2_results,
            'total_time': total_elapsed,
            'phase1_time': phase1_elapsed,
            'phase2_time': phase2_elapsed,
            'use_kalman': use_kalman
        }
        
        # å­¦ç¿’éç¨‹å¯è¦–åŒ–
        self._plot_training_progress(training_results)
        
        # å­¦ç¿’çµæœä¿å­˜
        results_path = self.output_dir / 'logs' / 'training_results.json'
        with open(results_path, 'w') as f:
            # Tensorç­‰ã¯JSONéå¯¾å¿œã®ãŸã‚ã€ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ãªå½¢å¼ã«å¤‰æ›
            serializable_results = self._make_json_serializable(training_results)
            json.dump(serializable_results, f, indent=2)
        
        # ãƒ­ã‚°è¨˜éŒ²
        self.experiment_log.append({
            'timestamp': datetime.now().isoformat(),
            'event': 'training_complete',
            'total_time': total_elapsed,
            'phase1_time': phase1_elapsed,
            'phase2_time': phase2_elapsed,
            'use_kalman': use_kalman
        })
        
        return {
            'trainer': trainer,
            'results': training_results
        }
    
    def step_3_model_analysis(self, trainer: TwoStageTrainer, data_dict: Dict[str, torch.Tensor]):
        """
        Step 3.3: å­¦ç¿’æ¸ˆã¿è»¢é€ä½œç”¨ç´ ã®è¡¨ç¾ç¢ºèª
        
        Args:
            trainer: å­¦ç¿’æ¸ˆã¿å­¦ç¿’å™¨
            data_dict: ãƒ‡ãƒ¼ã‚¿è¾æ›¸
        """
        print("\n" + "="*50)
        print("Step 3.3: è»¢é€ä½œç”¨ç´ ãƒ»è¡¨ç¾åˆ†æ")
        print("="*50)
        
        start_time = datetime.now()
        
        # è»¢é€ä½œç”¨ç´ å–å¾—ãƒ»ä¿å­˜
        operators_info = self._analyze_transfer_operators(trainer)
        
        # å†…éƒ¨è¡¨ç¾åˆ†æ
        representations_info = self._analyze_internal_representations(trainer, data_dict)
        
        # çŠ¶æ…‹ç©ºé–“å¯è¦–åŒ–
        self._visualize_state_space(trainer, data_dict['test'])
        
        elapsed = (datetime.now() - start_time).total_seconds()
        print(f"âœ… è¡¨ç¾åˆ†æå®Œäº† ({elapsed:.1f}ç§’)")
        
        # åˆ†æçµæœä¿å­˜
        analysis_results = {
            'operators': operators_info,
            'representations': representations_info,
            'analysis_time': elapsed
        }
        
        with open(self.output_dir / 'logs' / 'model_analysis.json', 'w') as f:
            serializable_analysis = self._make_json_serializable(analysis_results)
            json.dump(serializable_analysis, f, indent=2)
        
        # ãƒ­ã‚°è¨˜éŒ²
        self.experiment_log.append({
            'timestamp': datetime.now().isoformat(),
            'event': 'model_analysis_complete',
            'elapsed_seconds': elapsed
        })
    
    def finalize_experiment(self, trainer: TwoStageTrainer):
        """å®Ÿé¨“çµ‚äº†å‡¦ç†"""
        print("\n" + "="*50)
        print("å®Ÿé¨“çµ‚äº†å‡¦ç†")
        print("="*50)
        
        # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        model_path = self.output_dir / 'models' / 'final_model.pth'
        trainer._save_inference_ready_model(str(model_path))
        print(f"ğŸ’¾ æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {model_path}")
        
        # å®Ÿé¨“è¨­å®šä¿å­˜
        config_path = self.output_dir / 'logs' / 'experiment_config.yaml'
        with open(config_path, 'w') as f:
            yaml.dump(self.config, f, default_flow_style=False)
        
        # å®Œå…¨å®Ÿé¨“ãƒ­ã‚°ä¿å­˜
        end_time = datetime.now()
        total_time = (end_time - self.start_time).total_seconds()
        
        self.experiment_log.append({
            'timestamp': end_time.isoformat(),
            'event': 'experiment_complete',
            'total_experiment_time': total_time
        })
        
        with open(self.output_dir / 'logs' / 'full_experiment_log.json', 'w') as f:
            json.dump(self.experiment_log, f, indent=2)
        
        print(f"â±ï¸  ç·å®Ÿé¨“æ™‚é–“: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†)")
        print(f"ğŸ“Š å®Ÿé¨“å®Œäº†: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ“ å…¨çµæœä¿å­˜å…ˆ: {self.output_dir}")
    
    def _plot_data_overview(self, data_dict: Dict[str, torch.Tensor]):
        """ãƒ‡ãƒ¼ã‚¿æ¦‚è¦ãƒ—ãƒ­ãƒƒãƒˆ"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        fig.suptitle('Data Overview', fontsize=14)
        
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ™‚ç³»åˆ—ãƒ—ãƒ­ãƒƒãƒˆï¼ˆæœ€åˆã®3æ¬¡å…ƒï¼‰
        train_data = data_dict['train'].cpu().numpy()
        for i in range(min(3, train_data.shape[1])):
            axes[0, 0].plot(train_data[:, i], label=f'Feature {i+1}')
        axes[0, 0].set_title('Training Data Time Series')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²æ¯”ç‡
        sizes = [data_dict['train'].shape[0], data_dict['val'].shape[0], data_dict['test'].shape[0]]
        axes[0, 1].pie(sizes, labels=['Train', 'Val', 'Test'], autopct='%1.1f%%')
        axes[0, 1].set_title('Data Split Ratio')
        
        # ç‰¹å¾´é‡åˆ†å¸ƒï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼‰
        axes[1, 0].hist(train_data.flatten(), bins=50, alpha=0.7)
        axes[1, 0].set_title('Feature Value Distribution')
        axes[1, 0].set_xlabel('Value')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].grid(True)
        
        # ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ
        stats_text = f"""
        Data Shape: {train_data.shape}
        Mean: {train_data.mean():.3f}
        Std: {train_data.std():.3f}
        Min: {train_data.min():.3f}
        Max: {train_data.max():.3f}
        """
        axes[1, 1].text(0.1, 0.5, stats_text, transform=axes[1, 1].transAxes,
                        verticalalignment='center', fontsize=10)
        axes[1, 1].set_title('Data Statistics')
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'plots' / 'data_overview.png', dpi=300)
        plt.close()
    
    def _plot_training_progress(self, results: Dict[str, Any]):
        """å­¦ç¿’éç¨‹å¯è¦–åŒ–"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        fig.suptitle('Training Progress', fontsize=14)
        
        # Phase-1æå¤±æ¨ç§»ï¼ˆä¾‹ï¼šå®Ÿéš›ã®resultsã‹ã‚‰å–å¾—ï¼‰
        if 'phase1' in results and 'losses' in results['phase1']:
            phase1_losses = results['phase1']['losses']
            axes[0, 0].plot(phase1_losses)
            axes[0, 0].set_title('Phase-1 Loss')
            axes[0, 0].set_xlabel('Epoch')
            axes[0, 0].set_ylabel('Loss')
            axes[0, 0].grid(True)
        
        # Phase-2æå¤±æ¨ç§»
        if 'phase2' in results and 'losses' in results['phase2']:
            phase2_losses = results['phase2']['losses']
            axes[0, 1].plot(phase2_losses)
            axes[0, 1].set_title('Phase-2 Loss')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('Loss')
            axes[0, 1].grid(True)
        
        # å­¦ç¿’æ™‚é–“æ¯”è¼ƒ
        times = [results.get('phase1_time', 0), results.get('phase2_time', 0)]
        axes[1, 0].bar(['Phase-1', 'Phase-2'], times)
        axes[1, 0].set_title('Training Time Comparison')
        axes[1, 0].set_ylabel('Time (seconds)')
        
        # å­¦ç¿’è¨­å®šæƒ…å ±
        info_text = f"""
        Phase-1 Time: {results.get('phase1_time', 0):.1f}s
        Phase-2 Time: {results.get('phase2_time', 0):.1f}s
        Total Time: {results.get('total_time', 0):.1f}s
        Kalman Used: {results.get('use_kalman', False)}
        """
        axes[1, 1].text(0.1, 0.5, info_text, transform=axes[1, 1].transAxes,
                        verticalalignment='center', fontsize=10)
        axes[1, 1].set_title('Training Info')
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'plots' / 'training_progress.png', dpi=300)
        plt.close()
    
    def _analyze_transfer_operators(self, trainer: TwoStageTrainer) -> Dict[str, Any]:
        """è»¢é€ä½œç”¨ç´ åˆ†æ"""
        operators_info = {}
        
        # DF-Aè»¢é€ä½œç”¨ç´ ï¼ˆV_A, U_Aï¼‰
        if hasattr(trainer, 'df_state') and trainer.df_state is not None:
            try:
                state_dict = trainer.df_state.get_state_dict()
                if 'V_A' in state_dict:
                    V_A = state_dict['V_A']
                    operators_info['V_A_shape'] = list(V_A.shape)
                    operators_info['V_A_norm'] = float(torch.norm(V_A).item())
                if 'U_A' in state_dict:
                    U_A = state_dict['U_A']
                    operators_info['U_A_shape'] = list(U_A.shape)
                    operators_info['U_A_norm'] = float(torch.norm(U_A).item())
            except Exception as e:
                print(f"âš ï¸  DF-Aè»¢é€ä½œç”¨ç´ åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        
        # DF-Bè»¢é€ä½œç”¨ç´ ï¼ˆV_B, u_Bï¼‰
        if hasattr(trainer, 'df_obs') and trainer.df_obs is not None:
            try:
                obs_dict = trainer.df_obs.get_state_dict()
                if 'V_B' in obs_dict:
                    V_B = obs_dict['V_B']
                    operators_info['V_B_shape'] = list(V_B.shape)
                    operators_info['V_B_norm'] = float(torch.norm(V_B).item())
                if 'u_B' in obs_dict:
                    u_B = obs_dict['u_B']
                    operators_info['u_B_shape'] = list(u_B.shape)
                    operators_info['u_B_norm'] = float(torch.norm(u_B).item())
            except Exception as e:
                print(f"âš ï¸  DF-Bè»¢é€ä½œç”¨ç´ åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        
        # è»¢é€ä½œç”¨ç´ ä¿å­˜
        operators_path = self.output_dir / 'artifacts' / 'transfer_operators.pth'
        try:
            operators_data = {
                'df_state': trainer.df_state.get_state_dict() if hasattr(trainer, 'df_state') and trainer.df_state else None,
                'df_obs': trainer.df_obs.get_state_dict() if hasattr(trainer, 'df_obs') and trainer.df_obs else None
            }
            torch.save(operators_data, operators_path)
            print(f"ğŸ’¾ è»¢é€ä½œç”¨ç´ ä¿å­˜: {operators_path}")
        except Exception as e:
            print(f"âš ï¸  è»¢é€ä½œç”¨ç´ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        
        return operators_info
    
    def _analyze_internal_representations(self, trainer: TwoStageTrainer, data_dict: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """å†…éƒ¨è¡¨ç¾åˆ†æ"""
        representations_info = {}
        
        try:
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€è¡¨ç¾åˆ†æ
            if hasattr(trainer, 'encoder'):
                test_sample = data_dict['test'][:100]  # æœ€åˆã®100ã‚µãƒ³ãƒ—ãƒ«
                with torch.no_grad():
                    encoded = trainer.encoder(test_sample.unsqueeze(0)).squeeze(0)
                    representations_info['encoder_output_shape'] = list(encoded.shape)
                    representations_info['encoder_output_mean'] = float(encoded.mean().item())
                    representations_info['encoder_output_std'] = float(encoded.std().item())
        
        except Exception as e:
            print(f"âš ï¸  å†…éƒ¨è¡¨ç¾åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        
        return representations_info
    
    def _visualize_state_space(self, trainer: TwoStageTrainer, test_data: torch.Tensor):
        """çŠ¶æ…‹ç©ºé–“å¯è¦–åŒ–"""
        try:
            # çŠ¶æ…‹æ¨å®šï¼ˆç°¡ç•¥ç‰ˆï¼‰
            with torch.no_grad():
                # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
                if hasattr(trainer, 'encoder'):
                    encoded = trainer.encoder(test_data[:200].unsqueeze(0)).squeeze(0)
                    
                    # 2D ãƒ—ãƒ­ãƒƒãƒˆï¼ˆæœ€åˆã®2æ¬¡å…ƒã¾ãŸã¯ä¸»æˆåˆ†ï¼‰
                    if encoded.shape[1] >= 2:
                        plt.figure(figsize=(10, 6))
                        
                        plt.subplot(1, 2, 1)
                        plt.plot(encoded[:, 0].cpu().numpy(), label='State Dim 1')
                        plt.plot(encoded[:, 1].cpu().numpy(), label='State Dim 2')
                        plt.title('State Trajectory (Time Series)')
                        plt.xlabel('Time')
                        plt.ylabel('State Value')
                        plt.legend()
                        plt.grid(True)
                        
                        plt.subplot(1, 2, 2)
                        plt.scatter(encoded[:, 0].cpu().numpy(), encoded[:, 1].cpu().numpy(), 
                                  c=np.arange(len(encoded)), cmap='viridis', alpha=0.6)
                        plt.colorbar(label='Time')
                        plt.title('State Space Plot')
                        plt.xlabel('State Dim 1')
                        plt.ylabel('State Dim 2')
                        plt.grid(True)
                        
                        plt.tight_layout()
                        plt.savefig(self.output_dir / 'plots' / 'state_space_visualization.png', dpi=300)
                        plt.close()
                        
                        print("ğŸ“Š çŠ¶æ…‹ç©ºé–“å¯è¦–åŒ–å®Œäº†")
            
        except Exception as e:
            print(f"âš ï¸  çŠ¶æ…‹ç©ºé–“å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _make_json_serializable(self, obj):
        """JSONå¯¾å¿œå½¢å¼ã«å¤‰æ›"""
        if isinstance(obj, dict):
            return {k: self._make_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_json_serializable(v) for v in obj]
        elif isinstance(obj, torch.Tensor):
            return obj.cpu().numpy().tolist()
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.integer):
            return int(obj)
        else:
            return obj


def parse_args():
    """å¼•æ•°è§£æ"""
    parser = argparse.ArgumentParser(description="ã‚¿ã‚¹ã‚¯3: å®Œå…¨å®Ÿé¨“ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ")
    
    parser.add_argument(
        '--config', '-c', type=str, required=True,
        help='å®Ÿé¨“è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« (.yaml)'
    )
    parser.add_argument(
        '--data', '-d', type=str, required=True,
        help='ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹'
    )
    parser.add_argument(
        '--output', '-o', type=str, required=True,
        help='çµæœå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª'
    )
    parser.add_argument(
        '--device', type=str, default=None,
        help='è¨ˆç®—ãƒ‡ãƒã‚¤ã‚¹ (autoé¸æŠæ™‚ã¯None)'
    )
    parser.add_argument(
        '--use-kalman', action='store_true',
        help='Kalmanãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æœ‰åŠ¹åŒ–'
    )
    parser.add_argument(
        '--skip-analysis', action='store_true',
        help='Step 3.3 è¡¨ç¾åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—'
    )
    
    return parser.parse_args()


def load_experiment_config(config_path: str) -> Dict[str, Any]:
    """å®Ÿé¨“è¨­å®šèª­ã¿è¾¼ã¿"""
    if not Path(config_path).exists():
        raise FileNotFoundError(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}")
    
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # å¿…é ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯
    required_sections = ['model', 'training']
    for section in required_sections:
        if section not in config:
            raise ValueError(f"è¨­å®šã« {section} ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒå¿…è¦ã§ã™")
    
    return config


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    args = parse_args()
    
    print("ğŸš€ ã‚¿ã‚¹ã‚¯3: å®Œå…¨å®Ÿé¨“ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")
    print("="*60)
    
    # è¨­å®šèª­ã¿è¾¼ã¿
    config = load_experiment_config(args.config)
    
    # Kalmanãƒ•ãƒ©ã‚°è¨­å®š
    if args.use_kalman:
        config.setdefault('training', {})['use_kalman_filtering'] = True
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = torch.device(args.device) if args.device else select_device()
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    output_dir = Path(args.output)
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    pipeline = FullExperimentPipeline(config, output_dir, device)
    
    try:
        # Step 3.1: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        data_dict = pipeline.step_1_data_loading(args.data)
        
        # Step 3.2: å®Œå…¨å­¦ç¿’å®Ÿè¡Œ
        training_result = pipeline.step_2_training_execution(data_dict)
        
        # Step 3.3: è¡¨ç¾åˆ†æï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if not args.skip_analysis:
            pipeline.step_3_model_analysis(training_result['trainer'], data_dict)
        
        # å®Ÿé¨“å®Œäº†å‡¦ç†
        pipeline.finalize_experiment(training_result['trainer'])
        
        print("\nğŸ‰ å®Œå…¨å®Ÿé¨“ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ­£å¸¸çµ‚äº†ï¼")
        print(f"ğŸ“ çµæœ: {output_dir}")
        
    except Exception as e:
        print(f"\nâŒ å®Ÿé¨“ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())