#!/usr/bin/env python3
# scripts/run_full_experiment.py
"""
タスク3: 実データ学習・評価環境構築
完全実験パイプライン実行スクリプト

機能:
- 統一データローダーによる前処理
- Phase-1 + Phase-2学習（Kalman含む）
- 学習済みモデル・転送作用素の保存
- 学習過程の可視化・ログ記録
- 実験再現性担保

実行例:
python scripts/run_full_experiment.py \
    --config configs/full_experiment_config.yaml \
    --data data/sim_complex.npz \
    --output results/full_experiment_001 \
    --use-kalman
"""

import argparse
import sys
import yaml
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional

import numpy as np
import torch
import matplotlib.pyplot as plt

# プロジェクトルートパス設定
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(PROJECT_ROOT / 'src'))

# 統一データローダー
from src.utils.data_loader import load_experimental_data_with_architecture, DataMetadata

# 既存の学習クラス
from src.training.two_stage_trainer import TwoStageTrainer
from src.utils.gpu_utils import select_device

# 新しい確率実現クラスとモード分解機能
from src.ssm.realization import StochasticRealizationWithEncoder
from src.evaluation.mode_decomposition import TrainedModelSpectrumAnalysis, SpectrumResultsSaver


class FullExperimentPipeline:
    """
    完全実験パイプライン実行クラス
    
    タスク3の要件を満たす完全な学習・評価環境
    """
    
    def __init__(self, config: Dict[str, Any], output_dir: Path, device: torch.device):
        self.config = config
        self.output_dir = output_dir
        self.device = device
        
        # 出力ディレクトリ準備
        self.output_dir.mkdir(parents=True, exist_ok=True)
        (self.output_dir / 'models').mkdir(exist_ok=True)
        (self.output_dir / 'plots').mkdir(exist_ok=True)
        (self.output_dir / 'logs').mkdir(exist_ok=True)
        (self.output_dir / 'artifacts').mkdir(exist_ok=True)
        
        # ログ設定
        self.experiment_log = []
        self.start_time = datetime.now()
        
        self._log_experiment_start()
    
    def _log_experiment_start(self):
        """実験開始ログ"""
        log_entry = {
            'timestamp': self.start_time.isoformat(),
            'event': 'experiment_start',
            'config': self.config,
            'device': str(self.device),
            'output_dir': str(self.output_dir)
        }
        self.experiment_log.append(log_entry)
        print(f"実験開始: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"出力: {self.output_dir}")
        print(f"デバイス: {self.device}")
    
    def step_1_data_loading(self, data_path: str) -> Dict[str, torch.Tensor]:
        """
        Step 3.1: データ読み込み・前処理

        Args:
            data_path: データファイルパス

        Returns:
            処理済みデータ辞書
        """
        print("\n" + "="*5)
        print("Step 3.1: データ読み込み")
        print("="*5)

        start_time = datetime.now()

        # Step 5: experiment_mode自動判定
        experiment_mode = self.config.get('experiment', {}).get('mode', 'reconstruction')
        print(f"実験モード: {experiment_mode}")

        # データ読み込み設定
        data_config = self.config.get('data', {})

        # 統一データローダーによる読み込み（アーキテクチャ対応）
        print(f"データ読み込み: {data_path}")
        datasets = load_experimental_data_with_architecture(
            data_path=data_path,
            config=self.config,  # 全体設定を渡してアーキテクチャ判定
            split="all",
            return_dataloaders=False
        )

        # 後方互換性のためTensor辞書形式に変換
        data_dict = {split: dataset.get_full_data() for split, dataset in datasets.items()}
        data_dict['metadata'] = datasets['train'].metadata

        # ターゲットデータをdata_dictに追加（包括的対応）
        for split, dataset in datasets.items():
            if hasattr(dataset, 'target_data') and dataset.target_data is not None:
                # データセットが検出したターゲットデータを保存
                split_size = data_dict[split].shape[0]
                if split == 'train':
                    target_data = dataset.target_data
                elif split == 'test' and hasattr(dataset, 'target_test_data') and dataset.target_test_data is not None:
                    # target_test_dataのサイズ確認（分割サイズと一致するかチェック）
                    if dataset.target_test_data.shape[0] == split_size:
                        target_data = dataset.target_test_data
                        print(f"{split}分割: データローダーのtarget_test_dataを使用（正しいサイズ）")
                    else:
                        # print(f"{split}分割: target_test_dataサイズ({dataset.target_test_data.shape[0]}) != 期待サイズ({split_size}), 分割ロジック適用")
                        # サイズが一致しない場合は分割ロジックを使用
                        if hasattr(dataset, 'target_data') and dataset.target_data is not None:
                            train_size = datasets['train'].data.shape[0]
                            val_size = datasets['val'].data.shape[0] if 'val' in datasets else 0
                            target_data = dataset.target_data[train_size + val_size:train_size + val_size + split_size]
                        else:
                            continue
                else:
                    # 分割に対応するターゲットデータがない場合、訓練データから分割
                    if hasattr(dataset, 'target_data') and dataset.target_data is not None:
                        # データセットの分割インデックスに基づいてターゲットデータを分割
                        train_size = datasets['train'].data.shape[0]
                        val_size = datasets['val'].data.shape[0] if 'val' in datasets else 0

                        if split == 'val':
                            target_data = dataset.target_data[train_size:train_size + val_size]
                        elif split == 'test':
                            # testデータは train_size + val_size 以降のデータを使用
                            target_data = dataset.target_data[train_size + val_size:train_size + val_size + split_size]
                        else:
                            target_data = dataset.target_data[:split_size]
                    else:
                        continue

                # ターゲットデータをTensorに変換してdata_dictに追加
                if isinstance(target_data, np.ndarray):
                    target_data = torch.from_numpy(target_data).float()
                data_dict[f'{split}_targets'] = target_data
                print(f"{split}分割ターゲットデータ追加: shape={target_data.shape}")
            else:
                print(f"{split}分割: ターゲットデータなし")

        # Step 5: ターゲットデータ検証
        metadata: DataMetadata = data_dict['metadata']
        if experiment_mode == "target_prediction":
            if not hasattr(metadata, 'has_target_data') or not metadata.has_target_data:
                raise ValueError("Target prediction mode requires target data")
            print(f"ターゲットデータ検出: {getattr(metadata, 'target_shape', 'Unknown shape')}")

        # データ統計表示
        print(f"データ統計:")
        print(f"  - 元データ形状: {metadata.original_shape}")
        print(f"  - 特徴量数: {len(metadata.feature_names)}")
        print(f"  - 欠損値率: {metadata.missing_ratio:.2%}")
        print(f"  - 正規化方法: {metadata.normalization_method}")
        print(f"  - 訓練データ: {data_dict['train'].shape}")
        print(f"  - 検証データ: {data_dict['val'].shape}")
        print(f"  - テストデータ: {data_dict['test'].shape}")
        if experiment_mode == "target_prediction" and hasattr(metadata, 'has_target_data') and metadata.has_target_data:
            print(f"  - ターゲットデータ利用可能: {metadata.has_target_data}")
        
        # データの次元数を取得して設定を動的に更新
        data_dim = data_dict['train'].shape[1]  # (T, d) の d を取得
        if 'model' in self.config:
            # エンコーダーの入力次元をデータに合わせて更新
            if 'encoder' in self.config['model']:
                original_input_dim = self.config['model']['encoder'].get('input_dim', data_dim)
                self.config['model']['encoder']['input_dim'] = data_dim
                if original_input_dim != data_dim:
                    print(f"エンコーダー入力次元を自動調整: {original_input_dim} → {data_dim}")
            
            # デコーダーの出力次元をデータに合わせて更新
            if 'decoder' in self.config['model']:
                original_output_dim = self.config['model']['decoder'].get('output_dim', data_dim)
                self.config['model']['decoder']['output_dim'] = data_dim
                if original_output_dim != data_dim:
                    print(f"デコーダー出力次元を自動調整: {original_output_dim} → {data_dim}")
        
        # データをデバイスに移動
        for key in ['train', 'val', 'test']:
            data_dict[key] = data_dict[key].to(self.device)

        # Step 5: experiment_modeをdata_dictに保存
        data_dict['experiment_mode'] = experiment_mode

        # メタデータ保存
        with open(self.output_dir / 'logs' / 'data_metadata.json', 'w') as f:
            # DataMetadataは__dict__がないため、asdict()を使用
            metadata_dict = {
                'original_shape': metadata.original_shape,
                'feature_names': metadata.feature_names,
                'time_index': metadata.time_index,
                'sampling_rate': metadata.sampling_rate,
                'missing_ratio': metadata.missing_ratio,
                'data_source': metadata.data_source,
                'normalization_method': metadata.normalization_method,
                'train_indices': metadata.train_indices,
                'val_indices': metadata.val_indices,
                'test_indices': metadata.test_indices
            }
            json.dump(metadata_dict, f, indent=2)
        
        # データ可視化（オプション）
        # self._plot_data_overview(data_dict)
        
        elapsed = (datetime.now() - start_time).total_seconds()
        print(f"データ前処理完了 ({elapsed:.1f}秒)")
        
        # ログ記録
        self.experiment_log.append({
            'timestamp': datetime.now().isoformat(),
            'event': 'data_loading_complete',
            'elapsed_seconds': elapsed,
            'data_shapes': {k: list(v.shape) for k, v in data_dict.items() if isinstance(v, torch.Tensor)},
            'metadata': metadata_dict
        })
        
        return data_dict
    
    def step_2_training_execution(self, data_dict: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """
        Step 3.2: 完全学習パイプライン実行

        Args:
            data_dict: 前処理済みデータ

        Returns:
            学習結果辞書
        """
        print("\n" + "="*5)
        print("Step 3.2: 学習実行")
        print("="*5)

        start_time = datetime.now()

        # Step 5: experiment_mode取得
        experiment_mode = data_dict.get('experiment_mode', 'reconstruction')
        print(f"学習モード: {experiment_mode}")

        # Step 5: デコーダ選択（experiment_mode対応）
        if experiment_mode == "target_prediction":
            if 'target_decoder' in self.config.get('model', {}):
                print("ターゲット予測デコーダを使用")
            else:
                print("target_decoderが設定されていません。通常のdecoderを使用します。")

        # 学習器初期化
        use_kalman = self.config.get('training', {}).get('use_kalman_filtering', False)
        print(f"Kalmanフィルタリング: {'有効' if use_kalman else '無効'}")

        # Step 5: TrainingConfigにexperiment_mode設定
        if 'training' in self.config:
            self.config['training']['experiment_mode'] = experiment_mode

        # 設定辞書を使用してTwoStageTrainerを直接初期化
        trainer = TwoStageTrainer(
            config=self.config,
            device=self.device,
            output_dir=str(self.output_dir),
            use_kalman_filtering=use_kalman
        )

        # 統合学習（各エポックでPhase-1 + Phase-2を連続実行）
        print("統合学習開始...")

        # Step 5: ターゲットデータ抽出と学習実行
        if experiment_mode == "target_prediction":
            target_train = self._extract_targets_from_dict(data_dict, 'train')
            target_val = self._extract_targets_from_dict(data_dict, 'val') if data_dict.get('val') is not None else None

            integrated_results = trainer.train_integrated(
                Y_train=data_dict['train'],
                Y_val=data_dict['val'],
                target_train=target_train,
                target_val=target_val
            )
        else:
            integrated_results = trainer.train_integrated(
                Y_train=data_dict['train'],
                Y_val=data_dict['val']
            )

        total_elapsed = (datetime.now() - start_time).total_seconds()
        print(f"統合学習完了 ({total_elapsed:.1f}秒)")

        # 学習結果統合（統合学習形式）
        training_results = {
            'integrated': integrated_results,
            'phase1_metrics': integrated_results['phase1_metrics'],
            'phase2_losses': integrated_results['phase2_losses'],
            'integrated_metrics': integrated_results['integrated_metrics'],
            'total_time': total_elapsed,
            'use_kalman': use_kalman
        }
        
        # 学習過程可視化
        self._plot_training_progress(training_results)
        
        # 学習結果保存
        results_path = self.output_dir / 'logs' / 'training_results.json'
        with open(results_path, 'w') as f:
            # Tensor等はJSON非対応のため、シリアライズ可能な形式に変換
            serializable_results = self._make_json_serializable(training_results)
            json.dump(serializable_results, f, indent=2)
        
        # ログ記録
        self.experiment_log.append({
            'timestamp': datetime.now().isoformat(),
            'event': 'integrated_training_complete',
            'total_time': total_elapsed,
            'epochs': len(integrated_results.get('integrated_metrics', [])),
            'use_kalman': use_kalman
        })
        
        return {
            'trainer': trainer,
            'results': training_results
        }
    
    def step_3_model_analysis(self, trainer: TwoStageTrainer, data_dict: Dict[str, torch.Tensor]):
        """
        Step 3.3: 学習済み転送作用素の表現確認

        Args:
            trainer: 学習済み学習器
            data_dict: データ辞書
        """
        print("\n" + "="*5)
        print("Step 3.3: モデル分析")
        print("="*5)

        start_time = datetime.now()

        # 転送作用素取得・保存
        operators_info = self._analyze_transfer_operators(trainer)

        # 内部表現分析
        representations_info = self._analyze_internal_representations(trainer, data_dict)

        # エンコード特徴空間可視化（設定可能パラメータ）
        # 旧名: state_space_viz → 新名: encoded_feature_space_viz
        viz_config = self.config.get('evaluation', {}).get('encoded_feature_space_viz', {})
        dim_indices = tuple(viz_config.get('dim_indices', [0, 1]))
        max_samples = viz_config.get('max_samples', 100)
        self._visualize_encoded_feature_space(trainer, data_dict['test'], dim_indices, max_samples)

        # モード分解分析（新機能）
        mode_decomp_info = self._perform_mode_decomposition_analysis(trainer)

        # Step 6-8: 実験モード別評価（統合）
        target_evaluation_info = {}
        reconstruction_evaluation_info = {}
        experiment_mode = data_dict.get('experiment_mode', 'reconstruction')

        if experiment_mode == "target_prediction":
            target_evaluation_info = self._perform_target_prediction_evaluation(trainer, data_dict)
        elif experiment_mode == "reconstruction":
            reconstruction_evaluation_info = self._perform_reconstruction_evaluation(trainer, data_dict)

        elapsed = (datetime.now() - start_time).total_seconds()
        print(f"表現分析・モード分解・評価完了 ({elapsed:.1f}秒)")

        # 分析結果保存
        analysis_results = {
            'operators': operators_info,
            'representations': representations_info,
            'mode_decomposition': mode_decomp_info,
            'target_evaluation': target_evaluation_info,  # Step 6
            'reconstruction_evaluation': reconstruction_evaluation_info,  # Step 8追加
            'analysis_time': elapsed
        }

        with open(self.output_dir / 'logs' / 'model_analysis.json', 'w') as f:
            serializable_analysis = self._make_json_serializable(analysis_results)
            json.dump(serializable_analysis, f, indent=2)

        # ログ記録
        self.experiment_log.append({
            'timestamp': datetime.now().isoformat(),
            'event': 'model_analysis_complete',
            'elapsed_seconds': elapsed
        })
    
    def finalize_experiment(self, trainer: TwoStageTrainer):
        """実験終了処理"""
        print("\n" + "="*5)
        print("実験終了処理")
        print("="*5)
        
        # 最終モデル保存
        model_path = self.output_dir / 'models' / 'final_model.pth'
        trainer._save_inference_ready_model(str(model_path))
        print(f"最終モデル保存: {model_path}")
        
        # 実験設定保存（YAML + TXT形式）
        config_path = self.output_dir / 'logs' / 'experiment_config.yaml'
        with open(config_path, 'w') as f:
            yaml.dump(self.config, f, default_flow_style=False)

        # TXT形式でも設定情報を保存
        config_txt_path = self.output_dir / 'logs' / 'experiment_config.txt'
        with open(config_txt_path, 'w', encoding='utf-8') as f:
            f.write("=== DFIV Kalman Filter実験設定情報 ===\n")
            f.write(f"実験日時: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"出力ディレクトリ: {self.output_dir}\n\n")

            # 設定内容を階層的に出力
            def write_config_section(config_dict, prefix=""):
                for key, value in config_dict.items():
                    if isinstance(value, dict):
                        f.write(f"{prefix}[{key}]\n")
                        write_config_section(value, prefix + "  ")
                    else:
                        f.write(f"{prefix}{key}: {value}\n")

            write_config_section(self.config)
        
        # 完全実験ログ保存
        end_time = datetime.now()
        total_time = (end_time - self.start_time).total_seconds()
        
        self.experiment_log.append({
            'timestamp': end_time.isoformat(),
            'event': 'experiment_complete',
            'total_experiment_time': total_time
        })
        
        with open(self.output_dir / 'logs' / 'full_experiment_log.json', 'w') as f:
            json.dump(self.experiment_log, f, indent=2)
        
        print(f"総実験時間: {total_time:.1f}秒 ({total_time/60:.1f}分)")
        print(f"実験完了: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"全結果保存先: {self.output_dir}")
    
    def _plot_data_overview(self, data_dict: Dict[str, torch.Tensor]):
        """データ概要プロット（画像・時系列データ対応）"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        fig.suptitle('Data Overview', fontsize=14)

        train_data = data_dict['train'].cpu().numpy()

        # データ形状に応じた処理
        if len(train_data.shape) == 4:  # 画像データ (T, H, W, C)
            T, H, W, C = train_data.shape

            # 画像サンプル表示（最初の数フレーム）
            sample_images = train_data[:min(6, T)]  # 最初の6フレーム
            for i, img in enumerate(sample_images):
                if i >= 6:
                    break
                row = i // 3
                col = i % 3
                if row < 2 and col < 2:
                    if C == 1:
                        axes[row, col].imshow(img.squeeze(-1), cmap='gray')
                    else:
                        axes[row, col].imshow(img)
                    axes[row, col].set_title(f'Frame {i}')
                    axes[row, col].axis('off')

            # データ分割比率
            sizes = [data_dict['train'].shape[0], data_dict['val'].shape[0], data_dict['test'].shape[0]]
            if len(sizes) >= 3:
                axes[0, 1].pie(sizes, labels=['Train', 'Val', 'Test'], autopct='%1.1f%%')
                axes[0, 1].set_title('Data Split Ratio')

            # ピクセル値分布
            axes[1, 0].hist(train_data.flatten(), bins=50, alpha=0.7)
            axes[1, 0].set_title('Pixel Value Distribution')
            axes[1, 0].set_xlabel('Pixel Value')
            axes[1, 0].set_ylabel('Frequency')
            axes[1, 0].grid(True)

            # データ統計
            stats_text = f"""
            Data Type: Image Sequence
            Shape: {train_data.shape}
            Time steps: {T}
            Image size: {H}×{W}×{C}
            Mean: {train_data.mean():.3f}
            Std: {train_data.std():.3f}
            Min: {train_data.min():.3f}
            Max: {train_data.max():.3f}
            """

        else:  # 時系列データ (T, d)
            # 時系列プロット（最初の3次元）
            for i in range(min(3, train_data.shape[1])):
                axes[0, 0].plot(train_data[:, i], label=f'Feature {i+1}')
            axes[0, 0].set_title('Training Data Time Series')
            axes[0, 0].legend()
            axes[0, 0].grid(True)

            # データ分割比率
            sizes = [data_dict['train'].shape[0], data_dict['val'].shape[0], data_dict['test'].shape[0]]
            axes[0, 1].pie(sizes, labels=['Train', 'Val', 'Test'], autopct='%1.1f%%')
            axes[0, 1].set_title('Data Split Ratio')

            # 特徴量分布
            axes[1, 0].hist(train_data.flatten(), bins=50, alpha=0.7)
            axes[1, 0].set_title('Feature Value Distribution')
            axes[1, 0].set_xlabel('Value')
            axes[1, 0].set_ylabel('Frequency')
            axes[1, 0].grid(True)

            # データ統計
            stats_text = f"""
            Data Type: Time Series
            Shape: {train_data.shape}
            Mean: {train_data.mean():.3f}
            Std: {train_data.std():.3f}
            Min: {train_data.min():.3f}
            Max: {train_data.max():.3f}
            """

        axes[1, 1].text(0.1, 0.5, stats_text, transform=axes[1, 1].transAxes,
                        verticalalignment='center', fontsize=10)
        axes[1, 1].set_title('Data Statistics')
        axes[1, 1].axis('off')

        plt.tight_layout()
        plt.savefig(self.output_dir / 'plots' / 'data_overview.png', dpi=300)
        plt.close()
    
    def _plot_training_progress(self, results: Dict[str, Any]):
        """学習過程可視化（統合学習対応、ターゲット予測対応）"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 8))
        fig.suptitle('Training Progress', fontsize=14)

        # Step 5: experiment_mode判定
        experiment_mode = self.config.get('experiment', {}).get('mode', 'reconstruction')

        # Phase-2損失推移（統合学習）
        if 'phase2_losses' in results and len(results['phase2_losses']) > 0:
            phase2_data = results['phase2_losses']
            epochs = list(range(len(phase2_data)))

            if experiment_mode == "target_prediction":
                # Step 5: ターゲット予測モード用の可視化
                total_losses = [entry.get('total_loss', 0) for entry in phase2_data]
                target_losses = [entry.get('loss_target', entry.get('target_loss', 0)) for entry in phase2_data]
                cca_losses = [entry.get('cca_loss', 0) for entry in phase2_data]

                axes[0, 0].plot(epochs, target_losses, label='Target Loss (MSE)', color='red', linewidth=2)
                axes[0, 0].plot(epochs, total_losses, label='Total Loss', color='blue')
                axes[0, 0].plot(epochs, cca_losses, label='CCA Loss', color='green')
                axes[0, 0].set_title('Target Prediction Loss')
                axes[0, 0].set_ylabel('MSE / Total Loss')
                axes[0, 0].set_xlabel('Epoch')
                axes[0, 0].legend()
                axes[0, 0].grid(True)

                # ターゲット損失の詳細
                axes[0, 1].plot(epochs, target_losses, 'r-', linewidth=2)
                axes[0, 1].set_title('Target Prediction MSE Loss')
                axes[0, 1].set_xlabel('Epoch')
                axes[0, 1].set_ylabel('MSE')
                axes[0, 1].grid(True)

            else:
                # 既存の再構成モード用の可視化
                total_losses = [entry['total_loss'] for entry in phase2_data]
                rec_losses = [entry.get('rec_loss', entry.get('loss_rec', 0)) for entry in phase2_data]
                cca_losses = [entry['cca_loss'] for entry in phase2_data]

                axes[0, 0].plot(epochs, total_losses, label='Total Loss')
                axes[0, 0].plot(epochs, rec_losses, label='Reconstruction Loss')
                axes[0, 0].plot(epochs, cca_losses, label='CCA Loss')
                axes[0, 0].set_title('Phase-2 Loss Components')
                axes[0, 0].set_xlabel('Epoch')
                axes[0, 0].set_ylabel('Loss')
                axes[0, 0].legend()
                axes[0, 0].grid(True)

                # CCA損失の詳細（動的変化確認用）
                axes[0, 1].plot(epochs, cca_losses, 'r-', linewidth=2)
                axes[0, 1].set_title('CCA Loss (Dynamic Check)')
                axes[0, 1].set_xlabel('Epoch')
                axes[0, 1].set_ylabel('CCA Loss')
                axes[0, 1].grid(True)
        else:
            # データがない場合の表示
            loss_type = "Target Loss" if experiment_mode == "target_prediction" else "Phase-2 Loss"
            axes[0, 0].text(0.5, 0.5, f'No {loss_type} training data available',
                           ha='center', va='center', transform=axes[0, 0].transAxes)
            axes[0, 0].set_title(f'{loss_type} Components')

            detail_type = "Target MSE" if experiment_mode == "target_prediction" else "CCA"
            axes[0, 1].text(0.5, 0.5, f'No {detail_type} loss data available',
                           ha='center', va='center', transform=axes[0, 1].transAxes)
            axes[0, 1].set_title(f'{detail_type} Loss')
        
        # 統合学習時間表示
        total_time = results.get('total_time', 0)
        axes[1, 0].bar(['Integrated Training'], [total_time])
        axes[1, 0].set_title('Training Time')
        axes[1, 0].set_ylabel('Time (seconds)')

        # 学習設定情報（統合学習版、experiment_mode対応）
        phase2_count = len(results.get('phase2_losses', []))
        integrated_count = len(results.get('integrated_metrics', []))

        info_text = f"""
        Experiment Mode: {experiment_mode}
        Total Training Time: {total_time:.1f}s
        Kalman Filtering: {results.get('use_kalman', False)}
        Phase-2 Epochs: {phase2_count}
        Integrated Epochs: {integrated_count}
        Status: {'Completed' if phase2_count > 0 else 'Phase-1 Only'}
        """
        axes[1, 1].text(0.1, 0.5, info_text, transform=axes[1, 1].transAxes,
                        verticalalignment='center', fontsize=10)
        axes[1, 1].set_title('Training Info')
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        plt.savefig(self.output_dir / 'plots' / 'training_progress.png', dpi=300)
        plt.close()
    
    def _analyze_transfer_operators(self, trainer: TwoStageTrainer) -> Dict[str, Any]:
        """転送作用素分析"""
        operators_info = {}
        
        # DF-A転送作用素（V_A, U_A）
        if hasattr(trainer, 'df_state') and trainer.df_state is not None:
            try:
                state_dict = trainer.df_state.get_state_dict()
                if 'V_A' in state_dict:
                    V_A = state_dict['V_A']
                    operators_info['V_A_shape'] = list(V_A.shape)
                    operators_info['V_A_norm'] = float(torch.norm(V_A).item())
                if 'U_A' in state_dict:
                    U_A = state_dict['U_A']
                    operators_info['U_A_shape'] = list(U_A.shape)
                    operators_info['U_A_norm'] = float(torch.norm(U_A).item())
            except Exception as e:
                print(f"DF-A転送作用素分析エラー: {e}")
        
        # DF-B転送作用素（V_B, u_B）
        if hasattr(trainer, 'df_obs') and trainer.df_obs is not None:
            try:
                obs_dict = trainer.df_obs.get_state_dict()
                if 'V_B' in obs_dict:
                    V_B = obs_dict['V_B']
                    operators_info['V_B_shape'] = list(V_B.shape)
                    operators_info['V_B_norm'] = float(torch.norm(V_B).item())
                if 'u_B' in obs_dict:
                    u_B = obs_dict['u_B']
                    operators_info['u_B_shape'] = list(u_B.shape)
                    operators_info['u_B_norm'] = float(torch.norm(u_B).item())
            except Exception as e:
                print(f"DF-B転送作用素分析エラー: {e}")
        
        # 転送作用素保存
        operators_path = self.output_dir / 'artifacts' / 'transfer_operators.pth'
        try:
            operators_data = {
                'df_state': trainer.df_state.get_state_dict() if hasattr(trainer, 'df_state') and trainer.df_state else None,
                'df_obs': trainer.df_obs.get_state_dict() if hasattr(trainer, 'df_obs') and trainer.df_obs else None
            }
            torch.save(operators_data, operators_path)
            print(f"転送作用素保存: {operators_path}")
        except Exception as e:
            print(f"転送作用素保存エラー: {e}")
        
        return operators_info
    
    def _analyze_internal_representations(self, trainer: TwoStageTrainer, data_dict: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """内部表現分析"""
        representations_info = {}
        
        try:
            # エンコーダ表現分析
            if hasattr(trainer, 'encoder'):
                # デバイス整合性確保
                trainer.encoder = trainer.encoder.to(trainer.device)

                test_sample = data_dict['test'][:100]  # 最初の100サンプル
                test_sample = test_sample.to(trainer.device)  # デバイス統一

                with torch.no_grad():
                    # unsqueeze/squeeze操作を削除（TCN廃止に伴う修正）
                    encoded = trainer.encoder(test_sample)
                    representations_info['encoder_output_shape'] = list(encoded.shape)
                    representations_info['encoder_output_mean'] = float(encoded.mean().item())
                    representations_info['encoder_output_std'] = float(encoded.std().item())

        except Exception as e:
            import traceback
            print(f"内部表現分析エラー: {e}")
            print(f"詳細トレースバック:\n{traceback.format_exc()}")
        
        return representations_info
    
    def _visualize_encoded_feature_space(self, trainer: TwoStageTrainer, test_data: torch.Tensor,
                                        dim_indices: tuple = (0, 1), max_samples: int = 100):
        """エンコード特徴空間可視化（時系列プロットのみ）

        Args:
            trainer: 学習済みトレーナー
            test_data: テストデータ
            dim_indices: 表示する次元のインデックス (デフォルト: 最初の2次元)
            max_samples: 最大表示サンプル数 (デフォルト: 100)

        設定例:
            evaluation:
              encoded_feature_space_viz:
                dim_indices: [2, 5]  # 3番目と6番目の次元を表示
                max_samples: 150     # 最大150サンプル表示
        """
        try:
            # デバイス整合性確保
            trainer.encoder = trainer.encoder.to(trainer.device)
            test_data = test_data.to(trainer.device)

            # データ数調整: 入力が指定数以下なら入力長、そうでなければ指定数
            n_samples = min(len(test_data), max_samples)
            print(f"エンコード特徴空間可視化: {n_samples}サンプル、次元{dim_indices}")

            with torch.no_grad():
                if hasattr(trainer, 'encoder'):
                    # エンコード実行
                    encoded = trainer.encoder(test_data[:n_samples])

                    # 指定次元の確認
                    if encoded.shape[1] <= max(dim_indices):
                        print(f"警告: 指定次元{dim_indices}が特徴量次元{encoded.shape[1]}を超えています")
                        dim_indices = (0, min(1, encoded.shape[1]-1))
                        print(f"次元を調整: {dim_indices}")

                    # 時系列プロットのみ（簡略版）
                    plt.figure(figsize=(12, 6))

                    plt.plot(encoded[:, dim_indices[0]].cpu().numpy(),
                            label=f'Dim {dim_indices[0]}', linewidth=2)
                    plt.plot(encoded[:, dim_indices[1]].cpu().numpy(),
                            label=f'Dim {dim_indices[1]}', linewidth=2)

                    plt.title(f'Feature Trajectory (Dims {dim_indices[0]}, {dim_indices[1]})')
                    plt.xlabel('Time Step')
                    plt.ylabel('Feature Value')
                    plt.legend()
                    plt.grid(True, alpha=0.3)


                    plt.tight_layout()
                    plt.savefig(self.output_dir / 'plots' / 'encoded_feature_space_visualization.png', dpi=300)
                    plt.close()

                    print("エンコード特徴空間可視化完了")

        except Exception as e:
            import traceback
            print(f"エンコード特徴空間可視化エラー: {e}")
            print(f"詳細トレースバック:\n{traceback.format_exc()}")

    def _perform_mode_decomposition_analysis(self, trainer: TwoStageTrainer) -> Dict[str, Any]:
        """モード分解分析実行"""
        mode_decomp_info = {}

        try:
            # サンプリング間隔取得（設定から、またはデフォルト値）
            sampling_interval = self.config.get('evaluation', {}).get('spectrum_analysis', {}).get('sampling_interval', 0.1)

            print(f"モード分解分析開始 (Δt={sampling_interval})")

            # モデルスペクトル分析器作成
            model_spectrum_analyzer = TrainedModelSpectrumAnalysis(sampling_interval)

            # V_A行列抽出・スペクトル分析
            if hasattr(trainer, 'df_state') and trainer.df_state is not None:
                try:
                    # DF-A状態層からV_A抽出（複数の方法で試行）
                    V_A = None

                    # 方法1: get_state_dict()を使用
                    state_dict = trainer.df_state.get_state_dict()
                    if 'V_A' in state_dict:
                        V_A = state_dict['V_A']
                        print(f"V_A行列をstate_dictから取得: shape={V_A.shape}")

                    # 方法2: 直接アクセス
                    elif hasattr(trainer.df_state, 'V_A') and trainer.df_state.V_A is not None:
                        V_A = trainer.df_state.V_A
                        print(f"V_A行列を直接取得: shape={V_A.shape}")

                    # 方法3: キャッシュから取得
                    elif hasattr(trainer.df_state, '_stage1_cache') and 'V_A' in trainer.df_state._stage1_cache:
                        V_A = trainer.df_state._stage1_cache['V_A']
                        print(f"V_A行列をキャッシュから取得: shape={V_A.shape}")

                    if V_A is not None:

                        # スペクトル分析実行
                        spectrum_analysis = model_spectrum_analyzer.analyzer.analyze_spectrum(V_A)

                        # 結果統計
                        mode_decomp_info = {
                            'V_A_shape': list(V_A.shape),
                            'spectral_radius': spectrum_analysis['spectral_radius'],
                            'n_stable_modes': spectrum_analysis['n_stable_modes'],
                            'n_dominant_modes': spectrum_analysis['n_dominant_modes'],
                            'dominant_indices': spectrum_analysis['dominant_indices'],
                            'stable_indices': spectrum_analysis['stable_indices'],
                            'sampling_interval': sampling_interval
                        }

                        # 固有値統計（複素数は分離して保存）
                        eigenvals_continuous = spectrum_analysis['eigenvalues_continuous']
                        mode_decomp_info['eigenvalues_statistics'] = {
                            'mean_growth_rate': float(eigenvals_continuous.real.mean().item()),
                            'std_growth_rate': float(eigenvals_continuous.real.std().item()),
                            'mean_frequency_hz': float(spectrum_analysis['frequencies_hz'].mean().item()),
                            'std_frequency_hz': float(spectrum_analysis['frequencies_hz'].std().item())
                        }

                        # スペクトル分析結果の詳細保存
                        spectrum_save_path = self.output_dir / 'artifacts' / 'mode_decomposition'
                        SpectrumResultsSaver.save_results(
                            {'spectrum': spectrum_analysis, 'V_A': V_A, 'sampling_interval': sampling_interval},
                            str(spectrum_save_path),
                            save_format='both'
                        )

                        mode_decomp_info['detailed_results_saved'] = True
                        mode_decomp_info['save_path'] = str(spectrum_save_path)

                        print(f"モード分解完了:")
                        print(f"  - スペクトル半径: {spectrum_analysis['spectral_radius']:.4f}")
                        print(f"  - 安定モード数: {spectrum_analysis['n_stable_modes']}")
                        print(f"  - 主要モード数: {spectrum_analysis['n_dominant_modes']}")
                        print(f"  - 詳細結果保存: {spectrum_save_path}")

                    else:
                        # 詳細なデバッグ情報を出力
                        print(f"V_A行列が見つかりません")
                        # print(f"デバッグ情報:")
                        # print(f"  - df_state._is_fitted: {getattr(trainer.df_state, '_is_fitted', 'N/A')}")
                        # print(f"  - hasattr(df_state, 'V_A'): {hasattr(trainer.df_state, 'V_A')}")
                        # if hasattr(trainer.df_state, 'V_A'):
                        #     print(f"  - df_state.V_A is None: {trainer.df_state.V_A is None}")
                        # print(f"  - state_dict keys: {list(state_dict.keys())}")
                        if hasattr(trainer.df_state, '_stage1_cache'):
                            print(f"  - _stage1_cache keys: {list(trainer.df_state._stage1_cache.keys()) if trainer.df_state._stage1_cache else 'None'}")
                        mode_decomp_info['error'] = 'V_A not found in df_state'
                        mode_decomp_info['debug_info'] = {
                            'is_fitted': getattr(trainer.df_state, '_is_fitted', False),
                            'has_V_A_attr': hasattr(trainer.df_state, 'V_A'),
                            'state_dict_keys': list(state_dict.keys())
                        }

                except Exception as e:
                    print(f"モード分解分析エラー: {e}")
                    mode_decomp_info['error'] = str(e)
            else:
                print(f"DF-A状態層が見つかりません")
                mode_decomp_info['error'] = 'df_state layer not found'

        except Exception as e:
            print(f"モード分解分析初期化エラー: {e}")
            mode_decomp_info['error'] = str(e)

        return mode_decomp_info

    def _extract_targets_from_dict(self, data_dict: Dict[str, torch.Tensor], split: str) -> torch.Tensor:
        """
        データ辞書からターゲット情報抽出（包括的対応）

        Args:
            data_dict: 全データ辞書（ターゲットデータ含む）
            split: データ分割名 ('train', 'val', 'test')

        Returns:
            抽出されたターゲットデータ
        """
        try:
            # Step 1: データローダーが追加したターゲットデータを優先使用
            target_key = f'{split}_targets'
            if target_key in data_dict:
                target_data = data_dict[target_key]
                print(f"{split}分割専用ターゲットデータ使用: shape={target_data.shape}")
                return target_data

            # Step 2: メタデータベースのターゲット抽出（従来実装の保持）
            data = data_dict[split]
            metadata = data_dict.get('metadata')
            if hasattr(metadata, 'has_target_data') and metadata.has_target_data:
                if hasattr(metadata, 'target_indices'):
                    target_indices = metadata.target_indices
                    if isinstance(target_indices, (list, tuple, torch.Tensor)):
                        targets = data[:, target_indices] if len(data.shape) >= 2 else data[target_indices]
                        print(f"メタデータベースターゲット抽出: shape={targets.shape}")
                        return targets

            # Step 3: フォールバック - 入力データを自己予測ターゲットとして使用
            print(f"専用ターゲットデータが見つかりません。入力データをターゲットとして使用: shape={data.shape}")
            return data

        except Exception as e:
            # print(f"ターゲットデータ抽出エラー: {e}")
            # print(f"フォールバック: 入力データをターゲットとして使用")
            return data_dict[split]

    def _extract_targets(self, data: torch.Tensor, metadata: Optional[DataMetadata] = None) -> torch.Tensor:
        """
        データからターゲット情報抽出（既存メソッド保持・後方互換性）

        Args:
            data: 入力データテンソル
            metadata: データメタデータ

        Returns:
            抽出されたターゲットデータ
        """
        try:
            # メタデータベースのターゲット抽出
            if hasattr(metadata, 'has_target_data') and metadata.has_target_data:
                if hasattr(metadata, 'target_indices'):
                    target_indices = metadata.target_indices
                    if isinstance(target_indices, (list, tuple, torch.Tensor)):
                        targets = data[:, target_indices] if len(data.shape) >= 2 else data[target_indices]
                        print(f"ターゲットデータ抽出成功: shape={targets.shape}")
                        return targets

            # フォールバック: データ全体をターゲットとして使用（自己予測）
            # print(f"専用ターゲットデータが見つかりません。入力データをターゲットとして使用: shape={data.shape}")
            return data

        except Exception as e:
            # print(f"ターゲットデータ抽出エラー: {e}")
            # print(f"フォールバック: 入力データをターゲットとして使用")
            return data

    def _predict_targets(self, test_data: torch.Tensor, trainer: TwoStageTrainer) -> torch.Tensor:
        """
        ターゲット予測実行（Step 10: 定式化準拠プロセス統一）

        既存の_perform_reconstruction_with_existing_process()と同一のステップで、
        最後のデコード部分のみtarget_decoderに変更して状態空間モデル完全活用

        Args:
            test_data: テストデータテンソル
            trainer: 学習済み学習器

        Returns:
            予測されたターゲットデータ
        """
        trainer.encoder.eval()
        if hasattr(trainer, 'decoder'):
            trainer.decoder.eval()
        if hasattr(trainer, 'target_decoder') and trainer.target_decoder is not None:
            trainer.target_decoder.eval()

        with torch.no_grad():
            try:
                # 既存再構成プロセスの再現（_perform_reconstruction_with_existing_processと同じ構造）

                # 時系列長とパラメータ取得
                T = test_data.shape[0]
                h = trainer.realization.h

                if T <= 2 * h:
                    # 短時系列の場合：簡単なencoder→target_decoder
                    M_features = trainer.encoder(test_data)
                    if M_features.dim() == 1:
                        M_features = M_features.unsqueeze(1)
                    if hasattr(trainer, 'target_decoder') and trainer.target_decoder is not None:
                        return trainer.target_decoder(M_features)
                    else:
                        # フォールバック: 簡易プロセス
                        encoded = trainer.encoder(test_data)
                        if hasattr(trainer, 'target_decoder') and trainer.target_decoder is not None:
                            return trainer.target_decoder(encoded)
                        elif hasattr(trainer, 'decoder') and trainer.decoder is not None:
                            return trainer.decoder(encoded)
                        else:
                            return encoded

                # Step 1: エンコード y_t → m_t（既存プロセス準拠）
                M_features = trainer.encoder(test_data)
                if M_features.dim() == 1:
                    M_features = M_features.unsqueeze(1)

                # Step 2: 確率的実現（既存の実装を活用）
                try:
                    from src.ssm.realization import StochasticRealizationWithEncoder
                    if isinstance(trainer.realization, StochasticRealizationWithEncoder):
                        trainer.realization.fit(test_data, trainer.encoder)
                        X_states = trainer.realization.estimate_states(test_data)
                    else:
                        # スカラー化処理（既存実装準拠）
                        m_series_scalar = M_features.mean(dim=1)
                        X_states = trainer.realization.estimate_states(m_series_scalar.unsqueeze(1))
                except Exception:
                    # 確率実現エラー時の簡略化
                    X_states = M_features

                # Step 3: DF-A予測 x_{t-1} → x̂_{t|t-1}（学習時フロー完全準拠）
                print(f"[DF-A] 入力状態: X_states.shape={X_states.shape}")
                X_hat_states = trainer.df_state.predict_sequence(X_states)
                T_pred = X_hat_states.size(0)
                print(f"[DF-A] 予測状態: X_hat_states.shape={X_hat_states.shape}, T_pred={T_pred}")

                # Step 4: DF-B予測 x̂_{t|t-1} → m̂_{t|t-1}（学習時フロー完全準拠）
                M_hat_series = []
                for t in range(T_pred):
                    m_hat_t = trainer.df_obs.predict_one_step(X_hat_states[t])
                    M_hat_series.append(m_hat_t)
                M_hat_tensor = torch.stack(M_hat_series)  # (T_pred, 50次元)
                M_hat_tensor = trainer._ensure_device(M_hat_tensor)  # GPU整合性確保

                # Step 5: ターゲット予測 m̂_{t|t-1} → target_t（学習と完全同一入力）
                targets = trainer.target_decoder(M_hat_tensor)
                print(f"[DF-Flow] 定式化準拠ターゲット予測完了: {targets.shape}")
                return targets

            except Exception as e:
                print(f"ターゲット予測エラー: {e}")
                raise RuntimeError(f"ターゲット予測フロー実行失敗: {e}") from e

    def _perform_target_prediction_evaluation(
        self,
        trainer: TwoStageTrainer,
        data_dict: Dict[str, torch.Tensor]
    ) -> Dict[str, Any]:
        """
        ターゲット予測評価実行（Step 6機能）

        Args:
            trainer: 学習済み学習器
            data_dict: データ辞書

        Returns:
            ターゲット予測評価結果
        """
        print("\n" + "-"*40)
        print("ターゲット予測評価開始")
        print("-"*40)

        evaluation_results = {}

        try:
            # 既存の評価クラスをインポート
            from src.evaluation.metrics import TargetPredictionMetrics

            # 評価設定読み込み（デフォルトRMSE）
            evaluation_config = self.config.get('evaluation', {}).get('target_metrics', {})
            selected_metrics = evaluation_config.get('metrics', ['rmse'])
            print(f"評価指標: {selected_metrics}")

            # ターゲット予測評価器作成（既存パターンと統一）
            target_evaluator = TargetPredictionMetrics(device=str(self.device))

            # テストデータでの予測実行
            test_predictions = self._predict_targets(data_dict['test'], trainer)
            # data_dictから直接ターゲットデータを抽出（修正）
            test_targets = self._extract_targets_from_dict(data_dict, 'test')

            # 形状確認・調整（包括的対応）
            print(f"[Step1] 予測データ取得完了: {test_predictions.shape}")
            print(f"[Step1] ターゲットデータ取得完了: {test_targets.shape}")

            if test_predictions.shape != test_targets.shape:
                print(f"[Step2] 形状調整開始: predictions {test_predictions.shape} vs targets {test_targets.shape}")

                # 画像データ → ベクトル変換（フォールバック時の対応）
                if len(test_predictions.shape) > 2:
                    test_predictions = test_predictions.view(test_predictions.shape[0], -1)
                if len(test_targets.shape) > 2:
                    test_targets = test_targets.view(test_targets.shape[0], -1)

                # past_horizon処理統一（学習時フロー準拠）
                if test_predictions.shape[0] != test_targets.shape[0]:
                    pred_samples = test_predictions.shape[0]  # T_pred (111)
                    target_samples = test_targets.shape[0]    # T (151)

                    # 学習時処理準拠: target_data[h+1:h+1+T_pred] 相当の調整
                    # 仮定: h=past_horizon=20, T_pred=111
                    # 学習時: target_data[21:21+111] = target_data[21:132]
                    # 推論時: test_targets[21:132] に合わせる
                    if pred_samples < target_samples:
                        # past_horizon + 1 から開始して T_pred サンプル取得（学習時フロー準拠）
                        h = self.config.get('ssm', {}).get('realization', {}).get('past_horizon', 20)  # config駆動past_horizon取得
                        start_idx = h + 1  # 21
                        end_idx = start_idx + pred_samples  # 21 + 111 = 132
                        if end_idx <= target_samples:
                            test_targets = test_targets[start_idx:end_idx]
                            print(f"[Step3] past_horizon調整完了: target[{start_idx}:{end_idx}] → {test_targets.shape}")
                        else:
                            # フォールバック: 最後から T_pred サンプル
                            test_targets = test_targets[-pred_samples:]
                            print(f"[Step3] フォールバック調整: 最後{pred_samples}サンプル → {test_targets.shape}")
                    else:
                        test_predictions = test_predictions[:target_samples]
                        print(f"[Step3] 予測データ短縮: {target_samples}サンプルに調整")

            # 最終形状確認
            print(f"[Step4] 最終確認: predictions={test_predictions.shape}, targets={test_targets.shape}")
            if test_predictions.shape != test_targets.shape:
                print(f"[Step4] まだ形状不一致あり - 次元調整実行")
                min_dim = min(test_predictions.shape[1], test_targets.shape[1])
                test_predictions = test_predictions[:, :min_dim]
                test_targets = test_targets[:, :min_dim]
                print(f"[Step4] 次元調整完了: {test_predictions.shape}")
            else:
                print(f"[Step4] 形状一致確認OK！評価実行します")

            # 評価指標計算・表示（既存verboseパターンと統一）
            target_metrics = target_evaluator.compute_target_metrics(
                test_targets, test_predictions, metrics=selected_metrics, verbose=True
            )

            # 可視化生成（一旦スキップ、数値出力・保存で代替）
            generated_files = target_evaluator.create_target_visualizations(
                test_targets, test_predictions,
                metrics=selected_metrics,
                output_dir=str(self.output_dir / 'plots')
            )

            # 評価結果のJSONファイル保存
            experiment_info = {
                'experiment_mode': 'target_prediction',
                'test_data_shape': list(test_targets.shape),
                'predictions_shape': list(test_predictions.shape),
                'selected_metrics': selected_metrics,
                'model_architecture': 'RKN'
            }

            saved_metrics_file = target_evaluator.save_target_metrics_results(
                results=target_metrics,
                output_dir=str(self.output_dir / 'logs'),
                experiment_info=experiment_info
            )

            # 結果格納
            evaluation_results = {
                'metrics': target_metrics,
                'selected_metrics': selected_metrics,
                'generated_visualizations': generated_files,
                'saved_metrics_file': saved_metrics_file,
                'test_data_shape': list(test_targets.shape),
                'predictions_shape': list(test_predictions.shape),
                'evaluation_success': True
            }

            print(f"ターゲット予測評価完了")

        except Exception as e:
            print(f"ターゲット予測評価エラー: {e}")
            import traceback
            print(f"詳細トレースバック:\n{traceback.format_exc()}")

            evaluation_results = {
                'error': str(e),
                'evaluation_success': False
            }

        return evaluation_results

    def _perform_reconstruction_evaluation(
        self,
        trainer: TwoStageTrainer,
        data_dict: Dict[str, torch.Tensor]
    ) -> Dict[str, Any]:
        """
        再構成評価実行（Step 8機能、TargetPredictionパターン継承）

        Args:
            trainer: 学習済み学習器
            data_dict: データ辞書

        Returns:
            再構成評価結果
        """
        print("\n" + "-"*40)
        print("🖼️  データ再構成評価開始")
        print("-"*40)

        evaluation_results = {}

        try:
            # Step 8で実装した評価クラスをインポート
            from src.evaluation.metrics import ReconstructionMetrics

            # 評価設定読み込み（デフォルトreconstruction_rmse）
            evaluation_config = self.config.get('evaluation', {}).get('reconstruction_metrics', {})
            selected_metrics = evaluation_config.get('metrics', ['reconstruction_rmse'])
            print(f"評価指標: {selected_metrics}")

            # 再構成評価器作成（統一インターフェース）
            reconstruction_evaluator = ReconstructionMetrics(device=str(self.device))

            # テストデータでの再構成実行
            test_reconstructions = self._reconstruct_data(data_dict['test'], trainer)
            test_originals = data_dict['test']

            # 形状確認・調整（TargetPredictionパターン継承）
            print(f"[Step1] 再構成データ取得完了: {test_reconstructions.shape}")
            print(f"[Step1] 元データ取得完了: {test_originals.shape}")

            if test_reconstructions.shape != test_originals.shape:
                print(f"[Step2] 形状調整開始: reconstructions {test_reconstructions.shape} vs originals {test_originals.shape}")

                # 形状調整（任意データ型対応）
                if len(test_reconstructions.shape) > 2:
                    test_reconstructions = test_reconstructions.view(test_reconstructions.shape[0], -1)
                if len(test_originals.shape) > 2:
                    test_originals = test_originals.view(test_originals.shape[0], -1)

                # past_horizon影響によるサイズ調整（再構成特有の処理）
                if test_reconstructions.shape[0] != test_originals.shape[0]:
                    rec_samples = test_reconstructions.shape[0]  # 130
                    orig_samples = test_originals.shape[0]       # 151

                    if rec_samples < orig_samples:
                        # past_horizon影響で再構成データが短い場合、元データも同じサイズに調整
                        # 最初のpast_horizon+1サンプルを削除（時系列開始部分）
                        trim_start = orig_samples - rec_samples  # 151 - 111 = 40
                        test_originals = test_originals[trim_start:]
                        print(f"[Step3] past_horizon調整完了: 元データ最初{trim_start}サンプル削除 → {test_originals.shape}")
                    else:
                        # 再構成データが長い場合（稀なケース）
                        test_reconstructions = test_reconstructions[:orig_samples]
                        print(f"[Step3] 再構成データ短縮: {orig_samples}サンプルに調整")

            # 最終形状確認
            print(f"[Step4] 最終確認: reconstructions={test_reconstructions.shape}, originals={test_originals.shape}")
            if test_reconstructions.shape != test_originals.shape:
                print(f"[Step4] まだ形状不一致あり - 次元調整実行")
                min_dim = min(test_reconstructions.shape[1], test_originals.shape[1])
                test_reconstructions = test_reconstructions[:, :min_dim]
                test_originals = test_originals[:, :min_dim]
                print(f"[Step4] 次元調整完了: {test_reconstructions.shape}")
            else:
                print(f"[Step4] 形状一致確認OK！評価実行します")

            # 評価指標計算・表示（統一verboseパターン）
            reconstruction_metrics = reconstruction_evaluator.compute_reconstruction_metrics(
                test_originals, test_reconstructions, metrics=selected_metrics, verbose=True
            )

            # 可視化生成（段階的実装、数値出力・保存で代替）
            generated_files = reconstruction_evaluator.create_reconstruction_visualizations(
                test_originals, test_reconstructions,
                metrics=selected_metrics,
                output_dir=str(self.output_dir / 'plots')
            )

            # 評価結果のJSONファイル保存（TargetPredictionパターン継承）
            experiment_info = {
                'experiment_mode': 'reconstruction',
                'test_data_shape': list(test_originals.shape),
                'reconstructions_shape': list(test_reconstructions.shape),
                'selected_metrics': selected_metrics,
                'model_architecture': 'RKN'
            }

            # 結果保存（既存パターン継承）
            saved_metrics_file = reconstruction_evaluator.save_reconstruction_metrics_results(
                results=reconstruction_metrics,
                output_dir=str(self.output_dir / 'logs'),
                experiment_info=experiment_info
            )

            # 結果格納（統一フォーマット）
            evaluation_results = {
                'metrics': reconstruction_metrics,
                'selected_metrics': selected_metrics,
                'generated_visualizations': generated_files,
                'saved_metrics_file': saved_metrics_file,
                'test_data_shape': list(test_originals.shape),
                'reconstructions_shape': list(test_reconstructions.shape),
                'evaluation_success': True
            }

            print(f"データ再構成評価完了")

        except Exception as e:
            print(f"データ再構成評価エラー: {e}")
            import traceback
            print(f"詳細トレースバック:\n{traceback.format_exc()}")

            evaluation_results = {
                'error': str(e),
                'evaluation_success': False
            }

        return evaluation_results

    def _reconstruct_data(self, test_data: torch.Tensor, trainer: TwoStageTrainer) -> torch.Tensor:
        """
        テストデータの再構成実行（既存TwoStageTrainer再構成プロセス活用）

        Args:
            test_data: テストデータ
            trainer: 学習済み学習器

        Returns:
            再構成されたデータ
        """
        try:
            with torch.no_grad():
                # 既存の再構成プロセスを活用
                trainer.encoder.eval()
                trainer.decoder.eval()

                # 既存の_forward_and_loss_phase2_reconstruction()プロセスを部分実行
                reconstructed_data = self._perform_reconstruction_with_existing_process(test_data, trainer)

                return reconstructed_data

        except Exception as e:
            print(f"既存再構成プロセスエラー: {e}")
            # フォールバック1: 既存メソッドでの完全再構成試行
            try:
                with torch.no_grad():
                    trainer.encoder.eval()
                    trainer.decoder.eval()
                    loss_total, loss_rec, loss_cca = trainer._forward_and_loss_phase2_reconstruction(test_data)

                    # 既存プロセスから再構成データを抽出（部分的に再実行）
                    M_features = trainer.encoder(test_data)
                    if M_features.dim() == 1:
                        M_features = M_features.unsqueeze(1)

                    # 簡略化: エンコード→デコード（フォールバック時）
                    reconstructed_data = trainer.decoder(M_features)
                    print("フォールバック再構成成功")
                    return reconstructed_data

            except Exception as e2:
                print(f"フォールバック再構成エラー: {e2}")
                # フォールバック2: 元データを返す
                return test_data

    def _perform_reconstruction_with_existing_process(self, test_data: torch.Tensor, trainer: TwoStageTrainer) -> torch.Tensor:
        """
        定式化準拠再構成フロー実行（Step 11修正）
        学習時フロー（src/training/two_stage_trainer.py:1532-1545）と完全同一
        """
        # 時系列長とパラメータ取得
        T = test_data.shape[0]
        h = trainer.realization.h

        if T <= 2 * h:
            raise RuntimeError(f"時系列長({T})が短すぎます: T <= 2*h({2*h})")

        # Step 1: エンコード y_t → m_t（学習時フロー準拠）
        M_features = trainer.encoder(test_data)
        if M_features.dim() == 1:
            M_features = M_features.unsqueeze(1)

        # Step 2: 確率的実現 m_t → x_t（学習時フロー準拠）
        from src.ssm.realization import StochasticRealizationWithEncoder
        if isinstance(trainer.realization, StochasticRealizationWithEncoder):
            trainer.realization.fit(test_data, trainer.encoder)
            X_states = trainer.realization.estimate_states(test_data)
        else:
            # スカラー化処理（既存実装準拠）
            m_series_scalar = M_features.mean(dim=1)
            trainer.realization.fit(m_series_scalar.unsqueeze(1))
            X_states = trainer.realization.filter(m_series_scalar.unsqueeze(1))

        # Step 3: DF-A予測 x_{t-1} → x̂_{t|t-1}（学習時フロー完全準拠）
        X_hat_states = trainer.df_state.predict_sequence(X_states)
        T_pred = X_hat_states.size(0)

        # Step 4: DF-B予測 x̂_{t|t-1} → m̂_{t|t-1}（学習時フロー完全準拠）
        M_hat_series = []
        for t in range(T_pred):
            m_hat_t = trainer.df_obs.predict_one_step(X_hat_states[t])
            M_hat_series.append(m_hat_t)
        M_hat_tensor = torch.stack(M_hat_series)  # (T_pred, 50次元)
        M_hat_tensor = trainer._ensure_device(M_hat_tensor)  # GPU整合性確保

        # Step 5: 再構成 m̂_{t|t-1} → ŷ_{t|t-1}（学習と完全同一入力）
        Y_hat = trainer.decoder(M_hat_tensor)
        print(f"[DF-Flow] 定式化準拠再構成完了: {Y_hat.shape}")
        return Y_hat

    def _make_json_serializable(self, obj):
        """JSON対応形式に変換"""
        if isinstance(obj, dict):
            return {k: self._make_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_json_serializable(v) for v in obj]
        elif isinstance(obj, torch.Tensor):
            return obj.cpu().numpy().tolist()
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.integer):
            return int(obj)
        else:
            return obj


def parse_args():
    """引数解析"""
    parser = argparse.ArgumentParser(description="タスク3: 完全実験パイプライン実行")
    
    parser.add_argument(
        '--config', '-c', type=str, required=True,
        help='実験設定ファイル (.yaml)'
    )
    parser.add_argument(
        '--data', '-d', type=str, required=True,
        help='データファイルパス'
    )
    parser.add_argument(
        '--output', '-o', type=str, required=True,
        help='結果出力ディレクトリ'
    )
    parser.add_argument(
        '--device', type=str, default=None,
        help='計算デバイス (auto選択時はNone)'
    )
    parser.add_argument(
        '--use-kalman', action='store_true',
        help='Kalmanフィルタリング有効化'
    )
    parser.add_argument(
        '--skip-analysis', action='store_true',
        help='Step 3.3 表現分析をスキップ'
    )
    
    return parser.parse_args()


def load_experiment_config(config_path: str) -> Dict[str, Any]:
    """実験設定読み込み"""
    if not Path(config_path).exists():
        raise FileNotFoundError(f"設定ファイルが見つかりません: {config_path}")
    
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # 必須セクションチェック
    required_sections = ['model', 'training']
    for section in required_sections:
        if section not in config:
            raise ValueError(f"設定に {section} セクションが必要です")
    
    return config


def main():
    """メイン実行関数"""
    args = parse_args()
    
    print("実験開始")
    print("="*5)
    
    # 設定読み込み
    config = load_experiment_config(args.config)
    
    # Kalmanフラグ設定
    if args.use_kalman:
        config.setdefault('training', {})['use_kalman_filtering'] = True
    
    # デバイス設定
    device = torch.device(args.device) if args.device else select_device()
    
    # 出力ディレクトリ
    output_dir = Path(args.output)
    
    # パイプライン実行
    pipeline = FullExperimentPipeline(config, output_dir, device)
    
    try:
        # Step 3.1: データ前処理
        data_dict = pipeline.step_1_data_loading(args.data)
        
        # Step 3.2: 完全学習実行
        training_result = pipeline.step_2_training_execution(data_dict)
        
        # Step 3.3: 表現分析（オプション）
        if not args.skip_analysis:
            pipeline.step_3_model_analysis(training_result['trainer'], data_dict)
        
        # 実験完了処理
        pipeline.finalize_experiment(training_result['trainer'])
        
        print("\n完全実験パイプライン正常終了！")
        print(f"結果: {output_dir}")
        
    except Exception as e:
        print(f"\n実験エラー: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())