#!/usr/bin/env python3
"""
ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

DFIV Kalman Filterã®çŠ¶æ…‹æ¨å®šæ€§èƒ½ã‚’åŒ…æ‹¬çš„ã«è©•ä¾¡ã—ã€
çµæœã‚’ã‚¿ãƒ¼ãƒŸãƒŠãƒ«å‡ºåŠ›ãƒ»æ•°å€¤ä¿å­˜ã™ã‚‹ã€‚

Usage:
    python scripts/evaluate_filtering_performance.py \
        --model_path results/trained_model.pth \
        --data_path data/test_data.npz \
        --output_dir results/filtering_evaluation \
        --config configs/inference_config.yaml
"""

import sys
import argparse
import torch
import numpy as np
from pathlib import Path
from datetime import datetime
import yaml
import json

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

from src.models.inference_model import InferenceModel
from src.evaluation.filtering_analysis import FilteringAnalyzer
from src.evaluation.uncertainly_evaluation import UncertaintyEvaluator
from src.utils.data_loader import load_experimental_data


class FilteringPerformanceEvaluator:
    """ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½è©•ä¾¡ã®çµ±åˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(
        self,
        model_path: str,
        config_path: str,
        output_dir: str,
        device: str = 'auto',
        config: dict = None
    ):
        self.model_path = Path(model_path)
        self.config_path = Path(config_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        if device == 'auto':
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        else:
            self.device = device
            
        print(f"ğŸ–¥ï¸  ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
        
        # è¨­å®šèª­ã¿è¾¼ã¿
        if config is not None:
            # å¤–éƒ¨ã‹ã‚‰è¨­å®šãŒæ¸¡ã•ã‚ŒãŸå ´åˆï¼ˆæ¨å¥¨ï¼‰
            self.config = config
            print(f"ğŸ“ å¤–éƒ¨è¨­å®šã‚’ä½¿ç”¨")
        else:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰ - æ˜ç¢ºãªé¸æŠåŸºæº–ãªã—
            raise ValueError(
                "è¨­å®šãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚run_filtering_evaluation.pyã‹ã‚‰é©åˆ‡ãªè¨­å®šã‚’æ¸¡ã—ã¦ãã ã•ã„ã€‚"
                "è¤‡æ•°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆYAMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®è‡ªå‹•é¸æŠã¯æœªå®Ÿè£…ã§ã™ã€‚"
            )

        # åˆ†æå™¨ã®åˆæœŸåŒ–
        self.filtering_analyzer = FilteringAnalyzer(str(self.output_dir), self.device)
        self.uncertainty_evaluator = UncertaintyEvaluator(str(self.output_dir))

        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        self.inference_model = None
        self._load_inference_model()
        
    def _load_inference_model(self):
        """æ¨è«–ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"""
        try:
            print(f"ğŸ“‚ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {self.model_path}")
            self.inference_model = InferenceModel(
                str(self.model_path), str(self.config_path)
            )
            print("âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def evaluate_comprehensive(
        self,
        data_path: str,
        experiment_name: str = None,
        data_split: str = 'test',
        save_detailed_results: bool = True,
        create_visualizations: bool = True
    ) -> dict:
        """
        åŒ…æ‹¬çš„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½è©•ä¾¡
        
        Args:
            data_path: ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            experiment_name: å®Ÿé¨“å
            data_split: ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿åˆ†å‰² ('test', 'val', 'all')
            save_detailed_results: è©³ç´°çµæœä¿å­˜ã™ã‚‹ã‹
            create_visualizations: å¯è¦–åŒ–ä½œæˆã™ã‚‹ã‹
            
        Returns:
            Dict: è©•ä¾¡çµæœ
        """
        if experiment_name is None:
            experiment_name = f"filtering_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
        print(f"\nğŸš€ åŒ…æ‹¬çš„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è©•ä¾¡é–‹å§‹")
        print(f"ğŸ“Š å®Ÿé¨“å: {experiment_name}")
        print(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿: {data_path}")
        print("="*70)
        
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        evaluation_data = self._load_evaluation_data(data_path, data_split)
        test_data = evaluation_data['observations']
        true_states = evaluation_data.get('true_states', None)
        
        print(f"ğŸ“ è©•ä¾¡ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {test_data.shape}")
        if true_states is not None:
            print(f"ğŸ“ çœŸå€¤çŠ¶æ…‹å½¢çŠ¶: {true_states.shape}")
        else:
            print("âš ï¸  çœŸå€¤çŠ¶æ…‹ãªã—ï¼ˆæ¨å®šç²¾åº¦è©•ä¾¡ã¯åˆ¶é™ã•ã‚Œã‚‹ï¼‰")
        
        # 2. æ¨è«–ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        self._setup_inference_environment(evaluation_data)
        
        # 3. ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°åˆ†æå®Ÿè¡Œ
        filtering_results = self.filtering_analyzer.analyze_filtering_performance(
            self.inference_model,
            test_data,
            true_states,
            experiment_name=experiment_name,
            save_results=save_detailed_results,
            verbose=True
        )
        
        # 4. ä¸ç¢ºå®Ÿæ€§è©³ç´°åˆ†æ
        uncertainty_results = self._analyze_uncertainty_details(
            filtering_results, true_states, create_visualizations
        )
        
        # 5. çµæœçµ±åˆãƒ»ä¿å­˜
        complete_results = self._compile_final_results(
            filtering_results, uncertainty_results, experiment_name, evaluation_data
        )
        
        # 6. æœ€çµ‚ã‚µãƒãƒªå‡ºåŠ›
        self._print_final_summary(complete_results)
        
        # 7. çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        if save_detailed_results:
            self._export_results(complete_results, experiment_name)
        
        return complete_results
    
    def _load_evaluation_data(self, data_path: str, data_split: str) -> dict:
        """è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        print(f"\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ç”¨
            data_dict = load_experimental_data(data_path)
            
            # æŒ‡å®šã•ã‚ŒãŸåˆ†å‰²ã‚’å–å¾—
            if data_split == 'test' and 'test' in data_dict:
                observations = data_dict['test']
            elif data_split == 'val' and 'val' in data_dict:
                observations = data_dict['val']
            elif data_split == 'all':
                # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
                obs_list = []
                for key in ['train', 'val', 'test']:
                    if key in data_dict:
                        obs_list.append(data_dict[key])
                if obs_list:
                    observations = torch.cat(obs_list, dim=0)
                else:
                    observations = data_dict[list(data_dict.keys())[0]]
            else:
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šåˆ©ç”¨å¯èƒ½ãªæœ€åˆã®ãƒ‡ãƒ¼ã‚¿
                observations = data_dict[list(data_dict.keys())[0]]
                print(f"âš ï¸  æŒ‡å®šåˆ†å‰² '{data_split}' ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã€‚'{list(data_dict.keys())[0]}' ã‚’ä½¿ç”¨ã€‚")
            
            # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
            observations = observations.to(self.device)
            
            # çœŸå€¤çŠ¶æ…‹ï¼ˆã‚‚ã—ã‚ã‚Œã°ï¼‰
            true_states = None
            if 'true_states' in data_dict:
                true_states = data_dict['true_states'].to(self.device)
            elif f'{data_split}_states' in data_dict:
                true_states = data_dict[f'{data_split}_states'].to(self.device)
                
            evaluation_data = {
                'observations': observations,
                'true_states': true_states,
                'metadata': {
                    'data_path': data_path,
                    'data_split': data_split,
                    'available_keys': list(data_dict.keys())
                }
            }
            
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
            return evaluation_data
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _setup_inference_environment(self, evaluation_data: dict):
        """æ¨è«–ç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        print(f"\nâš™ï¸  æ¨è«–ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
        
        try:
            # ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ï¼ˆè¦³æ¸¬ã®ä¸€éƒ¨ã‚’ä½¿ç”¨ï¼‰
            observations = evaluation_data['observations']

            # past_horizonã‚’è€ƒæ…®ã—ãŸæœ€å°ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºã‚’è¨ˆç®—
            past_horizon = self.config.get('ssm', {}).get('realization', {}).get('past_horizon', 10)
            min_required = 2 * past_horizon + 1  # realization.filterã«å¿…è¦ãªæœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°

            # åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºã‚’æ±ºå®š
            total_samples = observations.size(0)

            # DEBUG: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®è©³ç´°ãƒ­ã‚° (Resolved in Step 7)
            # print(f"ğŸ” DEBUG - ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºåˆ†æ:")
            # print(f"   è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ç·æ•°: {total_samples}")
            # print(f"   past_horizon: {past_horizon}")
            # print(f"   å¿…è¦æœ€å°ã‚µãƒ³ãƒ—ãƒ«: {min_required} (2*{past_horizon}+1)")
            # print(f"   ãƒ‡ãƒ¼ã‚¿åˆ†å‰²: {total_samples} // 4 = {total_samples // 4}")

            if total_samples >= min_required:
                calibration_size = min(50, max(min_required, total_samples // 4))
                print(f"âœ… ååˆ†ãªãƒ‡ãƒ¼ã‚¿: ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³{calibration_size}ã‚µãƒ³ãƒ—ãƒ«ä½¿ç”¨")
            else:
                # ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã€past_horizonã‚’èª¿æ•´
                calibration_size = total_samples
                print(f"âŒ ã€æ ¹æœ¬åŸå› ã€‘ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä¸è¶³: {total_samples}ã‚µãƒ³ãƒ—ãƒ« < å¿…è¦{min_required}")
                print(f"   â†’ ã‚ˆã‚Šå¤§ããªãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(æ¨å¥¨: >50ã‚µãƒ³ãƒ—ãƒ«)ãŒå¿…è¦")
                print(f"   â†’ ä¸€æ™‚å¯¾å‡¦: past_horizon={past_horizon}ã‚’èª¿æ•´ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨")

            calibration_data = observations[:calibration_size]
            
            # æ¨è«–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
            self.inference_model.setup_inference(
                calibration_data=calibration_data,
                method='data_driven'
            )
            
            print(f"âœ… æ¨è«–ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
            print(f"ğŸ“Š ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿: {calibration_data.shape}")
            
        except Exception as e:
            print(f"âŒ æ¨è«–ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _analyze_uncertainty_details(
        self, 
        filtering_results: dict, 
        true_states: torch.Tensor, 
        create_visualizations: bool
    ) -> dict:
        """ä¸ç¢ºå®Ÿæ€§è©³ç´°åˆ†æ"""
        if not filtering_results.get('batch_filtering', {}).get('success', False):
            print("âš ï¸  ãƒãƒƒãƒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¤±æ•—ã®ãŸã‚ã€ä¸ç¢ºå®Ÿæ€§åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—")
            return {}
        
        print(f"\nğŸ² ä¸ç¢ºå®Ÿæ€§è©³ç´°åˆ†æä¸­...")
        
        try:
            batch_result = filtering_results['batch_filtering']
            predictions = batch_result['estimated_states']
            covariances = batch_result['covariances']
            
            # æ¨™æº–åå·®æŠ½å‡º
            uncertainties = torch.sqrt(torch.diagonal(covariances, dim1=1, dim2=2))
            
            # ä¸ç¢ºå®Ÿæ€§è©•ä¾¡å®Ÿè¡Œ
            uncertainty_analysis = self.uncertainty_evaluator.evaluate_uncertainty_quality(
                predictions,
                uncertainties,
                true_states,
                save_plots=create_visualizations,
                verbose=True
            )
            
            print(f"âœ… ä¸ç¢ºå®Ÿæ€§è©³ç´°åˆ†æå®Œäº†")
            return uncertainty_analysis
            
        except Exception as e:
            print(f"âŒ ä¸ç¢ºå®Ÿæ€§åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {'error': str(e)}
    
    def _compile_final_results(
        self, 
        filtering_results: dict, 
        uncertainty_results: dict, 
        experiment_name: str,
        evaluation_data: dict
    ) -> dict:
        """æœ€çµ‚çµæœã®çµ±åˆ"""
        return {
            'experiment_info': {
                'name': experiment_name,
                'timestamp': datetime.now().isoformat(),
                'model_path': str(self.model_path),
                'config_path': str(self.config_path),
                'device': self.device,
                'data_info': evaluation_data['metadata']
            },
            'filtering_analysis': filtering_results,
            'uncertainty_analysis': uncertainty_results,
            'summary_metrics': self._extract_summary_metrics(filtering_results, uncertainty_results)
        }
    
    def _extract_summary_metrics(self, filtering_results: dict, uncertainty_results: dict) -> dict:
        """ã‚µãƒãƒªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æŠ½å‡º"""
        summary = {}
        
        # ãƒãƒƒãƒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä¸»è¦æŒ‡æ¨™
        if filtering_results.get('batch_filtering', {}).get('success', False):
            batch_metrics = filtering_results['batch_filtering']['metrics']
            
            if 'accuracy' in batch_metrics:
                acc = batch_metrics['accuracy']
                summary['batch_filtering'] = {
                    'mse': acc['mse'],
                    'mae': acc['mae'],
                    'rmse': acc['rmse'],
                    'correlation': acc['correlation']
                }
            
            if 'uncertainty' in batch_metrics:
                unc = batch_metrics['uncertainty']
                summary['batch_filtering'].update({
                    'mean_uncertainty': unc['mean_uncertainty'],
                    'coverage_95': unc.get('coverage_95', None)
                })
                
            summary['batch_filtering']['processing_time'] = filtering_results['batch_filtering']['processing_time']
        
        # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ä¸»è¦æŒ‡æ¨™
        if filtering_results.get('online_filtering', {}).get('success', False):
            online_metrics = filtering_results['online_filtering']['metrics']
            
            if 'accuracy' in online_metrics:
                acc = online_metrics['accuracy']
                summary['online_filtering'] = {
                    'mse': acc['mse'],
                    'mae': acc['mae'],
                    'rmse': acc['rmse'],
                    'correlation': acc['correlation']
                }
            
            summary['online_filtering']['total_time'] = filtering_results['online_filtering']['total_processing_time']
            summary['online_filtering']['avg_step_time'] = filtering_results['online_filtering']['average_step_time']
        
        # ä¸ç¢ºå®Ÿæ€§ä¸»è¦æŒ‡æ¨™
        if 'confidence_intervals' in uncertainty_results:
            ci_results = uncertainty_results['confidence_intervals']
            summary['uncertainty'] = {}
            for level in [68, 95]:
                key = f'confidence_{level}'
                if key in ci_results:
                    summary['uncertainty'][f'coverage_{level}'] = ci_results[key]['actual_coverage']
                    summary['uncertainty'][f'coverage_error_{level}'] = ci_results[key]['coverage_error']
        
        if 'calibration' in uncertainty_results:
            summary['uncertainty']['ece'] = uncertainty_results['calibration']['ece']
        
        return summary
    
    def _print_final_summary(self, results: dict):
        """æœ€çµ‚çµæœã‚µãƒãƒªã®å‡ºåŠ›"""
        print(f"\n" + "="*70)
        print(f"ğŸ“Š ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½è©•ä¾¡ æœ€çµ‚çµæœ")
        print(f"ğŸ·ï¸  å®Ÿé¨“å: {results['experiment_info']['name']}")
        print(f"â° å®Ÿè¡Œæ™‚åˆ»: {results['experiment_info']['timestamp']}")
        print("="*70)
        
        summary = results['summary_metrics']
        
        # ãƒãƒƒãƒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ
        if 'batch_filtering' in summary:
            batch = summary['batch_filtering']
            print(f"\nğŸ“Š ãƒãƒƒãƒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½:")
            print(f"   MSE:          {batch.get('mse', 'N/A'):.6f}")
            print(f"   MAE:          {batch.get('mae', 'N/A'):.6f}")
            print(f"   RMSE:         {batch.get('rmse', 'N/A'):.6f}")
            print(f"   ç›¸é–¢ä¿‚æ•°:     {batch.get('correlation', 'N/A'):.4f}")
            print(f"   å¹³å‡ä¸ç¢ºå®Ÿæ€§: {batch.get('mean_uncertainty', 'N/A'):.6f}")
            print(f"   95%ã‚«ãƒãƒ¬ãƒƒã‚¸:{batch.get('coverage_95', 'N/A'):.4f}")
            print(f"   å‡¦ç†æ™‚é–“:     {batch.get('processing_time', 'N/A'):.4f}ç§’")
        
        # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ
        if 'online_filtering' in summary:
            online = summary['online_filtering']
            print(f"\nğŸ“± ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½:")
            print(f"   MSE:          {online.get('mse', 'N/A'):.6f}")
            print(f"   MAE:          {online.get('mae', 'N/A'):.6f}")
            print(f"   RMSE:         {online.get('rmse', 'N/A'):.6f}")
            print(f"   ç›¸é–¢ä¿‚æ•°:     {online.get('correlation', 'N/A'):.4f}")
            print(f"   ç·å‡¦ç†æ™‚é–“:   {online.get('total_time', 'N/A'):.4f}ç§’")
            print(f"   å¹³å‡ã‚¹ãƒ†ãƒƒãƒ—æ™‚é–“: {online.get('avg_step_time', 'N/A'):.6f}ç§’")
        
        # ä¸ç¢ºå®Ÿæ€§çµæœ
        if 'uncertainty' in summary:
            unc = summary['uncertainty']
            print(f"\nğŸ² ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–å“è³ª:")
            print(f"   68%ã‚«ãƒãƒ¬ãƒƒã‚¸: {unc.get('coverage_68', 'N/A'):.4f}")
            print(f"   95%ã‚«ãƒãƒ¬ãƒƒã‚¸: {unc.get('coverage_95', 'N/A'):.4f}")
            print(f"   ECE:          {unc.get('ece', 'N/A'):.4f}")
        
        # æ‰‹æ³•æ¯”è¼ƒ
        if 'batch_filtering' in summary and 'online_filtering' in summary:
            batch = summary['batch_filtering']
            online = summary['online_filtering']
            
            print(f"\nğŸ” ãƒãƒƒãƒ vs ã‚ªãƒ³ãƒ©ã‚¤ãƒ³æ¯”è¼ƒ:")
            if 'mse' in batch and 'mse' in online:
                mse_diff = online['mse'] - batch['mse']
                print(f"   MSEå·®åˆ†:      {mse_diff:+.6f} (ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ - ãƒãƒƒãƒ)")
            
            if 'processing_time' in batch and 'total_time' in online:
                speed_ratio = online['total_time'] / batch['processing_time']
                print(f"   é€Ÿåº¦æ¯”:       {speed_ratio:.2f}x (ã‚ªãƒ³ãƒ©ã‚¤ãƒ³/ãƒãƒƒãƒ)")
        
        print("="*70)
        print(f"âœ… è©•ä¾¡å®Œäº†")
    
    def _export_results(self, results: dict, experiment_name: str):
        """çµæœã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSONå½¢å¼ã§è©³ç´°çµæœä¿å­˜
        json_path = self.output_dir / f"{experiment_name}_{timestamp}_complete_results.json"
        with open(json_path, 'w') as f:
            json.dump(self._make_json_serializable(results), f, indent=2)
        
        # ã‚µãƒãƒªã‚’CSVå½¢å¼ã§ä¿å­˜
        self._export_summary_csv(results, experiment_name, timestamp)
        
        print(f"\nğŸ“ çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†:")
        print(f"   è©³ç´°çµæœ: {json_path}")
        print(f"   å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
    
    def _export_summary_csv(self, results: dict, experiment_name: str, timestamp: str):
        """ã‚µãƒãƒªã‚’CSVå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        import csv
        
        csv_path = self.output_dir / f"{experiment_name}_{timestamp}_summary.csv"
        summary = results['summary_metrics']
        
        with open(csv_path, 'w', newline='') as f:
            writer = csv.writer(f)
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼
            writer.writerow([
                'experiment_name', 'method', 'mse', 'mae', 'rmse', 'correlation',
                'mean_uncertainty', 'coverage_95', 'processing_time', 'avg_step_time'
            ])
            
            # ãƒãƒƒãƒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if 'batch_filtering' in summary:
                batch = summary['batch_filtering']
                writer.writerow([
                    experiment_name, 'batch',
                    batch.get('mse', ''), batch.get('mae', ''), batch.get('rmse', ''),
                    batch.get('correlation', ''), batch.get('mean_uncertainty', ''),
                    batch.get('coverage_95', ''), batch.get('processing_time', ''), ''
                ])
            
            # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if 'online_filtering' in summary:
                online = summary['online_filtering']
                writer.writerow([
                    experiment_name, 'online',
                    online.get('mse', ''), online.get('mae', ''), online.get('rmse', ''),
                    online.get('correlation', ''), '', '', 
                    online.get('total_time', ''), online.get('avg_step_time', '')
                ])
        
        print(f"   ã‚µãƒãƒªCSV: {csv_path}")
    
    def _make_json_serializable(self, obj):
        """JSONå¯¾å¿œå½¢å¼ã«å¤‰æ›"""
        if isinstance(obj, dict):
            return {k: self._make_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_json_serializable(v) for v in obj]
        elif isinstance(obj, torch.Tensor):
            return obj.cpu().numpy().tolist()
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.floating, np.integer)):
            return float(obj)
        elif isinstance(obj, Path):
            return str(obj)
        else:
            return obj


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="DFIV Kalman Filter ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½è©•ä¾¡")
    
    parser.add_argument(
        '--model_path', 
        required=True,
        help='å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (.pth)'
    )
    parser.add_argument(
        '--data_path',
        required=True, 
        help='è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (.npz)'
    )
    parser.add_argument(
        '--output_dir',
        required=True,
        help='çµæœå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª'
    )
    parser.add_argument(
        '--config',
        default='configs/inference_config.yaml',
        help='æ¨è«–è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹'
    )
    parser.add_argument(
        '--experiment_name',
        default=None,
        help='å®Ÿé¨“åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šè‡ªå‹•ç”Ÿæˆï¼‰'
    )
    parser.add_argument(
        '--data_split',
        default='test',
        choices=['test', 'val', 'all'],
        help='ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿åˆ†å‰²'
    )
    parser.add_argument(
        '--device',
        default='auto',
        help='è¨ˆç®—ãƒ‡ãƒã‚¤ã‚¹ (auto, cpu, cuda)'
    )
    parser.add_argument(
        '--no_visualization',
        action='store_true',
        help='å¯è¦–åŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—'
    )
    parser.add_argument(
        '--quick',
        action='store_true',
        help='ã‚¯ã‚¤ãƒƒã‚¯è©•ä¾¡ï¼ˆè©³ç´°ä¿å­˜ãªã—ï¼‰'
    )
    
    args = parser.parse_args()
    
    # å¼•æ•°æ¤œè¨¼
    if not Path(args.model_path).exists():
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.model_path}")
        return
    
    if not Path(args.data_path).exists():
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.data_path}")
        return
    
    if not Path(args.config).exists():
        print(f"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.config}")
        return
    
    # è©•ä¾¡å®Ÿè¡Œ
    try:
        evaluator = FilteringPerformanceEvaluator(
            model_path=args.model_path,
            config_path=args.config,
            output_dir=args.output_dir,
            device=args.device
        )
        
        results = evaluator.evaluate_comprehensive(
            data_path=args.data_path,
            experiment_name=args.experiment_name,
            data_split=args.data_split,
            save_detailed_results=not args.quick,
            create_visualizations=not args.no_visualization
        )
        
        print(f"\nğŸ‰ ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½è©•ä¾¡å®Œäº†ï¼")
        
    except Exception as e:
        print(f"\nâŒ è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        raise


if __name__ == "__main__":
    main()