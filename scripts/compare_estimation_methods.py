#!/usr/bin/env python3
"""
æ¨å®šæ‰‹æ³•æ¯”è¼ƒã‚¹ã‚¯ãƒªãƒ—ãƒˆ

DFIVå†…ã®æ±ºå®šçš„å®Ÿç¾ã¨Kalmanå®Ÿç¾ã‚’æ¯”è¼ƒã—ã€
Kalman Filteringã®åŠ¹æœã‚’å®šé‡çš„ã«è©•ä¾¡ã™ã‚‹ã€‚

Usage:
    python scripts/compare_estimation_methods.py \
        --model_path results/trained_model.pth \
        --data_path data/test_data.npz \
        --output_dir results/method_comparison \
        --config configs/inference_config.yaml
"""

import sys
import argparse
import torch
import numpy as np
from pathlib import Path
from datetime import datetime
import json
import csv
from typing import Dict, List, Any

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

from src.models.inference_model import InferenceModel
from src.training.two_stage_trainer import TwoStageTrainer
from src.evaluation.metrics import StateEstimationMetrics, print_comparison_summary
from src.utils.data_loader import load_experimental_data


class EstimationMethodComparator:
    """æ¨å®šæ‰‹æ³•æ¯”è¼ƒã‚¯ãƒ©ã‚¹"""
    
    def __init__(
        self, 
        model_path: str, 
        config_path: str, 
        output_dir: str,
        device: str = 'auto'
    ):
        self.model_path = Path(model_path)
        self.config_path = Path(config_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        if device == 'auto':
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        else:
            self.device = device
            
        print(f"ğŸ–¥ï¸  ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {self.device}")
        
        # è©•ä¾¡å™¨åˆæœŸåŒ–
        self.metrics_evaluator = StateEstimationMetrics(device=self.device)
        
        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãƒ»æº–å‚™
        self.models = {}
        self._prepare_models()
        
    def _prepare_models(self):
        """æ¯”è¼ƒç”¨ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™"""
        print(f"\nğŸ“‚ æ¯”è¼ƒç”¨ãƒ¢ãƒ‡ãƒ«æº–å‚™ä¸­...")
        
        try:
            # 1. Kalmanæ¨è«–ãƒ¢ãƒ‡ãƒ«
            print("  ğŸ“Š Kalmanæ¨è«–ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿...")
            self.models['kalman'] = InferenceModel(
                str(self.model_path), str(self.config_path)
            )
            
            # 2. æ±ºå®šçš„æ¨è«–ç”¨ã«ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚‚èª­ã¿è¾¼ã¿ï¼ˆæ±ºå®šçš„å®Ÿç¾ç”¨ï¼‰
            print("  ğŸ“ˆ æ±ºå®šçš„æ¨è«–ç”¨ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼èª­ã¿è¾¼ã¿...")

            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰æ§‹é€ ã‚’æ¤œå‡ºã—ã¦åˆæœŸåŒ–ï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä¸è¦ï¼‰
            self.deterministic_trainer = TwoStageTrainer.from_trained_model(
                str(self.model_path),
                device=self.device,
                output_dir=str(self.output_dir / 'temp')
            )
            
            print("âœ… ãƒ¢ãƒ‡ãƒ«æº–å‚™å®Œäº†")
            
        except Exception as e:
            print(f"âŒ ãƒ¢ãƒ‡ãƒ«æº–å‚™ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    
    def compare_methods(
        self,
        data_path: str,
        experiment_name: str = None,
        data_split: str = 'test',
        save_results: bool = True
    ) -> Dict[str, Any]:
        """
        æ¨å®šæ‰‹æ³•ã®åŒ…æ‹¬çš„æ¯”è¼ƒ
        
        Args:
            data_path: è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹
            experiment_name: å®Ÿé¨“å
            data_split: ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
            save_results: çµæœä¿å­˜ã™ã‚‹ã‹
            
        Returns:
            Dict: æ¯”è¼ƒçµæœ
        """
        if experiment_name is None:
            experiment_name = f"method_comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
        print(f"\nğŸ” æ¨å®šæ‰‹æ³•æ¯”è¼ƒé–‹å§‹")
        print(f"ğŸ“Š å®Ÿé¨“å: {experiment_name}")
        print(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿: {data_path}")
        print("="*70)
        
        # 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        comparison_data = self._load_comparison_data(data_path, data_split)
        test_data = comparison_data['observations']
        true_states = comparison_data.get('true_states', None)
        
        print(f"ğŸ“ è©•ä¾¡ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {test_data.shape}")
        
        # 2. å„æ‰‹æ³•ã§æ¨å®šå®Ÿè¡Œ
        method_results = {}
        
        # 2.1 Kalmanæ¨å®š
        kalman_results = self._run_kalman_estimation(test_data, true_states)
        method_results['kalman'] = kalman_results
        
        # 2.2 æ±ºå®šçš„æ¨å®š
        deterministic_results = self._run_deterministic_estimation(test_data, true_states)
        method_results['deterministic'] = deterministic_results
        
        # 3. æ‰‹æ³•æ¯”è¼ƒåˆ†æ
        comparison_analysis = self._analyze_method_comparison(method_results, true_states)
        
        # 4. çµæœçµ±åˆ
        complete_comparison = {
            'experiment_info': {
                'name': experiment_name,
                'timestamp': datetime.now().isoformat(),
                'data_path': data_path,
                'data_split': data_split,
                'data_shape': list(test_data.shape),
                'has_true_states': true_states is not None
            },
            'method_results': method_results,
            'comparison_analysis': comparison_analysis,
            'summary': self._create_comparison_summary(method_results, comparison_analysis)
        }
        
        # 5. çµæœå‡ºåŠ›
        self._print_comparison_results(complete_comparison)
        
        # 6. çµæœä¿å­˜
        if save_results:
            self._save_comparison_results(complete_comparison, experiment_name)
        
        return complete_comparison
    
    def _load_comparison_data(self, data_path: str, data_split: str) -> dict:
        """æ¯”è¼ƒç”¨ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        print(f"\nğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
        
        try:
            data_dict = load_experimental_data(data_path)
            
            # æŒ‡å®šã•ã‚ŒãŸåˆ†å‰²ã‚’å–å¾—
            if data_split in data_dict:
                observations = data_dict[data_split]
            else:
                observations = data_dict[list(data_dict.keys())[0]]
                print(f"âš ï¸  æŒ‡å®šåˆ†å‰² '{data_split}' ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã€‚'{list(data_dict.keys())[0]}' ã‚’ä½¿ç”¨ã€‚")
            
            observations = observations.to(self.device)
            
            # çœŸå€¤çŠ¶æ…‹
            true_states = None
            if 'true_states' in data_dict:
                true_states = data_dict['true_states'].to(self.device)
            elif f'{data_split}_states' in data_dict:
                true_states = data_dict[f'{data_split}_states'].to(self.device)
                
            print(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†")
            return {
                'observations': observations,
                'true_states': true_states,
                'metadata': {'available_keys': list(data_dict.keys())}
            }
            
        except Exception as e:
            print(f"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _run_kalman_estimation(self, test_data: torch.Tensor, true_states: torch.Tensor) -> dict:
        """Kalmanæ¨å®šã®å®Ÿè¡Œ"""
        print(f"\nğŸ² Kalmanæ¨å®šå®Ÿè¡Œä¸­...")
        
        try:
            # æ¨è«–ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
            # past_horizonã‚’è€ƒæ…®ã—ãŸå®‰å…¨ãªã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚µã‚¤ã‚ºè¨ˆç®—
            past_horizon = 10  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆè¨­å®šã‹ã‚‰å–å¾—ã™ã¹ãã ãŒä¸€æ™‚çš„ã«å›ºå®šï¼‰
            min_required = 2 * past_horizon + 1
            total_samples = test_data.size(0)

            print(f"ğŸ” Stage 2 - ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æ:")
            print(f"   è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ç·æ•°: {total_samples}")
            print(f"   past_horizon: {past_horizon}")
            print(f"   å¿…è¦æœ€å°ã‚µãƒ³ãƒ—ãƒ«: {min_required}")

            if total_samples >= min_required:
                calibration_size = min(50, max(min_required, total_samples // 4))
                print(f"âœ… ååˆ†ãªãƒ‡ãƒ¼ã‚¿: ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³{calibration_size}ã‚µãƒ³ãƒ—ãƒ«ä½¿ç”¨")
            else:
                calibration_size = total_samples
                print(f"âŒ ãƒ‡ãƒ¼ã‚¿ä¸è¶³: å…¨{total_samples}ã‚µãƒ³ãƒ—ãƒ«ä½¿ç”¨ã€æ•°å€¤ä¸å®‰å®šã®å¯èƒ½æ€§")

            calibration_data = test_data[:calibration_size]
            
            self.models['kalman'].setup_inference(
                calibration_data=calibration_data,
                method='data_driven'
            )
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
            start_time = datetime.now()
            filtering_result = self.models['kalman'].filter_sequence(
                test_data, return_likelihood=True
            )
            
            if len(filtering_result) == 3:
                X_means, X_covariances, likelihoods = filtering_result
            else:
                X_means, X_covariances = filtering_result
                likelihoods = None
                
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()
            
            # æ€§èƒ½è©•ä¾¡
            metrics = self.metrics_evaluator.compute_all_metrics(
                X_means, true_states, X_covariances, test_data, likelihoods, verbose=False
            )
            
            print(f"  âœ… Kalmanæ¨å®šå®Œäº† ({processing_time:.4f}ç§’)")
            print(f"  ğŸ“ æ¨å®šçŠ¶æ…‹å½¢çŠ¶: {X_means.shape}")
            
            return {
                'success': True,
                'method_name': 'Kalman Filtering',
                'estimated_states': X_means,
                'covariances': X_covariances,
                'likelihoods': likelihoods,
                'processing_time': processing_time,
                'metrics': metrics,
                'has_uncertainty': True
            }
            
        except Exception as e:
            import traceback
            error_details = {
                'error_message': str(e),
                'error_type': type(e).__name__,
                'full_traceback': traceback.format_exc(),
                'model_type': type(self.models['kalman']).__name__,
                'available_methods': [m for m in dir(self.models['kalman']) if not m.startswith('_')],
                'filter_methods': [m for m in dir(self.models['kalman']) if not m.startswith('_') and 'filter' in m.lower()],
                'test_data_shape': list(test_data.shape),
                'model_setup_status': getattr(self.models['kalman'], 'is_setup', 'unknown')
            }

            print(f"  âŒ Kalmanæ¨å®šã‚¨ãƒ©ãƒ¼: {e}")
            print(f"  ğŸ” ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}")
            print(f"  ğŸ“Š ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {test_data.shape}")
            print(f"  ğŸ¯ ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—: {type(self.models['kalman']).__name__}")
            print(f"  ğŸ”§ åˆ©ç”¨å¯èƒ½ãªfilterãƒ¡ã‚½ãƒƒãƒ‰: {error_details['filter_methods']}")
            print(f"  âš™ï¸  ãƒ¢ãƒ‡ãƒ«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—çŠ¶æ³: {error_details['model_setup_status']}")
            print(f"  ğŸ“ è©³ç´°ãƒˆãƒ¬ãƒ¼ã‚¹:\n{traceback.format_exc()}")

            return {
                'success': False,
                'method_name': 'Kalman Filtering',
                'error': str(e),
                'error_details': error_details
            }
    
    def _run_deterministic_estimation(self, test_data: torch.Tensor, true_states: torch.Tensor) -> dict:
        """æ±ºå®šçš„æ¨å®šã®å®Ÿè¡Œ"""
        print(f"\nğŸ“ˆ æ±ºå®šçš„æ¨å®šå®Ÿè¡Œä¸­...")
        
        try:
            # æ±ºå®šçš„å®Ÿç¾ã‚’ä½¿ç”¨ã—ãŸçŠ¶æ…‹æ¨å®š
            start_time = datetime.now()
            
            # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
            with torch.no_grad():
                self.deterministic_trainer.encoder.eval()
                encoded = self.deterministic_trainer.encoder(test_data.unsqueeze(0)).squeeze(0)
                
                # å®Ÿç¾åŒ–ã«ã‚ˆã‚‹çŠ¶æ…‹æ¨å®šï¼ˆå½¢çŠ¶èª¿æ•´ç‰ˆï¼‰
                # realizationç”¨ã®2æ¬¡å…ƒå½¢çŠ¶èª¿æ•´: [T, feature_dim] â†’ [T, d]
                if hasattr(self.deterministic_trainer, 'realization'):
                    # encodedãŒ[T, feature_dim]ã®å ´åˆã€é©åˆ‡ã«2æ¬¡å…ƒã«èª¿æ•´
                    if encoded.dim() == 2:
                        if encoded.shape[1] == 1:
                            encoded_2d = encoded  # [T, 1] â† æ—¢ã«æ­£ã—ã„
                        else:
                            # feature_dimãŒè¤‡æ•°ã®å ´åˆã€1æ¬¡å…ƒã«èª¿æ•´ï¼ˆæœ€åˆã®ç‰¹å¾´é‡ã‚’ä½¿ç”¨ï¼‰
                            encoded_2d = encoded[:, :1]  # [T, 1]
                    elif encoded.dim() == 1:
                        encoded_2d = encoded.unsqueeze(1)  # [T, 1]
                    else:
                        raise ValueError(f"Unexpected encoded dimension: {encoded.dim()}, shape: {encoded.shape}")

                    self.deterministic_trainer.realization.fit(encoded_2d)
                    X_estimated = self.deterministic_trainer.realization.filter(encoded_2d)
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰çµæœã‚’ãã®ã¾ã¾çŠ¶æ…‹ã¨ã—ã¦ä½¿ç”¨
                    X_estimated = encoded
                    
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()
            
            # æ€§èƒ½è©•ä¾¡
            metrics = self.metrics_evaluator.compute_all_metrics(
                X_estimated, true_states, None, test_data, None, verbose=False
            )
            
            print(f"  âœ… æ±ºå®šçš„æ¨å®šå®Œäº† ({processing_time:.4f}ç§’)")
            print(f"  ğŸ“ æ¨å®šçŠ¶æ…‹å½¢çŠ¶: {X_estimated.shape}")
            
            return {
                'success': True,
                'method_name': 'Deterministic Realization',
                'estimated_states': X_estimated,
                'covariances': None,
                'likelihoods': None,
                'processing_time': processing_time,
                'metrics': metrics,
                'has_uncertainty': False
            }
            
        except Exception as e:
            import traceback
            error_details = {
                'error_message': str(e),
                'error_type': type(e).__name__,
                'full_traceback': traceback.format_exc(),
                'trainer_type': type(self.deterministic_trainer).__name__,
                'available_methods': [m for m in dir(self.deterministic_trainer) if not m.startswith('_')],
                'realization_methods': [m for m in dir(self.deterministic_trainer) if not m.startswith('_') and 'realization' in m.lower()],
                'test_data_shape': list(test_data.shape),
                'has_realization': hasattr(self.deterministic_trainer, 'realization')
            }

            print(f"  âŒ æ±ºå®šçš„æ¨å®šã‚¨ãƒ©ãƒ¼: {e}")
            print(f"  ğŸ” ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}")
            print(f"  ğŸ“Š ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {test_data.shape}")
            print(f"  ğŸ¯ ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚¿ã‚¤ãƒ—: {type(self.deterministic_trainer).__name__}")
            print(f"  ğŸ”§ åˆ©ç”¨å¯èƒ½ãªrealizationãƒ¡ã‚½ãƒƒãƒ‰: {error_details['realization_methods']}")
            print(f"  âš™ï¸  realizationå­˜åœ¨: {error_details['has_realization']}")
            print(f"  ğŸ“ è©³ç´°ãƒˆãƒ¬ãƒ¼ã‚¹:\n{traceback.format_exc()}")

            return {
                'success': False,
                'method_name': 'Deterministic Realization',
                'error': str(e),
                'error_details': error_details
            }
    
    def _analyze_method_comparison(self, method_results: dict, true_states: torch.Tensor) -> dict:
        """æ‰‹æ³•æ¯”è¼ƒã®è©³ç´°åˆ†æ"""
        print(f"\nğŸ” æ‰‹æ³•æ¯”è¼ƒåˆ†æä¸­...")
        
        analysis = {
            'methods_compared': list(method_results.keys()),
            'comparison_available': True
        }
        
        # æˆåŠŸã—ãŸæ‰‹æ³•ã®ã¿ã‚’æ¯”è¼ƒ
        successful_methods = {k: v for k, v in method_results.items() if v.get('success', False)}
        
        if len(successful_methods) < 2:
            analysis['comparison_available'] = False
            analysis['error'] = 'Not enough successful methods for comparison'
            return analysis
        
        # ç²¾åº¦æ¯”è¼ƒ
        if all('accuracy' in method['metrics'] for method in successful_methods.values()):
            analysis['accuracy_comparison'] = self._compare_accuracy_metrics(successful_methods)
        
        # è¨ˆç®—åŠ¹ç‡æ¯”è¼ƒ
        analysis['efficiency_comparison'] = self._compare_efficiency_metrics(successful_methods)
        
        # Kalmanç‰¹æœ‰ã®åˆ†æï¼ˆä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–ï¼‰
        if 'kalman' in successful_methods and successful_methods['kalman']['has_uncertainty']:
            analysis['uncertainty_analysis'] = self._analyze_kalman_uncertainty(
                successful_methods['kalman'], true_states
            )
        
        return analysis
    
    def _compare_accuracy_metrics(self, successful_methods: dict) -> dict:
        """ç²¾åº¦æŒ‡æ¨™ã®æ¯”è¼ƒ"""
        accuracy_comparison = {}
        
        # å„æŒ‡æ¨™ã‚’æ¯”è¼ƒ
        metrics_to_compare = ['mse', 'mae', 'rmse', 'correlation']
        
        for metric in metrics_to_compare:
            metric_values = {}
            for method_name, method_result in successful_methods.items():
                if metric in method_result['metrics']['accuracy']:
                    metric_values[method_name] = method_result['metrics']['accuracy'][metric]
            
            if len(metric_values) >= 2:
                # æœ€è‰¯ãƒ»æœ€æ‚ªå€¤
                best_method = min(metric_values, key=metric_values.get) if metric != 'correlation' else max(metric_values, key=metric_values.get)
                worst_method = max(metric_values, key=metric_values.get) if metric != 'correlation' else min(metric_values, key=metric_values.get)
                
                accuracy_comparison[metric] = {
                    'values': metric_values,
                    'best_method': best_method,
                    'worst_method': worst_method,
                    'improvement': self._calculate_improvement(metric_values, metric)
                }
        
        return accuracy_comparison
    
    def _compare_efficiency_metrics(self, successful_methods: dict) -> dict:
        """è¨ˆç®—åŠ¹ç‡ã®æ¯”è¼ƒ"""
        efficiency_comparison = {}
        
        processing_times = {}
        for method_name, method_result in successful_methods.items():
            processing_times[method_name] = method_result['processing_time']
        
        if len(processing_times) >= 2:
            fastest_method = min(processing_times, key=processing_times.get)
            slowest_method = max(processing_times, key=processing_times.get)
            
            efficiency_comparison['processing_time'] = {
                'values': processing_times,
                'fastest_method': fastest_method,
                'slowest_method': slowest_method,
                'speed_ratio': max(processing_times.values()) / min(processing_times.values())
            }
        
        return efficiency_comparison
    
    def _analyze_kalman_uncertainty(self, kalman_result: dict, true_states: torch.Tensor) -> dict:
        """Kalmanæ‰‹æ³•ã®ä¸ç¢ºå®Ÿæ€§åˆ†æ"""
        if not kalman_result['has_uncertainty'] or true_states is None:
            return {'analysis_available': False}
        
        uncertainty_analysis = {}
        
        # ä¸ç¢ºå®Ÿæ€§ã®åŸºæœ¬çµ±è¨ˆ
        covariances = kalman_result['covariances']
        uncertainties = torch.sqrt(torch.diagonal(covariances, dim1=1, dim2=2))
        
        uncertainty_analysis['basic_stats'] = {
            'mean_uncertainty': uncertainties.mean().item(),
            'std_uncertainty': uncertainties.std().item(),
            'min_uncertainty': uncertainties.min().item(),
            'max_uncertainty': uncertainties.max().item()
        }
        
        # ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        if 'uncertainty' in kalman_result['metrics']:
            unc_metrics = kalman_result['metrics']['uncertainty']
            uncertainty_analysis['coverage_rates'] = {
                key: value for key, value in unc_metrics.items() 
                if key.startswith('coverage_')
            }
        
        return uncertainty_analysis
    
    def _calculate_improvement(self, metric_values: dict, metric_name: str) -> dict:
        """æ”¹å–„ç‡ã®è¨ˆç®—"""
        if len(metric_values) != 2:
            return {}
        
        methods = list(metric_values.keys())
        if 'kalman' in methods and 'deterministic' in methods:
            kalman_value = metric_values['kalman']
            det_value = metric_values['deterministic']
            
            if metric_name == 'correlation':
                # ç›¸é–¢ã¯é«˜ã„æ–¹ãŒè‰¯ã„
                improvement = (kalman_value - det_value) / abs(det_value) * 100
            else:
                # MSE, MAE, RMSEã¯ä½ã„æ–¹ãŒè‰¯ã„
                improvement = (det_value - kalman_value) / det_value * 100
            
            return {
                'kalman_vs_deterministic': improvement,
                'interpretation': 'positive means Kalman is better'
            }
        
        return {}
    
    def _create_comparison_summary(self, method_results: dict, comparison_analysis: dict) -> dict:
        """æ¯”è¼ƒã‚µãƒãƒªã®ä½œæˆ"""
        summary = {}
        
        # å„æ‰‹æ³•ã®ä¸»è¦æŒ‡æ¨™
        for method_name, method_result in method_results.items():
            if method_result.get('success', False) and 'accuracy' in method_result['metrics']:
                acc = method_result['metrics']['accuracy']
                summary[method_name] = {
                    'mse': acc['mse'],
                    'mae': acc['mae'],
                    'rmse': acc['rmse'],
                    'correlation': acc['correlation'],
                    'processing_time': method_result['processing_time'],
                    'has_uncertainty': method_result['has_uncertainty']
                }
        
        # æ¯”è¼ƒçµæœã‚µãƒãƒª
        if comparison_analysis.get('comparison_available', False):
            summary['comparison_summary'] = {}
            
            if 'accuracy_comparison' in comparison_analysis:
                acc_comp = comparison_analysis['accuracy_comparison']
                summary['comparison_summary']['accuracy'] = {
                    metric: {
                        'best_method': result['best_method'],
                        'improvement': result.get('improvement', {})
                    }
                    for metric, result in acc_comp.items()
                }
            
            if 'efficiency_comparison' in comparison_analysis:
                eff_comp = comparison_analysis['efficiency_comparison']
                if 'processing_time' in eff_comp:
                    summary['comparison_summary']['efficiency'] = {
                        'fastest_method': eff_comp['processing_time']['fastest_method'],
                        'speed_ratio': eff_comp['processing_time']['speed_ratio']
                    }
        
        return summary
    
    def _print_comparison_results(self, comparison: dict):
        """æ¯”è¼ƒçµæœã®å‡ºåŠ›"""
        print(f"\n" + "="*70)
        print(f"ğŸ” æ¨å®šæ‰‹æ³•æ¯”è¼ƒçµæœ")
        print(f"ğŸ·ï¸  å®Ÿé¨“å: {comparison['experiment_info']['name']}")
        print("="*70)
        
        summary = comparison['summary']
        
        # å„æ‰‹æ³•ã®çµæœ
        print(f"\nğŸ“Š æ‰‹æ³•åˆ¥æ€§èƒ½:")
        for method_name, method_summary in summary.items():
            if method_name != 'comparison_summary':
                print(f"\n  ğŸ¯ {method_name}:")
                print(f"     MSE:          {method_summary['mse']:.6f}")
                print(f"     MAE:          {method_summary['mae']:.6f}")
                print(f"     RMSE:         {method_summary['rmse']:.6f}")
                print(f"     ç›¸é–¢ä¿‚æ•°:     {method_summary['correlation']:.4f}")
                print(f"     å‡¦ç†æ™‚é–“:     {method_summary['processing_time']:.4f}ç§’")
                print(f"     ä¸ç¢ºå®Ÿæ€§:     {'ã‚ã‚Š' if method_summary['has_uncertainty'] else 'ãªã—'}")
        
        # æ¯”è¼ƒã‚µãƒãƒª
        if 'comparison_summary' in summary:
            comp_summary = summary['comparison_summary']
            print(f"\nğŸ” æ‰‹æ³•æ¯”è¼ƒã‚µãƒãƒª:")
            
            if 'accuracy' in comp_summary:
                print(f"  ğŸ“ˆ ç²¾åº¦æ¯”è¼ƒ:")
                for metric, result in comp_summary['accuracy'].items():
                    best_method = result['best_method']
                    print(f"     {metric.upper()}: {best_method} ãŒæœ€è‰¯")
                    
                    # æ”¹å–„ç‡è¡¨ç¤º
                    if 'improvement' in result and 'kalman_vs_deterministic' in result['improvement']:
                        improvement = result['improvement']['kalman_vs_deterministic']
                        print(f"              Kalmanæ”¹å–„ç‡: {improvement:+.2f}%")
            
            if 'efficiency' in comp_summary:
                eff = comp_summary['efficiency']
                print(f"  âš¡ åŠ¹ç‡æ¯”è¼ƒ:")
                print(f"     æœ€é«˜é€Ÿ: {eff['fastest_method']}")
                print(f"     é€Ÿåº¦æ¯”: {eff['speed_ratio']:.2f}x")
        
        # Kalmanç‰¹æœ‰ã®åˆ†æ
        if 'uncertainty_analysis' in comparison['comparison_analysis']:
            unc_analysis = comparison['comparison_analysis']['uncertainty_analysis']
            print(f"\nğŸ² Kalmanä¸ç¢ºå®Ÿæ€§åˆ†æ:")
            
            if 'basic_stats' in unc_analysis:
                stats = unc_analysis['basic_stats']
                print(f"     å¹³å‡ä¸ç¢ºå®Ÿæ€§: {stats['mean_uncertainty']:.6f}")
                print(f"     ä¸ç¢ºå®Ÿæ€§ç¯„å›²: [{stats['min_uncertainty']:.6f}, {stats['max_uncertainty']:.6f}]")
            
            if 'coverage_rates' in unc_analysis:
                coverage = unc_analysis['coverage_rates']
                for level, rate in coverage.items():
                    if isinstance(rate, (int, float)):
                        level_num = level.split('_')[1] if '_' in level else level
                        print(f"     {level_num}ã‚«ãƒãƒ¬ãƒƒã‚¸: {rate:.4f}")
        
        print("="*70)
        print(f"âœ… æ¯”è¼ƒå®Œäº†")
    
    def _save_comparison_results(self, comparison: dict, experiment_name: str):
        """æ¯”è¼ƒçµæœã®ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSONè©³ç´°çµæœ
        json_path = self.output_dir / f"{experiment_name}_{timestamp}_comparison.json"
        with open(json_path, 'w') as f:
            json.dump(self._make_json_serializable(comparison), f, indent=2)
        
        # CSV ã‚µãƒãƒª
        self._save_comparison_csv(comparison, experiment_name, timestamp)
        
        print(f"\nğŸ“ æ¯”è¼ƒçµæœä¿å­˜å®Œäº†:")
        print(f"   è©³ç´°çµæœ: {json_path}")
        print(f"   å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
    
    def _save_comparison_csv(self, comparison: dict, experiment_name: str, timestamp: str):
        """æ¯”è¼ƒçµæœã‚’CSVå½¢å¼ã§ä¿å­˜"""
        csv_path = self.output_dir / f"{experiment_name}_{timestamp}_comparison.csv"
        summary = comparison['summary']
        
        with open(csv_path, 'w', newline='') as f:
            writer = csv.writer(f)
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼
            writer.writerow([
                'experiment_name', 'method', 'mse', 'mae', 'rmse', 'correlation',
                'processing_time', 'has_uncertainty'
            ])
            
            # å„æ‰‹æ³•ã®çµæœ
            for method_name, method_summary in summary.items():
                if method_name != 'comparison_summary':
                    writer.writerow([
                        experiment_name, method_name,
                        method_summary['mse'], method_summary['mae'], 
                        method_summary['rmse'], method_summary['correlation'],
                        method_summary['processing_time'], method_summary['has_uncertainty']
                    ])
        
        print(f"   æ¯”è¼ƒCSV: {csv_path}")
    
    def _make_json_serializable(self, obj):
        """JSONå¯¾å¿œå½¢å¼ã«å¤‰æ›"""
        if isinstance(obj, dict):
            return {k: self._make_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_json_serializable(v) for v in obj]
        elif isinstance(obj, torch.Tensor):
            return obj.cpu().numpy().tolist()
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.floating, np.integer)):
            return float(obj)
        elif isinstance(obj, Path):
            return str(obj)
        else:
            return obj


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="DFIVæ¨å®šæ‰‹æ³•æ¯”è¼ƒ")
    
    parser.add_argument('--model_path', required=True, help='å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--data_path', required=True, help='è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹')
    parser.add_argument('--output_dir', required=True, help='çµæœå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--config', default='configs/inference_config.yaml', help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«')
    parser.add_argument('--experiment_name', default=None, help='å®Ÿé¨“å')
    parser.add_argument('--data_split', default='test', choices=['test', 'val', 'all'], help='ãƒ‡ãƒ¼ã‚¿åˆ†å‰²')
    parser.add_argument('--device', default='auto', help='è¨ˆç®—ãƒ‡ãƒã‚¤ã‚¹')
    parser.add_argument('--no_save', action='store_true', help='çµæœä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—')
    
    args = parser.parse_args()
    
    # å¼•æ•°æ¤œè¨¼
    for path_arg, path_value in [('model_path', args.model_path), ('data_path', args.data_path), ('config', args.config)]:
        if not Path(path_value).exists():
            print(f"âŒ {path_arg}ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {path_value}")
            return
    
    try:
        # æ¯”è¼ƒå®Ÿè¡Œ
        comparator = EstimationMethodComparator(
            model_path=args.model_path,
            config_path=args.config,
            output_dir=args.output_dir,
            device=args.device
        )
        
        results = comparator.compare_methods(
            data_path=args.data_path,
            experiment_name=args.experiment_name,
            data_split=args.data_split,
            save_results=not args.no_save
        )

        print(f"\nğŸ‰ æ¨å®šæ‰‹æ³•æ¯”è¼ƒå®Œäº†ï¼")
        print(f"ğŸ“Š æ¯”è¼ƒçµæœ: {len(results.get('method_results', {}))}å€‹ã®æ‰‹æ³•ã‚’æ¯”è¼ƒ")

        # ç°¡æ½”ãªã‚µãƒãƒªãƒ¼è¡¨ç¤º
        if 'method_results' in results:
            for method_name, result in results['method_results'].items():
                status = "âœ… æˆåŠŸ" if result.get('success', False) else "âŒ å¤±æ•—"
                print(f"  â€¢ {method_name}: {status}")
                if not result.get('success', False) and 'error' in result:
                    print(f"    ã‚¨ãƒ©ãƒ¼: {result['error'][:100]}...")
        
    except Exception as e:
        print(f"\nâŒ æ¯”è¼ƒä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        raise


if __name__ == "__main__":
    main()