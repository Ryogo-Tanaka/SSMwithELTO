#!/usr/bin/env python3
"""
ã‚¿ã‚¹ã‚¯4çµ±åˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ

DFIV Kalman Filterã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çŠ¶æ…‹æ¨å®šãƒ»è©•ä¾¡ã‚’çµ±åˆå®Ÿè¡Œã€‚
- ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½è©•ä¾¡
- æ¨å®šæ‰‹æ³•æ¯”è¼ƒ (Kalman vs æ±ºå®šçš„)
- ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–è©•ä¾¡
- çµæœã®å®Œå…¨å‡ºåŠ›ãƒ»ä¿å­˜

Usage:
    # åŸºæœ¬å®Ÿè¡Œ
    python scripts/run_filtering_evaluation.py \
        --model results/trained_model.pth \
        --data data/test.npz \
        --output results/task4_evaluation

    # åŒ…æ‹¬çš„è©•ä¾¡
    python scripts/run_filtering_evaluation.py \
        --model results/trained_model.pth \
        --data data/test.npz \
        --output results/comprehensive_eval \
        --config configs/evaluation_config.yaml \
        --mode comprehensive

    # ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ
    python scripts/run_filtering_evaluation.py \
        --model results/trained_model.pth \
        --data data/test.npz \
        --output results/quick_test \
        --mode quick
"""

import sys
import argparse
import yaml
import json
import traceback
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®š
sys.path.append(str(Path(__file__).parent.parent))

# è©•ä¾¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from scripts.evaluate_filtering_performance import FilteringPerformanceEvaluator
from scripts.compare_estimation_methods import EstimationMethodComparator


class Task4EvaluationPipeline:
    """ã‚¿ã‚¹ã‚¯4çµ±åˆè©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(
        self, 
        model_path: str, 
        config_path: str,
        output_dir: str,
        device: str = 'auto'
    ):
        self.model_path = Path(model_path)
        self.config_path = Path(config_path)
        self.output_dir = Path(output_dir)
        self.device = device
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ä½œæˆ
        self._setup_output_structure()
        
        # è¨­å®šèª­ã¿è¾¼ã¿
        self.config = self._load_config()
        
        # å®Ÿé¨“ãƒ­ã‚°åˆæœŸåŒ–
        self.experiment_log = {
            'start_time': datetime.now().isoformat(),
            'model_path': str(self.model_path),
            'config_path': str(self.config_path),
            'output_dir': str(self.output_dir),
            'device': self.device,
            'stages': []
        }
        
    def _setup_output_structure(self):
        """å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã®ä½œæˆ"""
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        subdirs = [
            'filtering_performance',    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½çµæœ
            'method_comparison',        # æ‰‹æ³•æ¯”è¼ƒçµæœ
            'uncertainty_analysis',     # ä¸ç¢ºå®Ÿæ€§åˆ†æçµæœ
            'summary',                  # çµ±åˆã‚µãƒãƒª
            'visualizations',           # å¯è¦–åŒ–çµæœ
            'raw_data'                  # ç”Ÿãƒ‡ãƒ¼ã‚¿å‡ºåŠ›
        ]
        
        for subdir in subdirs:
            (self.output_dir / subdir).mkdir(exist_ok=True)
    
    def _load_config(self, mode: str = 'standard') -> Dict[str, Any]:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ - è¤‡æ•°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå¯¾å¿œ"""
        try:
            with open(self.config_path, 'r') as f:
                # è¤‡æ•°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å…¨ã¦èª­ã¿è¾¼ã¿
                documents = list(yaml.safe_load_all(f))
            
            # ãƒ¡ã‚¤ãƒ³è¨­å®šã‹ã‚‰æ¨è«–è¨­å®šã‚’å–å¾—
            main_config = documents[0] if documents else {}
            inference_config = main_config.get('inference', {})
            
            # modeåˆ¥ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆé¸æŠ
            if mode == 'quick':
                # 2ã¤ç›®ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: quick_test_evaluation
                if len(documents) >= 2 and 'quick_test_evaluation' in documents[1]:
                    config = documents[1]['quick_test_evaluation']
                    print(f"ğŸ“ ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆè¨­å®šã‚’ä½¿ç”¨")
                else:
                    print(f"âš ï¸  ã‚¯ã‚¤ãƒƒã‚¯è¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨")
                    config = documents[0] if documents else self._get_default_config()
                    
            elif mode == 'comprehensive':
                # 3ã¤ç›®ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: comprehensive_evaluation
                if len(documents) >= 3 and 'comprehensive_evaluation' in documents[2]:
                    config = documents[2]['comprehensive_evaluation']
                    print(f"ğŸ“ åŒ…æ‹¬çš„è©•ä¾¡è¨­å®šã‚’ä½¿ç”¨")
                else:
                    print(f"âš ï¸  åŒ…æ‹¬çš„è¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨")
                    config = documents[0] if documents else self._get_default_config()
                    
            else:  # mode == 'standard'
                # 1ã¤ç›®ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
                config = documents[0] if documents else self._get_default_config()
                print(f"ğŸ“ æ¨™æº–è¨­å®šã‚’ä½¿ç”¨")
            
            # æ¨è«–è¨­å®šã‚’ãƒãƒ¼ã‚¸
            if inference_config and 'inference' not in config:
                config['inference'] = inference_config
                
            return config
            
        except Exception as e:
            print(f"âš ï¸  è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            print("ğŸ“ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã®å–å¾—"""
        return {
            'evaluation': {
                'experiment_name': f'task4_evaluation_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
                'save_detailed_results': True,
                'create_visualizations': True,
                'data': {'test_split': 'test'}
            },
            'filtering': {'batch': {'return_likelihood': True}},
            'uncertainty_analysis': {'enabled': True},
            'visualization': {'enabled': True}
        }
    
    def run_comprehensive_evaluation(
        self, 
        data_path: str,
        mode: str = 'standard'
    ) -> Dict[str, Any]:
        """
        åŒ…æ‹¬çš„è©•ä¾¡ã®å®Ÿè¡Œ
        
        Args:
            data_path: è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹
            mode: è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ ('quick', 'standard', 'comprehensive')
            
        Returns:
            çµ±åˆè©•ä¾¡çµæœ
        """
        print(f"\nğŸš€ ã‚¿ã‚¹ã‚¯4çµ±åˆè©•ä¾¡é–‹å§‹")
        print(f"ğŸ“Š è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰: {mode}")
        print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«: {self.model_path}")
        print(f"ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿: {data_path}")
        print(f"ğŸ“‚ å‡ºåŠ›: {self.output_dir}")
        print("="*70)
        
        evaluation_results = {}
        
        # ãƒ¢ãƒ¼ãƒ‰åˆ¥è¨­å®šé¸æŠãƒ»èª¿æ•´
        mode_config = self._load_config(mode)
        adjusted_config = self._adjust_config_for_mode(mode, mode_config)
        
        # Stage 1: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½è©•ä¾¡
        filtering_results = self._run_filtering_performance_evaluation(
            data_path, adjusted_config
        )
        evaluation_results['filtering_performance'] = filtering_results
        
        # Stage 2: æ¨å®šæ‰‹æ³•æ¯”è¼ƒ
        comparison_results = self._run_method_comparison_evaluation(
            data_path, adjusted_config
        )
        evaluation_results['method_comparison'] = comparison_results
        
        # Stage 3: çµ±åˆåˆ†æãƒ»çµæœæ•´ç†
        integrated_results = self._integrate_and_summarize_results(
            evaluation_results, adjusted_config
        )
        evaluation_results['integrated_analysis'] = integrated_results
        
        # Stage 4: æœ€çµ‚å‡ºåŠ›ãƒ»ä¿å­˜
        self._save_comprehensive_results(evaluation_results, mode)
        
        # æœ€çµ‚ã‚µãƒãƒªå‡ºåŠ›
        self._print_comprehensive_summary(evaluation_results)
        
        return evaluation_results
    
    def _adjust_config_for_mode(self, mode: str, base_config: Dict[str, Any] = None) -> Dict[str, Any]:
        """ãƒ¢ãƒ¼ãƒ‰åˆ¥è¨­å®šèª¿æ•´"""
        if base_config is None:
            config = self.config.copy()
        else:
            config = base_config.copy()
        
        if mode == 'quick':
            # ã‚¯ã‚¤ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰ï¼šé«˜é€ŸåŒ–ã®ãŸã‚ã®è¨­å®š
            # å®‰å…¨ãªè¾æ›¸ã‚¢ã‚¯ã‚»ã‚¹ã§æ—¢å­˜è¨­å®šã‚’å°Šé‡
            if 'evaluation' not in config:
                config['evaluation'] = {}
            config['evaluation']['save_detailed_results'] = config.get('evaluation', {}).get('save_detailed_results', False)
            config['evaluation']['create_visualizations'] = config.get('evaluation', {}).get('create_visualizations', False)
            
            # ãƒ‡ãƒ¼ã‚¿è¨­å®šã®å®‰å…¨ãªã‚¢ã‚¯ã‚»ã‚¹
            if 'evaluation' not in config:
                config['evaluation'] = {}
            if 'data' not in config['evaluation']:
                config['evaluation']['data'] = {}
            # å®Ÿéš›ã®YAMLæ§‹é€ ã‹ã‚‰å€¤ã‚’å–å¾—
            max_len = config.get('data', {}).get('max_evaluation_length', 100)
            config['evaluation']['data']['max_evaluation_length'] = max_len
            
            # ãã®ä»–ã®è¨­å®š
            if 'uncertainty_analysis' not in config:
                config['uncertainty_analysis'] = {}
            config['uncertainty_analysis']['enabled'] = config.get('uncertainty_analysis', {}).get('enabled', False)
            
            if 'visualization' not in config:
                config['visualization'] = {}
            config['visualization']['enabled'] = config.get('visualization', {}).get('enabled', False)
            
        elif mode == 'comprehensive':
            # åŒ…æ‹¬ãƒ¢ãƒ¼ãƒ‰ï¼šæœ€è©³ç´°è¨­å®šï¼ˆå®‰å…¨ãªã‚¢ã‚¯ã‚»ã‚¹ï¼‰
            if 'evaluation' not in config:
                config['evaluation'] = {}
            config['evaluation']['save_detailed_results'] = True
            config['evaluation']['create_visualizations'] = True
            
            if 'uncertainty_analysis' not in config:
                config['uncertainty_analysis'] = {}
            config['uncertainty_analysis']['enabled'] = True
            config['uncertainty_analysis']['temporal_analysis'] = {
                'trend_analysis': True,
                'volatility_analysis': True,
                'autocorr_analysis': True
            }
            
            if 'visualization' not in config:
                config['visualization'] = {}
            config['visualization']['enabled'] = True
            
            if 'output' not in config:
                config['output'] = {}
            if 'compression' not in config['output']:
                config['output']['compression'] = {}
            config['output']['compression']['enabled'] = True
            
        # else: 'standard' - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ãã®ã¾ã¾ä½¿ç”¨
        
        return config
    
    def _run_filtering_performance_evaluation(
        self, 
        data_path: str, 
        config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Stage 1: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½è©•ä¾¡ã®å®Ÿè¡Œ"""
        print(f"\nğŸ“Š Stage 1: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½è©•ä¾¡")
        print("-" * 50)
        
        stage_start = datetime.now()
        
        try:
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½è©•ä¾¡å™¨ã‚’ä½œæˆ
            performance_evaluator = FilteringPerformanceEvaluator(
                model_path=str(self.model_path),
                config_path=str(self.config_path),
                output_dir=str(self.output_dir / 'filtering_performance'),
                device=self.device
            )
            
            # è©•ä¾¡å®Ÿè¡Œ
            experiment_name = config['evaluation'].get(
                'experiment_name', 'filtering_performance'
            )
            
            # è¨­å®šæ§‹é€ ã®é•ã„ã‚’è€ƒæ…®ã—ãŸå®‰å…¨ãªã‚¢ã‚¯ã‚»ã‚¹
            evaluation_config = config.get('evaluation', {})
            data_config = config.get('data', evaluation_config.get('data', {}))
            
            results = performance_evaluator.evaluate_comprehensive(
                data_path=data_path,
                experiment_name=experiment_name,
                data_split=data_config.get('test_split', 'test'),
                save_detailed_results=evaluation_config.get('save_detailed_results', True),
                create_visualizations=evaluation_config.get('create_visualizations', True)
            )
            
            stage_duration = (datetime.now() - stage_start).total_seconds()
            
            self.experiment_log['stages'].append({
                'stage': 1,
                'name': 'filtering_performance',
                'start_time': stage_start.isoformat(),
                'duration': stage_duration,
                'status': 'completed'
            })
            
            print(f"âœ… Stage 1å®Œäº† ({stage_duration:.2f}ç§’)")
            return results
            
        except Exception as e:
            print(f"âŒ Stage 1ã‚¨ãƒ©ãƒ¼: {e}")
            print("âŒ è©³ç´°ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹:")
            traceback.print_exc()
            self.experiment_log['stages'].append({
                'stage': 1,
                'name': 'filtering_performance',
                'start_time': stage_start.isoformat(),
                'status': 'failed',
                'error': str(e)
            })
            return {'error': str(e), 'status': 'failed'}
    
    def _run_method_comparison_evaluation(
        self, 
        data_path: str, 
        config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Stage 2: æ¨å®šæ‰‹æ³•æ¯”è¼ƒã®å®Ÿè¡Œ"""
        print(f"\nğŸ” Stage 2: æ¨å®šæ‰‹æ³•æ¯”è¼ƒ")
        print("-" * 50)
        
        stage_start = datetime.now()
        
        try:
            # æ¨å®šæ‰‹æ³•æ¯”è¼ƒå™¨ã‚’ä½œæˆ
            method_comparator = EstimationMethodComparator(
                model_path=str(self.model_path),
                config_path=str(self.config_path),
                output_dir=str(self.output_dir / 'method_comparison'),
                device=self.device
            )
            
            # æ¯”è¼ƒå®Ÿè¡Œ
            experiment_name = config['evaluation'].get(
                'experiment_name', 'method_comparison'
            ) + '_comparison'
            
            # è¨­å®šæ§‹é€ ã®é•ã„ã‚’è€ƒæ…®ã—ãŸå®‰å…¨ãªã‚¢ã‚¯ã‚»ã‚¹
            evaluation_config = config.get('evaluation', {})
            data_config = config.get('data', evaluation_config.get('data', {}))
            
            results = method_comparator.compare_methods(
                data_path=data_path,
                experiment_name=experiment_name,
                data_split=data_config.get('test_split', 'test'),
                save_results=evaluation_config.get('save_detailed_results', True)
            )
            
            stage_duration = (datetime.now() - stage_start).total_seconds()
            
            self.experiment_log['stages'].append({
                'stage': 2,
                'name': 'method_comparison',
                'start_time': stage_start.isoformat(),
                'duration': stage_duration,
                'status': 'completed'
            })
            
            print(f"âœ… Stage 2å®Œäº† ({stage_duration:.2f}ç§’)")
            return results
            
        except Exception as e:
            print(f"âŒ Stage 2ã‚¨ãƒ©ãƒ¼: {e}")
            print("âŒ è©³ç´°ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹:")
            traceback.print_exc()
            self.experiment_log['stages'].append({
                'stage': 2,
                'name': 'method_comparison',
                'start_time': stage_start.isoformat(),
                'status': 'failed',
                'error': str(e)
            })
            return {'error': str(e), 'status': 'failed'}
    
    def _integrate_and_summarize_results(
        self, 
        evaluation_results: Dict[str, Any], 
        config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Stage 3: çµæœçµ±åˆãƒ»åˆ†æ"""
        print(f"\nğŸ“ˆ Stage 3: çµæœçµ±åˆãƒ»åˆ†æ")
        print("-" * 50)
        
        stage_start = datetime.now()
        
        try:
            integrated = {
                'summary_statistics': self._compute_summary_statistics(evaluation_results),
                'key_findings': self._extract_key_findings(evaluation_results),
                'performance_comparison': self._create_performance_comparison(evaluation_results),
                'recommendations': self._generate_recommendations(evaluation_results)
            }
            
            stage_duration = (datetime.now() - stage_start).total_seconds()
            
            self.experiment_log['stages'].append({
                'stage': 3,
                'name': 'integration',
                'start_time': stage_start.isoformat(),
                'duration': stage_duration,
                'status': 'completed'
            })
            
            print(f"âœ… Stage 3å®Œäº† ({stage_duration:.2f}ç§’)")
            return integrated
            
        except Exception as e:
            print(f"âŒ Stage 3ã‚¨ãƒ©ãƒ¼: {e}")
            print("âŒ è©³ç´°ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹:")
            traceback.print_exc()
            return {'error': str(e), 'status': 'failed'}
    
    def _compute_summary_statistics(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ã‚µãƒãƒªçµ±è¨ˆã®è¨ˆç®—"""
        summary = {}
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½çµ±è¨ˆ
        if 'filtering_performance' in results:
            filtering = results['filtering_performance']
            if 'summary_metrics' in filtering:
                summary['filtering'] = filtering['summary_metrics']
        
        # æ‰‹æ³•æ¯”è¼ƒçµ±è¨ˆ
        if 'method_comparison' in results:
            comparison = results['method_comparison']
            if 'summary' in comparison:
                summary['comparison'] = comparison['summary']
        
        return summary
    
    def _extract_key_findings(self, results: Dict[str, Any]) -> List[str]:
        """ä¸»è¦ç™ºè¦‹äº‹é …ã®æŠ½å‡º"""
        findings = []
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½ã‹ã‚‰
        if 'filtering_performance' in results:
            filtering = results['filtering_performance']
            if 'summary_metrics' in filtering:
                metrics = filtering['summary_metrics']
                
                # ãƒãƒƒãƒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ
                if 'batch_filtering' in metrics:
                    batch = metrics['batch_filtering']
                    findings.append(f"ãƒãƒƒãƒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°MSE: {batch.get('mse', 'N/A'):.6f}")
                    if 'coverage_95' in batch:
                        findings.append(f"95%ä¿¡é ¼åŒºé–“ã‚«ãƒãƒ¬ãƒƒã‚¸: {batch['coverage_95']:.4f}")
                
                # ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ
                if 'online_filtering' in metrics:
                    online = metrics['online_filtering']
                    findings.append(f"ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å¹³å‡ã‚¹ãƒ†ãƒƒãƒ—æ™‚é–“: {online.get('avg_step_time', 'N/A'):.6f}ç§’")
        
        # æ‰‹æ³•æ¯”è¼ƒã‹ã‚‰
        if 'method_comparison' in results:
            comparison = results['method_comparison']
            if 'summary' in comparison and 'comparison_summary' in comparison['summary']:
                comp_summary = comparison['summary']['comparison_summary']
                
                if 'accuracy' in comp_summary:
                    for metric, result in comp_summary['accuracy'].items():
                        findings.append(f"{metric.upper()}æœ€è‰¯æ‰‹æ³•: {result['best_method']}")
                        
                        if 'improvement' in result and 'kalman_vs_deterministic' in result['improvement']:
                            improvement = result['improvement']['kalman_vs_deterministic']
                            findings.append(f"Kalman {metric.upper()}æ”¹å–„ç‡: {improvement:+.2f}%")
        
        return findings
    
    def _create_performance_comparison(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """æ€§èƒ½æ¯”è¼ƒè¡¨ã®ä½œæˆ"""
        comparison = {}
        
        # å„æ‰‹æ³•ã®ä¸»è¦æŒ‡æ¨™ã‚’æ•´ç†
        methods = {}
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœã‹ã‚‰æŠ½å‡º
        if 'filtering_performance' in results:
            filtering = results['filtering_performance']
            if 'summary_metrics' in filtering:
                metrics = filtering['summary_metrics']
                
                if 'batch_filtering' in metrics:
                    methods['batch_kalman'] = metrics['batch_filtering']
                    
                if 'online_filtering' in metrics:
                    methods['online_kalman'] = metrics['online_filtering']
        
        # æ‰‹æ³•æ¯”è¼ƒçµæœã‹ã‚‰æŠ½å‡º
        if 'method_comparison' in results:
            comparison_data = results['method_comparison']
            if 'summary' in comparison_data:
                summary = comparison_data['summary']
                for method_name, method_data in summary.items():
                    if method_name != 'comparison_summary':
                        methods[method_name] = method_data
        
        comparison['methods'] = methods
        
        # æœ€è‰¯æ‰‹æ³•ã®ç‰¹å®š
        if methods:
            comparison['best_methods'] = self._identify_best_methods(methods)
        
        return comparison
    
    def _identify_best_methods(self, methods: Dict[str, Any]) -> Dict[str, str]:
        """æœ€è‰¯æ‰‹æ³•ã®ç‰¹å®š"""
        best = {}
        
        # å„æŒ‡æ¨™ã§æœ€è‰¯æ‰‹æ³•ã‚’ç‰¹å®š
        metrics_to_compare = ['mse', 'mae', 'rmse']  # å°ã•ã„ã»ã©è‰¯ã„
        
        for metric in metrics_to_compare:
            metric_values = {}
            for method_name, method_data in methods.items():
                if metric in method_data:
                    metric_values[method_name] = method_data[metric]
            
            if metric_values:
                best_method = min(metric_values, key=metric_values.get)
                best[metric] = best_method
        
        # ç›¸é–¢ã¯å¤§ãã„ã»ã©è‰¯ã„
        if any('correlation' in method_data for method_data in methods.values()):
            corr_values = {}
            for method_name, method_data in methods.items():
                if 'correlation' in method_data:
                    corr_values[method_name] = method_data['correlation']
            
            if corr_values:
                best_corr = max(corr_values, key=corr_values.get)
                best['correlation'] = best_corr
        
        return best
    
    def _generate_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = []
        
        # åŸºæœ¬æ¨å¥¨äº‹é …
        recommendations.append("ä¸ç¢ºå®Ÿæ€§å®šé‡åŒ–ãŒå¿…è¦ãªå ´åˆã¯Kalmanæ‰‹æ³•ã‚’ä½¿ç”¨")
        recommendations.append("é«˜é€Ÿæ¨è«–ãŒå„ªå…ˆã•ã‚Œã‚‹å ´åˆã¯æ±ºå®šçš„æ‰‹æ³•ã‚’æ¤œè¨")
        
        # çµæœã«åŸºã¥ãæ¨å¥¨äº‹é …
        if 'method_comparison' in results:
            comparison = results['method_comparison']
            if 'summary' in comparison and 'comparison_summary' in comparison['summary']:
                comp = comparison['summary']['comparison_summary']
                
                # ç²¾åº¦æ”¹å–„ãŒè¦‹ã‚‰ã‚Œã‚‹å ´åˆ
                if 'accuracy' in comp:
                    for metric, result in comp['accuracy'].items():
                        if 'improvement' in result and 'kalman_vs_deterministic' in result['improvement']:
                            improvement = result['improvement']['kalman_vs_deterministic']
                            if improvement > 5:  # 5%ä»¥ä¸Šæ”¹å–„
                                recommendations.append(f"Kalmanã¯{metric.upper()}ã§{improvement:.1f}%æ”¹å–„ã‚’å®Ÿç¾")
        
        return recommendations
    
    def _save_comprehensive_results(self, results: Dict[str, Any], mode: str):
        """çµ±åˆçµæœã®ä¿å­˜"""
        print(f"\nğŸ’¾ çµæœä¿å­˜ä¸­...")
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # çµ±åˆçµæœã‚’JSONå½¢å¼ã§ä¿å­˜
        comprehensive_results = {
            'experiment_info': {
                'mode': mode,
                'timestamp': timestamp,
                'model_path': str(self.model_path),
                'device': self.device,
                'total_duration': (datetime.now() - datetime.fromisoformat(self.experiment_log['start_time'])).total_seconds()
            },
            'experiment_log': self.experiment_log,
            'evaluation_results': results
        }
        
        # JSONä¿å­˜
        json_path = self.output_dir / 'summary' / f'task4_comprehensive_{timestamp}.json'
        with open(json_path, 'w') as f:
            json.dump(self._make_json_serializable(comprehensive_results), f, indent=2)
        
        # CSV ã‚µãƒãƒªä¿å­˜
        csv_path = self.output_dir / 'summary' / f'task4_summary_{timestamp}.csv'
        self._save_results_csv(results, csv_path)
        
        print(f"ğŸ“ çµ±åˆçµæœä¿å­˜: {json_path}")
        print(f"ğŸ“Š ã‚µãƒãƒªCSV: {csv_path}")
    
    def _save_results_csv(self, results: Dict[str, Any], csv_path: Path):
        """çµæœã‚’CSVå½¢å¼ã§ä¿å­˜"""
        import csv
        
        with open(csv_path, 'w', newline='') as f:
            writer = csv.writer(f)
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼
            writer.writerow([
                'category', 'method', 'mse', 'mae', 'rmse', 'correlation',
                'processing_time', 'coverage_95', 'has_uncertainty'
            ])
            
            # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½çµæœ
            if 'filtering_performance' in results and 'summary_metrics' in results['filtering_performance']:
                metrics = results['filtering_performance']['summary_metrics']
                
                for method_name, method_data in metrics.items():
                    if isinstance(method_data, dict):
                        writer.writerow([
                            'filtering_performance', method_name,
                            method_data.get('mse', ''),
                            method_data.get('mae', ''),
                            method_data.get('rmse', ''),
                            method_data.get('correlation', ''),
                            method_data.get('processing_time', ''),
                            method_data.get('coverage_95', ''),
                            'yes' if 'uncertainty' in method_name or 'kalman' in method_name else 'no'
                        ])
            
            # æ‰‹æ³•æ¯”è¼ƒçµæœ
            if 'method_comparison' in results and 'summary' in results['method_comparison']:
                summary = results['method_comparison']['summary']
                
                for method_name, method_data in summary.items():
                    if method_name != 'comparison_summary' and isinstance(method_data, dict):
                        writer.writerow([
                            'method_comparison', method_name,
                            method_data.get('mse', ''),
                            method_data.get('mae', ''),
                            method_data.get('rmse', ''),
                            method_data.get('correlation', ''),
                            method_data.get('processing_time', ''),
                            '',  # coverage not available in method comparison
                            'yes' if method_data.get('has_uncertainty', False) else 'no'
                        ])
    
    def _print_comprehensive_summary(self, results: Dict[str, Any]):
        """åŒ…æ‹¬çš„çµæœã‚µãƒãƒªã®å‡ºåŠ›"""
        print(f"\n" + "="*70)
        print(f"ğŸ‰ ã‚¿ã‚¹ã‚¯4çµ±åˆè©•ä¾¡å®Œäº†")
        print(f"ğŸ“Š å®Ÿé¨“å: {self.config['evaluation'].get('experiment_name', 'Task4 Evaluation')}")
        print("="*70)
        
        # ä¸»è¦ç™ºè¦‹äº‹é …
        if 'integrated_analysis' in results and 'key_findings' in results['integrated_analysis']:
            findings = results['integrated_analysis']['key_findings']
            print(f"\nğŸ” ä¸»è¦ç™ºè¦‹äº‹é …:")
            for i, finding in enumerate(findings, 1):
                print(f"   {i}. {finding}")
        
        # æ¨å¥¨äº‹é …
        if 'integrated_analysis' in results and 'recommendations' in results['integrated_analysis']:
            recommendations = results['integrated_analysis']['recommendations']
            print(f"\nğŸ’¡ æ¨å¥¨äº‹é …:")
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")
        
        # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«
        print(f"\nğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir}")
        print(f"   â”œâ”€â”€ filtering_performance/  # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ€§èƒ½è©³ç´°")
        print(f"   â”œâ”€â”€ method_comparison/      # æ‰‹æ³•æ¯”è¼ƒè©³ç´°")
        print(f"   â”œâ”€â”€ summary/                # çµ±åˆã‚µãƒãƒª")
        print(f"   â””â”€â”€ visualizations/         # å¯è¦–åŒ–çµæœ")
        
        print("="*70)
    
    def _make_json_serializable(self, obj):
        """JSONå¯¾å¿œå½¢å¼ã«å¤‰æ›"""
        import torch
        import numpy as np

        if isinstance(obj, dict):
            return {k: self._make_json_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_json_serializable(v) for v in obj]
        elif isinstance(obj, Path):
            return str(obj)
        elif isinstance(obj, torch.Tensor):
            return obj.detach().cpu().numpy().tolist()
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif hasattr(obj, '__float__'):  # numpy scalars
            return float(obj)
        elif hasattr(obj, '__int__'):  # numpy int scalars
            return int(obj)
        else:
            return obj


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description="ã‚¿ã‚¹ã‚¯4çµ±åˆè©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³")
    
    # å¿…é ˆå¼•æ•°
    parser.add_argument('--model', required=True, help='å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ (.pth)')
    parser.add_argument('--data', required=True, help='è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ (.npz)')
    parser.add_argument('--output', required=True, help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    
    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³å¼•æ•°
    parser.add_argument(
        '--config', 
        default='configs/evaluation_config.yaml',
        help='è©•ä¾¡è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹'
    )
    parser.add_argument(
        '--mode',
        default='standard',
        choices=['quick', 'standard', 'comprehensive'],
        help='è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰'
    )
    parser.add_argument(
        '--device',
        default='auto',
        help='è¨ˆç®—ãƒ‡ãƒã‚¤ã‚¹ (auto, cpu, cuda)'
    )
    
    args = parser.parse_args()
    
    # å¼•æ•°æ¤œè¨¼ - å¿…é ˆãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ãƒã‚§ãƒƒã‚¯ï¼ˆconfigã¯é™¤å¤–ï¼‰
    required_files = [
        ('model', args.model),
        ('data', args.data)
    ]
    
    for name, filepath in required_files:
        if not Path(filepath).exists():
            print(f"âŒ {name}ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {filepath}")
            return 1
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¯ä»»æ„ - å­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨
    if not Path(args.config).exists():
        print(f"âš ï¸  è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {args.config}")
        print("ğŸ“ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§å®Ÿè¡Œã—ã¾ã™")
    
    print(f"ğŸš€ ã‚¿ã‚¹ã‚¯4çµ±åˆè©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³")
    print(f"ğŸ“Š ãƒ¢ãƒ¼ãƒ‰: {args.mode}")
    print(f"ğŸ–¥ï¸  ãƒ‡ãƒã‚¤ã‚¹: {args.device}")
    
    try:
        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä½œæˆãƒ»å®Ÿè¡Œ
        pipeline = Task4EvaluationPipeline(
            model_path=args.model,
            config_path=args.config,
            output_dir=args.output,
            device=args.device
        )
        
        results = pipeline.run_comprehensive_evaluation(
            data_path=args.data,
            mode=args.mode
        )
        
        print(f"\nâœ… ã‚¿ã‚¹ã‚¯4çµ±åˆè©•ä¾¡å®Œäº†ï¼")
        print(f"ğŸ“‚ çµæœç¢ºèª: {args.output}")
        return 0
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹å®Ÿè¡Œä¸­æ–­")
        return 130
    except Exception as e:
        print(f"\nâŒ è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        print("âŒ è©³ç´°ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹:")
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())