# ====================================================================
# 画像再構成実験用設定ファイル (run_full_experiment.py用)
# 用途: クアッドコプター画像データでの画像再構成学習
# データ: data/rkn_quad/quad1_y.npz
# アーキテクチャ: rknEncoder + rknDecoder
# ====================================================================

# 実験モード設定（手動切り替え）
experiment:
  mode: "reconstruction"         # "reconstruction" or "target_prediction"

# データ設定
data:
  train_ratio: 0.7          # 学習データ比率 (1050 images)
  val_ratio: 0.2            # 検証データ比率 (300 images)
  test_ratio: 0.1           # テストデータ比率 (150 images)
  normalization: "unit_scale"  # 画像正規化: [0,255] → [0,1]
  batch_size: 300            # バッチサイズ (画像用に小さめ)
  image_shape: [48, 48, 1]  # 画像形状 (H, W, C)

# モデルアーキテクチャ設定 - RKN画像対応
model:
  encoder:
    type: "rkn"                    # Factory Pattern: rknEncoder選択
    input_resolution: [48, 48, 1]  # 入力画像解像度 (H, W, C)
    feature_dim: 50               # 潜在次元数（RKN仕様固定）
    hidden: 200                    # FC層隠れ次元数
    conv_channels: [32, 64]        # CNN層チャネル数
    activation: "relu"             # 活性化関数
    normalize_input: false         # 画像用は通常False
    normalize_output: false        # 潜在表現正規化
    track_running_stats: true      # 統計量追跡

  decoder:
    type: "rkn"                    # Factory Pattern: rknDecoder選択
    input_resolution: [48, 48, 1]  # 出力画像解像度 (H, W, C)
    feature_dim: 50               # 入力潜在次元数
    hidden: 200                    # FC層隠れ次元数
    grid: [12, 12, 64]            # 中間グリッドサイズ (H/4, W/4, Ch)
    upsample_mode: "nearest"       # アップサンプリング方法
    conv_channels: [64, 32, 1]     # 逆畳み込み層チャネル数
    activation: "relu"             # 活性化関数
    output_activation: "sigmoid"   # 出力活性化関数（画像用）


  # 状態デコーダ設定 (オプション - 制御状態推定用)
  state_decoder:
    feature_dim: 50               # 潜在次元数
    state_dim: 8                   # クアッドコプター制御状態次元
    hidden: 50                     # 隠れ層次元数
    activation: "relu"

# 学習設定
training:
  # 以下は統合学習で未使用のため無効化
  # max_epochs: 10           # 最大エポック数（統合学習では'epochs'を使用）
  # learning_rate: 0.001     # 学習率（統合学習では個別学習率lr_phi等を使用）
  # patience: 15             # Early stopping忍耐度（統合学習では未実装）

  # 統合学習設定 (各エポックでPhase-1 + Phase-2を連続実行)
  epochs: 10               # 総エポック数
  T1_iterations: 5         # DF-A/DF-B Stage-1反復数（各エポック内）
  T2_iterations: 5          # DF-A/DF-B Stage-2反復数（各エポック内）
  phase1_warmup_epochs: 5   # Phase-2(End-to-end)開始前のPhase-1(DF-A/DF-B)ウォームアップ

  # 実験モード設定（デフォルトは再構成モード）
  experiment_mode: "reconstruction"    # Phase-2での動作切り替え
  target_loss_weight: 0.0              # ターゲット損失重み（未使用）
  reconstruction_loss_weight: 1.0      # 再構成損失重み

  # Phase-1関連学習率（各エポック内のDF学習用）
  lr_phi: 1e-3             # φ_θ (状態特徴) 学習率
  lr_psi: 1e-3             # ψ_ω (観測特徴) 学習率
  weight_decay: 1e-4        # 正則化パラメータ

  # Phase-2関連学習率（各エポック内のEnd-to-end学習用）
  lr_encoder: 1e-3          # エンコーダ学習率
  lr_decoder: 1e-3          # デコーダ学習率（再構成用）
  lambda_cca: 0.0001         # CCA損失重み（画像用に小さめ）
  update_strategy: "encoder_decoder_only"  # Phase-2はencoder/decoderのみ更新（設計通り）

  # 画像再構成特有の設定（統合学習では未使用のため無効化）
  # reconstruction:
  #   loss_type: "mse"        # 再構成損失: mse, l1, ssim
  #   perceptual_loss_weight: 0.0  # 知覚損失重み（必要に応じて）

# SSM (状態空間モデル) 設定
ssm:
  realization:
    past_horizon: 20        # 過去窓長（画像時系列用）
    rank: 30                 # 状態次元数（画像用に少し大きめ）
    encoder_output_dim: 50   # エンコーダー出力次元 (model.encoder.feature_dimと合わせる)
    ridge_param: 1e-3        # Ridge正則化パラメータ
    jitter: 1e-6           # 数値安定化パラメータ
    m: 50                  # ラグ共分散推定サンプル数

    # ======== 特徴写像設定 (Step 14追加) ========
    feature_mapping:
      type: "mlp"      # "averaging" | "linear" | "mlp"
      hidden_dims: [32]        # MLPの隠れ層（[] で線形のみ、[32] で浅いMLP）
      activation: "relu"     # "relu" | "tanh" | "gelu"

  # DF-A (状態層) 設定
  df_state:
    feature_dim: 50         # d_A: 状態特徴次元 (R^r → R^{d_A})
    lambda_A: 1e-3          # V_A正則化パラメータ (特異値爆発抑制のため強化)
    lambda_B: 1e-3          # U_A正則化パラメータ (特異値爆発抑制のため強化)
    feature_net:            # StateFeatureNet (φ_θ) 深層学習設定
      hidden_sizes: [128, 64]  # MLP隠れ層サイズ (画像用に大きめ)
      activation: "ReLU"       # 活性化関数
      dropout: 0.1             # ドロップアウト率 (過学習防止)
    cross_fitting:
      n_blocks: 5           # クロスフィッティングブロック数
      min_block_size: 100   # 最小ブロックサイズ

  # DF-B (観測層) 設定
  df_observation:
    obs_feature_dim: 25         # d_B: 観測特徴次元 (R^m → R^{d_B})
    multivariate_feature_dim: 50  # m: エンコーダー出力次元 (model.encoder.feature_dimと合わせる)
    lambda_B: 1e-3              # V_B正則化パラメータ (特異値爆発抑制のため強化)
    lambda_dB: 1e-3             # u_B正則化パラメータ (特異値爆発抑制のため強化)
    obs_net:                    # ObservationFeatureNet (ψ_ω) 深層学習設定
      hidden_sizes: [64, 32]      # MLP隠れ層サイズ (観測用)
      activation: "ReLU"          # 活性化関数
      dropout: 0.1                # ドロップアウト率 (過学習防止)
    cross_fitting:
      n_blocks: 5               # クロスフィッティングブロック数
      min_block_size: 100       # 最小ブロックサイズ

# 計算設定
computation:
  device: "auto"            # デバイス選択 (auto/cuda/cpu)
  num_workers: 4            # データローダー並列数
  pin_memory: true          # GPU転送高速化

# 出力設定
output:
  save_model: true          # モデル保存
  save_history: true        # 学習履歴保存
  create_plots: true        # 可視化作成
  save_reconstructions: true # 再構成画像保存

# ログ設定
logging:
  level: "INFO"             # ログレベル (DEBUG/INFO/WARNING)
  console_output: true      # コンソール出力
  file_output: true         # ファイル出力

# 評価設定
evaluation:
  use_new_realization: true # 新しい確率実現使用 (StochasticRealizationWithEncoder)

  # Step 8追加: 再構成評価設定
  reconstruction_metrics:
    metrics: ['reconstruction_rmse', 'psnr', 'temporal_correlation']  # 評価指標選択
    max_samples: 100        # 評価対象サンプル数
    visualization: true     # 可視化生成（段階的実装）


  # 画像再構成評価指標（互換性のため保持）
  reconstruction:
    metrics:
      - "mse"                 # 平均二乗誤差
      - "psnr"                # ピーク信号対雑音比
      - "ssim"                # 構造類似性指数
    save_detailed_results: true

  # エンコード特徴空間可視化設定
  encoded_feature_space_viz:
    dim_indices: [0, 1]     # 表示する次元のインデックス（デフォルト: 最初の2次元）
    max_samples: 100        # 最大表示サンプル数（入力が少ない場合は入力データ長を使用）